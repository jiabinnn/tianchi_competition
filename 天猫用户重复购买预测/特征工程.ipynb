{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# 工具导入\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import gc\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "test_data = pd.read_csv('dataset/data_format1/test_format1.csv')\n",
    "train_data = pd.read_csv('dataset/data_format1/train_format1.csv')\n",
    "\n",
    "user_info = pd.read_csv('dataset/data_format1/user_info_format1.csv')\n",
    "user_log = pd.read_csv('dataset/data_format1/user_log_format1.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 对数据进行内存压缩\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    numerics = ['int16', 'int32','int64','float16','float32','float64']\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('decrease by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "num_rows = None\n",
    "train_file = './dataset/data_format1/train_format1.csv'\n",
    "test_file = './dataset/data_format1/test_format1.csv'\n",
    "\n",
    "user_info_file = './dataset/data_format1/user_info_format1.csv'\n",
    "user_log_file = './dataset/data_format1/user_log_format1.csv'\n",
    "\n",
    "# train_data = reduce_mem_usage(pd.read_csv(train_file, num_rows))\n",
    "# test_data = reduce_mem_usage(pd.read_csv(test_file, num_rows))\n",
    "# user_info = reduce_mem_usage(pd.read_csv(user_info_file, num_rows))\n",
    "# user_log = reduce_mem_usage(pd.read_csv(user_log_file, num_rows)) # 运行太慢"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260864 entries, 0 to 260863\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype\n",
      "---  ------       --------------   -----\n",
      " 0   user_id      260864 non-null  int64\n",
      " 1   merchant_id  260864 non-null  int64\n",
      " 2   label        260864 non-null  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 261477 entries, 0 to 261476\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   user_id      261477 non-null  int64  \n",
      " 1   merchant_id  261477 non-null  int64  \n",
      " 2   prob         0 non-null       float64\n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424170 entries, 0 to 424169\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    424170 non-null  int64  \n",
      " 1   age_range  421953 non-null  float64\n",
      " 2   gender     417734 non-null  float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 9.7 MB\n"
     ]
    }
   ],
   "source": [
    "user_info.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54925330 entries, 0 to 54925329\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   user_id      int64  \n",
      " 1   item_id      int64  \n",
      " 2   cat_id       int64  \n",
      " 3   seller_id    int64  \n",
      " 4   brand_id     float64\n",
      " 5   time_stamp   int64  \n",
      " 6   action_type  int64  \n",
      "dtypes: float64(1), int64(6)\n",
      "memory usage: 2.9 GB\n"
     ]
    }
   ],
   "source": [
    "user_log.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "15"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据处理\n",
    "all_data = train_data.append(test_data)\n",
    "all_data = all_data.merge(user_info, on=['user_id'], how='left')\n",
    "del train_data, test_data, user_info\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "   user_id  merchant_id  label  prob  age_range  gender\n0    34176         3906    0.0   NaN        6.0     0.0\n1    34176          121    0.0   NaN        6.0     0.0\n2    34176         4356    1.0   NaN        6.0     0.0\n3    34176         2217    0.0   NaN        6.0     0.0\n4   230784         4818    0.0   NaN        0.0     0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>merchant_id</th>\n      <th>label</th>\n      <th>prob</th>\n      <th>age_range</th>\n      <th>gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34176</td>\n      <td>3906</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>34176</td>\n      <td>121</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34176</td>\n      <td>4356</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>34176</td>\n      <td>2217</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>230784</td>\n      <td>4818</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 按时间排序\n",
    "user_log = user_log.sort_values(['user_id', 'time_stamp'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 对每个用户合并所有字段，合并字段为item_id\n",
    "list_join_func = lambda x: \" \".join([str(i) for i in x])\n",
    "\n",
    "agg_dict = {\n",
    "    'item_id': list_join_func,\n",
    "    'cat_id': list_join_func,\n",
    "    'seller_id': list_join_func,\n",
    "    'brand_id': list_join_func,\n",
    "    'time_stamp': list_join_func,\n",
    "    'action_type': list_join_func,\n",
    "}\n",
    "\n",
    "rename_dict = {\n",
    "    'item_id': 'item_path',\n",
    "    'cat_id': 'cat_path',\n",
    "    'seller_id': 'seller_path',\n",
    "    'brand_id': 'brand_path',\n",
    "    'time_stamp': 'time_stamp_path',\n",
    "    'action_type': 'action_type_path',\n",
    "}\n",
    "\n",
    "def merge_list(df_ID, join_columns, df_data, agg_dict, rename_dict):\n",
    "    df_data = df_data\\\n",
    "        .groupby(join_columns)\\\n",
    "        .agg(agg_dict)\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns=rename_dict)\n",
    "    df_ID = df_ID.merge(df_data, on=join_columns, how='left')\n",
    "    return df_ID\n",
    "'''\n",
    "按照user_id分组，分别按照agg_dict聚合，\n",
    "得到与该user_id有过交互的所有item_id cat_id ...保存在item_path,cat_path,...里\n",
    "'''\n",
    "all_data = merge_list(all_data, 'user_id', user_log, agg_dict, rename_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 522341 entries, 0 to 522340\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   user_id           522341 non-null  int64  \n",
      " 1   merchant_id       522341 non-null  int64  \n",
      " 2   label             260864 non-null  float64\n",
      " 3   prob              0 non-null       float64\n",
      " 4   age_range         519763 non-null  float64\n",
      " 5   gender            514796 non-null  float64\n",
      " 6   item_path         522341 non-null  object \n",
      " 7   cat_path          522341 non-null  object \n",
      " 8   seller_path       522341 non-null  object \n",
      " 9   brand_path        522341 non-null  object \n",
      " 10  time_stamp_path   522341 non-null  object \n",
      " 11  action_type_path  522341 non-null  object \n",
      "dtypes: float64(4), int64(2), object(6)\n",
      "memory usage: 51.8+ MB\n"
     ]
    }
   ],
   "source": [
    "all_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "        user_id  merchant_id  label  prob  age_range  gender  \\\n0         34176         3906    0.0   NaN        6.0     0.0   \n1         34176          121    0.0   NaN        6.0     0.0   \n2         34176         4356    1.0   NaN        6.0     0.0   \n3         34176         2217    0.0   NaN        6.0     0.0   \n4        230784         4818    0.0   NaN        0.0     0.0   \n...         ...          ...    ...   ...        ...     ...   \n522336   228479         3111    NaN   NaN        6.0     0.0   \n522337    97919         2341    NaN   NaN        8.0     1.0   \n522338    97919         3971    NaN   NaN        8.0     1.0   \n522339    32639         3536    NaN   NaN        0.0     0.0   \n522340    32639         3319    NaN   NaN        0.0     0.0   \n\n                                                item_path  \\\n0       581818 879005 581818 581818 1011673 52343 2773...   \n1       581818 879005 581818 581818 1011673 52343 2773...   \n2       581818 879005 581818 581818 1011673 52343 2773...   \n3       581818 879005 581818 581818 1011673 52343 2773...   \n4       191923 191923 191923 191923 964906 229470 2294...   \n...                                                   ...   \n522336  802791 977305 351177 122937 21972 863063 10903...   \n522337  484765 128769 128769 995386 128769 645625 9953...   \n522338  484765 128769 128769 995386 128769 645625 9953...   \n522339  394570 394570 394570 28017 110194 314126 95836...   \n522340  394570 394570 394570 28017 110194 314126 95836...   \n\n                                                 cat_path  \\\n0       1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n1       1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n2       1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n3       1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n4       1023 1023 1023 1023 662 664 664 1544 664 662 6...   \n...                                                   ...   \n522336  602 602 602 602 552 1271 1271 662 662 821 662 ...   \n522337  737 464 464 464 464 464 464 464 464 464 464 46...   \n522338  737 464 464 464 464 464 464 464 464 464 464 46...   \n522339  1413 1413 1413 812 1271 1271 1271 1198 1271 11...   \n522340  1413 1413 1413 812 1271 1271 1271 1198 1271 11...   \n\n                                              seller_path  \\\n0       416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n1       416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n2       416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n3       416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n4       3545 3545 3545 3545 4566 2537 2537 2420 2537 4...   \n...                                                   ...   \n522336  2823 2823 2664 2664 1076 2946 2781 4949 2412 4...   \n522337  4408 235 235 235 235 3416 235 235 235 235 235 ...   \n522338  4408 235 235 235 235 3416 235 235 235 235 235 ...   \n522339  1065 1065 1065 1506 38 1890 2280 4873 2280 487...   \n522340  1065 1065 1065 1506 38 1890 2280 4873 2280 487...   \n\n                                               brand_path  \\\n0       4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...   \n1       4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...   \n2       4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...   \n3       4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...   \n4       5860.0 5860.0 5860.0 5860.0 6322.0 6066.0 6066...   \n...                                                   ...   \n522336  1128.0 1128.0 8154.0 8154.0 3549.0 5559.0 3305...   \n522337  6967.0 2020.0 2020.0 2020.0 2020.0 6242.0 2020...   \n522338  6967.0 2020.0 2020.0 2020.0 2020.0 6242.0 2020...   \n522339  6374.0 4468.0 6374.0 4888.0 7010.0 5683.0 5372...   \n522340  6374.0 4468.0 6374.0 4888.0 7010.0 5683.0 5372...   \n\n                                          time_stamp_path  \\\n0       521 521 521 521 521 521 521 521 521 521 521 52...   \n1       521 521 521 521 521 521 521 521 521 521 521 52...   \n2       521 521 521 521 521 521 521 521 521 521 521 52...   \n3       521 521 521 521 521 521 521 521 521 521 521 52...   \n4       601 601 601 601 614 614 614 614 614 614 618 61...   \n...                                                   ...   \n522336  511 511 512 512 512 516 516 521 521 521 521 52...   \n522337  626 707 707 710 710 710 710 710 710 710 710 71...   \n522338  626 707 707 710 710 710 710 710 710 710 710 71...   \n522339  523 523 523 525 617 617 723 723 723 723 807 81...   \n522340  523 523 523 525 617 617 723 723 723 723 807 81...   \n\n                                         action_type_path  \n0       0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...  \n1       0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...  \n2       0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...  \n3       0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...  \n4       0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 ...  \n...                                                   ...  \n522336  3 3 2 2 2 3 3 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 ...  \n522337  2 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 3 ...  \n522338  2 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 3 ...  \n522339  0 2 0 0 0 0 0 0 0 2 0 0 0 0 2 0 2 0 0 3 0 0 0 ...  \n522340  0 2 0 0 0 0 0 0 0 2 0 0 0 0 2 0 2 0 0 3 0 0 0 ...  \n\n[522341 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>merchant_id</th>\n      <th>label</th>\n      <th>prob</th>\n      <th>age_range</th>\n      <th>gender</th>\n      <th>item_path</th>\n      <th>cat_path</th>\n      <th>seller_path</th>\n      <th>brand_path</th>\n      <th>time_stamp_path</th>\n      <th>action_type_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34176</td>\n      <td>3906</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>34176</td>\n      <td>121</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34176</td>\n      <td>4356</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>34176</td>\n      <td>2217</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>230784</td>\n      <td>4818</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>191923 191923 191923 191923 964906 229470 2294...</td>\n      <td>1023 1023 1023 1023 662 664 664 1544 664 662 6...</td>\n      <td>3545 3545 3545 3545 4566 2537 2537 2420 2537 4...</td>\n      <td>5860.0 5860.0 5860.0 5860.0 6322.0 6066.0 6066...</td>\n      <td>601 601 601 601 614 614 614 614 614 614 618 61...</td>\n      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>522336</th>\n      <td>228479</td>\n      <td>3111</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>802791 977305 351177 122937 21972 863063 10903...</td>\n      <td>602 602 602 602 552 1271 1271 662 662 821 662 ...</td>\n      <td>2823 2823 2664 2664 1076 2946 2781 4949 2412 4...</td>\n      <td>1128.0 1128.0 8154.0 8154.0 3549.0 5559.0 3305...</td>\n      <td>511 511 512 512 512 516 516 521 521 521 521 52...</td>\n      <td>3 3 2 2 2 3 3 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n    </tr>\n    <tr>\n      <th>522337</th>\n      <td>97919</td>\n      <td>2341</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>484765 128769 128769 995386 128769 645625 9953...</td>\n      <td>737 464 464 464 464 464 464 464 464 464 464 46...</td>\n      <td>4408 235 235 235 235 3416 235 235 235 235 235 ...</td>\n      <td>6967.0 2020.0 2020.0 2020.0 2020.0 6242.0 2020...</td>\n      <td>626 707 707 710 710 710 710 710 710 710 710 71...</td>\n      <td>2 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 3 ...</td>\n    </tr>\n    <tr>\n      <th>522338</th>\n      <td>97919</td>\n      <td>3971</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>484765 128769 128769 995386 128769 645625 9953...</td>\n      <td>737 464 464 464 464 464 464 464 464 464 464 46...</td>\n      <td>4408 235 235 235 235 3416 235 235 235 235 235 ...</td>\n      <td>6967.0 2020.0 2020.0 2020.0 2020.0 6242.0 2020...</td>\n      <td>626 707 707 710 710 710 710 710 710 710 710 71...</td>\n      <td>2 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 3 ...</td>\n    </tr>\n    <tr>\n      <th>522339</th>\n      <td>32639</td>\n      <td>3536</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>394570 394570 394570 28017 110194 314126 95836...</td>\n      <td>1413 1413 1413 812 1271 1271 1271 1198 1271 11...</td>\n      <td>1065 1065 1065 1506 38 1890 2280 4873 2280 487...</td>\n      <td>6374.0 4468.0 6374.0 4888.0 7010.0 5683.0 5372...</td>\n      <td>523 523 523 525 617 617 723 723 723 723 807 81...</td>\n      <td>0 2 0 0 0 0 0 0 0 2 0 0 0 0 2 0 2 0 0 3 0 0 0 ...</td>\n    </tr>\n    <tr>\n      <th>522340</th>\n      <td>32639</td>\n      <td>3319</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>394570 394570 394570 28017 110194 314126 95836...</td>\n      <td>1413 1413 1413 812 1271 1271 1271 1198 1271 11...</td>\n      <td>1065 1065 1065 1506 38 1890 2280 4873 2280 487...</td>\n      <td>6374.0 4468.0 6374.0 4888.0 7010.0 5683.0 5372...</td>\n      <td>523 523 523 525 617 617 723 723 723 723 807 81...</td>\n      <td>0 2 0 0 0 0 0 0 0 2 0 0 0 0 2 0 2 0 0 3 0 0 0 ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>522341 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "57"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del user_log\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 定义特征统计函数\n",
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def nunique(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "# 定义统计数据中对topN数据的函数\n",
    "def most_n(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        return -1\n",
    "# 定义统计数据中对topN数据总数的函数\n",
    "def most_n_cnt(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        return -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# 调用定义的统计函数\n",
    "'''\n",
    "df_data: 数据\n",
    "single_col：要进行统计的列名\n",
    "name：统计完成新建的列名\n",
    "'''\n",
    "def user_cnt(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    return df_data\n",
    "\n",
    "def user_nunique(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(nunique)\n",
    "    return df_data\n",
    "\n",
    "def user_max(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    return df_data\n",
    "\n",
    "def user_min(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    return df_data\n",
    "\n",
    "def user_std(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n_cnt(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3.6 提取统计特征"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 特征统计\n",
    "# 店铺特征统计：统计与店铺特点有关的特征，如店铺、商品、品牌等\n",
    "all_data_test = all_data.head(2000)\n",
    "# 统计用户点击、浏览、加购、购买行为\n",
    "# 总次数\n",
    "all_data_test = user_cnt(all_data_test, 'seller_path', 'user_cnt')\n",
    "# 不同店铺个数\n",
    "all_data_test = user_nunique(all_data_test, 'seller_path', 'seller_nunique')\n",
    "# 不同品类个数\n",
    "all_data_test = user_nunique(all_data_test, 'cat_path', 'cat_nunique')\n",
    "# 不同品牌个数\n",
    "all_data_test = user_nunique(all_data_test, 'brand_path', 'brand_nunique')\n",
    "# 不同商品个数\n",
    "all_data_test = user_nunique(all_data_test, 'item_path', 'item_nunique')\n",
    "# 活跃天数\n",
    "all_data_test = user_nunique(all_data_test, 'time_stamp_path', 'time_stamp_nunique')\n",
    "# 不同用户行为种数\n",
    "all_data_test = user_nunique(all_data_test, 'action_type_path', 'action_type_nunique')\n",
    "# 最晚时间\n",
    "all_data_test = user_max(all_data_test, 'action_type_path', 'time_stamp_max')\n",
    "# 最早时间\n",
    "all_data_test = user_min(all_data_test, 'action_type_path', 'time_stamp_min')\n",
    "# 活跃天数方差\n",
    "all_data_test = user_std(all_data_test, 'action_type_path', 'time_stamp_std')\n",
    "# 最早和最晚相差天数\n",
    "all_data_test['time_stamp_range'] = all_data_test['time_stamp_max'] - all_data_test['time_stamp_min']\n",
    "# 用户最喜欢的店铺\n",
    "all_data_test = user_most_n(all_data_test, 'seller_path', 'seller_most_1', n=1)\n",
    "# 最喜欢的类目\n",
    "all_data_test = user_most_n(all_data_test, 'cat_path', 'cat_most_1', n=1)\n",
    "# 最喜欢的品牌\n",
    "all_data_test = user_most_n(all_data_test, 'brand_path', 'brand_most_1', n=1)\n",
    "# 最常见的行为动作\n",
    "all_data_test = user_most_n(all_data_test, 'action_type_path', 'action_type_most_1', n=1)\n",
    "# ...\n",
    "# 用户最喜欢的店铺 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'seller_path', 'seller_most_1_cnt', n=1)\n",
    "# 最喜欢的类目 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'cat_path', 'cat_most_1_cnt', n=1)\n",
    "# 最喜欢的品牌 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'brand_path', 'brand_most_1_cnt', n=1)\n",
    "# 最常见的行为动作 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'action_type_path', 'action_type_most_1_cnt', n=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "             user_id  merchant_id        label  prob    age_range  \\\ncount    2000.000000  2000.000000  2000.000000   0.0  1995.000000   \nmean   208239.475500  2533.319000     0.064500   NaN     3.135840   \nstd    122331.596829  1457.359826     0.245703   NaN     1.889127   \nmin       132.000000    10.000000     0.000000   NaN     0.000000   \n25%    100930.500000  1329.500000     0.000000   NaN     2.000000   \n50%    206466.000000  2455.500000     0.000000   NaN     3.000000   \n75%    313024.500000  3910.000000     0.000000   NaN     4.000000   \nmax    424068.000000  4993.000000     1.000000   NaN     8.000000   \n\n            gender     user_cnt  seller_nunique  cat_nunique  brand_nunique  \\\ncount  1983.000000  2000.000000     2000.000000  2000.000000    2000.000000   \nmean      0.349975   140.082500       35.701500    23.987000      35.374000   \nstd       0.530178   202.303623       35.853021    18.091365      35.298146   \nmin       0.000000     2.000000        2.000000     1.000000       2.000000   \n25%       0.000000    39.000000       14.000000    12.000000      14.000000   \n50%       0.000000    84.000000       25.000000    20.000000      25.000000   \n75%       1.000000   163.000000       46.000000    31.000000      45.000000   \nmax       2.000000  2470.000000      344.000000   161.000000     320.000000   \n\n       ...  time_stamp_nunique  action_type_nunique  time_stamp_max  \\\ncount  ...         2000.000000           2000.00000     2000.000000   \nmean   ...           17.845500              2.63700        2.577000   \nstd    ...           14.733505              0.54807        0.494159   \nmin    ...            2.000000              1.00000        2.000000   \n25%    ...            7.000000              2.00000        2.000000   \n50%    ...           13.000000              3.00000        3.000000   \n75%    ...           24.000000              3.00000        3.000000   \nmax    ...          105.000000              4.00000        3.000000   \n\n       time_stamp_min  time_stamp_std  time_stamp_range  seller_most_1_cnt  \\\ncount     2000.000000     2000.000000        2000.00000        2000.000000   \nmean         0.004000        0.743902           2.57300          29.575000   \nstd          0.089376        0.264871           0.50279          46.845711   \nmin          0.000000        0.000000           0.00000           1.000000   \n25%          0.000000        0.554763           2.00000           9.000000   \n50%          0.000000        0.706387           3.00000          16.000000   \n75%          0.000000        0.903796           3.00000          32.250000   \nmax          2.000000        1.475535           3.00000         966.000000   \n\n       cat_most_1_cnt  brand_most_1_cnt  action_type_most_1_cnt  \ncount      2000.00000       2000.000000             2000.000000  \nmean         30.53200         29.844500              123.778000  \nstd          39.37676         47.037364              186.462639  \nmin           1.00000          1.000000                2.000000  \n25%          10.00000          9.000000               32.000000  \n50%          19.00000         16.000000               70.000000  \n75%          34.00000         32.000000              141.250000  \nmax         532.00000        966.000000             2270.000000  \n\n[8 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>merchant_id</th>\n      <th>label</th>\n      <th>prob</th>\n      <th>age_range</th>\n      <th>gender</th>\n      <th>user_cnt</th>\n      <th>seller_nunique</th>\n      <th>cat_nunique</th>\n      <th>brand_nunique</th>\n      <th>...</th>\n      <th>time_stamp_nunique</th>\n      <th>action_type_nunique</th>\n      <th>time_stamp_max</th>\n      <th>time_stamp_min</th>\n      <th>time_stamp_std</th>\n      <th>time_stamp_range</th>\n      <th>seller_most_1_cnt</th>\n      <th>cat_most_1_cnt</th>\n      <th>brand_most_1_cnt</th>\n      <th>action_type_most_1_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>0.0</td>\n      <td>1995.000000</td>\n      <td>1983.000000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>...</td>\n      <td>2000.000000</td>\n      <td>2000.00000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>2000.00000</td>\n      <td>2000.000000</td>\n      <td>2000.00000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>208239.475500</td>\n      <td>2533.319000</td>\n      <td>0.064500</td>\n      <td>NaN</td>\n      <td>3.135840</td>\n      <td>0.349975</td>\n      <td>140.082500</td>\n      <td>35.701500</td>\n      <td>23.987000</td>\n      <td>35.374000</td>\n      <td>...</td>\n      <td>17.845500</td>\n      <td>2.63700</td>\n      <td>2.577000</td>\n      <td>0.004000</td>\n      <td>0.743902</td>\n      <td>2.57300</td>\n      <td>29.575000</td>\n      <td>30.53200</td>\n      <td>29.844500</td>\n      <td>123.778000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>122331.596829</td>\n      <td>1457.359826</td>\n      <td>0.245703</td>\n      <td>NaN</td>\n      <td>1.889127</td>\n      <td>0.530178</td>\n      <td>202.303623</td>\n      <td>35.853021</td>\n      <td>18.091365</td>\n      <td>35.298146</td>\n      <td>...</td>\n      <td>14.733505</td>\n      <td>0.54807</td>\n      <td>0.494159</td>\n      <td>0.089376</td>\n      <td>0.264871</td>\n      <td>0.50279</td>\n      <td>46.845711</td>\n      <td>39.37676</td>\n      <td>47.037364</td>\n      <td>186.462639</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>132.000000</td>\n      <td>10.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>2.000000</td>\n      <td>1.00000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>100930.500000</td>\n      <td>1329.500000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>39.000000</td>\n      <td>14.000000</td>\n      <td>12.000000</td>\n      <td>14.000000</td>\n      <td>...</td>\n      <td>7.000000</td>\n      <td>2.00000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.554763</td>\n      <td>2.00000</td>\n      <td>9.000000</td>\n      <td>10.00000</td>\n      <td>9.000000</td>\n      <td>32.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>206466.000000</td>\n      <td>2455.500000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>84.000000</td>\n      <td>25.000000</td>\n      <td>20.000000</td>\n      <td>25.000000</td>\n      <td>...</td>\n      <td>13.000000</td>\n      <td>3.00000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.706387</td>\n      <td>3.00000</td>\n      <td>16.000000</td>\n      <td>19.00000</td>\n      <td>16.000000</td>\n      <td>70.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>313024.500000</td>\n      <td>3910.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>163.000000</td>\n      <td>46.000000</td>\n      <td>31.000000</td>\n      <td>45.000000</td>\n      <td>...</td>\n      <td>24.000000</td>\n      <td>3.00000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.903796</td>\n      <td>3.00000</td>\n      <td>32.250000</td>\n      <td>34.00000</td>\n      <td>32.000000</td>\n      <td>141.250000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>424068.000000</td>\n      <td>4993.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>8.000000</td>\n      <td>2.000000</td>\n      <td>2470.000000</td>\n      <td>344.000000</td>\n      <td>161.000000</td>\n      <td>320.000000</td>\n      <td>...</td>\n      <td>105.000000</td>\n      <td>4.00000</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>1.475535</td>\n      <td>3.00000</td>\n      <td>966.000000</td>\n      <td>532.00000</td>\n      <td>966.000000</td>\n      <td>2270.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# 用户特征统计：对用户对点击、加购、购买、收藏等特征进行统计。\n",
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            # 此处与书上不一样\n",
    "            if data_txt != '':\n",
    "                data_out.append(data_txt)\n",
    "            # if data_dict['action_type_path'][i_] == action_type:\n",
    "            #     data_out.append('_'+data_dict['action_type_path'][i_])\n",
    "        return len(data_out)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def col_nunique_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            # 此处课本与书上不一样\n",
    "            if data_txt != '':\n",
    "                data_out.append(data_txt)\n",
    "        return len(set(data_out))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "'''\n",
    "columns_list: 要计算的列名\n",
    "action_type：\n",
    "'''\n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis=1)\n",
    "    return df_data\n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_nunique_(x, columns_list, action_type), axis=1)\n",
    "    return df_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# 统计用户和店铺的关系：对店铺的用户点击次数、加购次数、购买次数、收藏次数等\n",
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '0', 'user_cnt_0') # 该用户的点击次数\n",
    "# 加购次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '1', 'user_cnt_1') # 该用户的加购次数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "      user_id  merchant_id  label  prob  age_range  gender  \\\n1995   220293         2883    0.0   NaN        4.0     1.0   \n1996   155013         3727    0.0   NaN        2.0     0.0   \n1997    24453         1487    0.0   NaN        5.0     1.0   \n1998   155781         1861    0.0   NaN        0.0     2.0   \n1999    90501         3205    0.0   NaN        3.0     0.0   \n\n                                              item_path  \\\n1995  551995 497734 703172 468886 813265 1424 1424 1...   \n1996  881928 833279 961513 305410 305410 305410 1057...   \n1997  1049364 1049364 282508 251929 291529 166817 45...   \n1998  437216 1040600 305379 168779 463220 895836 305...   \n1999  966016 420078 463793 102071 402374 463793 1517...   \n\n                                               cat_path  \\\n1995  50 50 683 683 331 1401 1401 1401 1401 1401 737...   \n1996  737 737 737 1349 1349 1349 737 737 737 737 737...   \n1997  1075 1075 1213 1438 1200 407 1611 407 407 178 ...   \n1998  407 407 1438 1438 178 1505 656 612 1553 184 18...   \n1999  245 1656 720 720 125 720 1023 1023 177 177 177...   \n\n                                            seller_path  \\\n1995  638 2782 2884 192 2982 2982 2982 2982 2982 298...   \n1996  4390 3718 4993 702 702 702 3727 3727 3727 3727...   \n1997  3365 3365 1256 639 4016 4346 4346 4346 4346 43...   \n1998  3163 118 4211 4685 4645 3910 816 4364 1910 203...   \n1999  4600 4600 1071 3402 306 1071 206 344 2051 3155...   \n\n                                             brand_path  ... seller_most_1  \\\n1995  1444.0 84.0 7239.0 5638.0 5750.0 5750.0 5750.0...  ...          2677   \n1996  5223.0 3169.0 1865.0 1360.0 1360.0 1360.0 6816...  ...          3727   \n1997  7524.0 7524.0 1573.0 1565.0 4358.0 2919.0 2919...  ...          4346   \n1998  3881.0 6020.0 5242.0 7647.0 7837.0 5484.0 5784...  ...          1199   \n1999  404.0 1307.0 2100.0 4564.0 7592.0 2100.0 2512....  ...          1123   \n\n     cat_most_1  brand_most_1  action_type_most_1  seller_most_1_cnt  \\\n1995        656        1579.0                   0                 17   \n1996        737        6816.0                   0                 47   \n1997        407        2919.0                   0                 42   \n1998       1389        5409.0                   0                  5   \n1999        389        4029.0                   0                 11   \n\n      cat_most_1_cnt  brand_most_1_cnt  action_type_most_1_cnt  user_cnt_0  \\\n1995              14                17                      41          41   \n1996              51                47                     104         104   \n1997              25                42                      69          69   \n1998              32                 5                      55          55   \n1999               7                11                      40          40   \n\n      user_cnt_1  \n1995           0  \n1996           0  \n1997           0  \n1998           0  \n1999           0  \n\n[5 rows x 33 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>merchant_id</th>\n      <th>label</th>\n      <th>prob</th>\n      <th>age_range</th>\n      <th>gender</th>\n      <th>item_path</th>\n      <th>cat_path</th>\n      <th>seller_path</th>\n      <th>brand_path</th>\n      <th>...</th>\n      <th>seller_most_1</th>\n      <th>cat_most_1</th>\n      <th>brand_most_1</th>\n      <th>action_type_most_1</th>\n      <th>seller_most_1_cnt</th>\n      <th>cat_most_1_cnt</th>\n      <th>brand_most_1_cnt</th>\n      <th>action_type_most_1_cnt</th>\n      <th>user_cnt_0</th>\n      <th>user_cnt_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1995</th>\n      <td>220293</td>\n      <td>2883</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>551995 497734 703172 468886 813265 1424 1424 1...</td>\n      <td>50 50 683 683 331 1401 1401 1401 1401 1401 737...</td>\n      <td>638 2782 2884 192 2982 2982 2982 2982 2982 298...</td>\n      <td>1444.0 84.0 7239.0 5638.0 5750.0 5750.0 5750.0...</td>\n      <td>...</td>\n      <td>2677</td>\n      <td>656</td>\n      <td>1579.0</td>\n      <td>0</td>\n      <td>17</td>\n      <td>14</td>\n      <td>17</td>\n      <td>41</td>\n      <td>41</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>155013</td>\n      <td>3727</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>881928 833279 961513 305410 305410 305410 1057...</td>\n      <td>737 737 737 1349 1349 1349 737 737 737 737 737...</td>\n      <td>4390 3718 4993 702 702 702 3727 3727 3727 3727...</td>\n      <td>5223.0 3169.0 1865.0 1360.0 1360.0 1360.0 6816...</td>\n      <td>...</td>\n      <td>3727</td>\n      <td>737</td>\n      <td>6816.0</td>\n      <td>0</td>\n      <td>47</td>\n      <td>51</td>\n      <td>47</td>\n      <td>104</td>\n      <td>104</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>24453</td>\n      <td>1487</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>1049364 1049364 282508 251929 291529 166817 45...</td>\n      <td>1075 1075 1213 1438 1200 407 1611 407 407 178 ...</td>\n      <td>3365 3365 1256 639 4016 4346 4346 4346 4346 43...</td>\n      <td>7524.0 7524.0 1573.0 1565.0 4358.0 2919.0 2919...</td>\n      <td>...</td>\n      <td>4346</td>\n      <td>407</td>\n      <td>2919.0</td>\n      <td>0</td>\n      <td>42</td>\n      <td>25</td>\n      <td>42</td>\n      <td>69</td>\n      <td>69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>155781</td>\n      <td>1861</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>437216 1040600 305379 168779 463220 895836 305...</td>\n      <td>407 407 1438 1438 178 1505 656 612 1553 184 18...</td>\n      <td>3163 118 4211 4685 4645 3910 816 4364 1910 203...</td>\n      <td>3881.0 6020.0 5242.0 7647.0 7837.0 5484.0 5784...</td>\n      <td>...</td>\n      <td>1199</td>\n      <td>1389</td>\n      <td>5409.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>32</td>\n      <td>5</td>\n      <td>55</td>\n      <td>55</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>90501</td>\n      <td>3205</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>966016 420078 463793 102071 402374 463793 1517...</td>\n      <td>245 1656 720 720 125 720 1023 1023 177 177 177...</td>\n      <td>4600 4600 1071 3402 306 1071 206 344 2051 3155...</td>\n      <td>404.0 1307.0 2100.0 4564.0 7592.0 2100.0 2512....</td>\n      <td>...</td>\n      <td>1123</td>\n      <td>389</td>\n      <td>4029.0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>7</td>\n      <td>11</td>\n      <td>40</td>\n      <td>40</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 33 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# 特征组合\n",
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path', 'item_path'], '0', 'user_cnt_0')\n",
    "# 不同店铺数\n",
    "all_data_test = user_col_nunique(all_data_test, ['seller_path', 'item_path'], '0', 'seller_nunique_0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "             user_id  merchant_id        label  prob    age_range  \\\ncount    2000.000000  2000.000000  2000.000000   0.0  1995.000000   \nmean   208239.475500  2533.319000     0.064500   NaN     3.135840   \nstd    122331.596829  1457.359826     0.245703   NaN     1.889127   \nmin       132.000000    10.000000     0.000000   NaN     0.000000   \n25%    100930.500000  1329.500000     0.000000   NaN     2.000000   \n50%    206466.000000  2455.500000     0.000000   NaN     3.000000   \n75%    313024.500000  3910.000000     0.000000   NaN     4.000000   \nmax    424068.000000  4993.000000     1.000000   NaN     8.000000   \n\n            gender     user_cnt  seller_nunique  cat_nunique  brand_nunique  \\\ncount  1983.000000  2000.000000     2000.000000  2000.000000    2000.000000   \nmean      0.349975   140.082500       35.701500    23.987000      35.374000   \nstd       0.530178   202.303623       35.853021    18.091365      35.298146   \nmin       0.000000     2.000000        2.000000     1.000000       2.000000   \n25%       0.000000    39.000000       14.000000    12.000000      14.000000   \n50%       0.000000    84.000000       25.000000    20.000000      25.000000   \n75%       1.000000   163.000000       46.000000    31.000000      45.000000   \nmax       2.000000  2470.000000      344.000000   161.000000     320.000000   \n\n       ...  time_stamp_min  time_stamp_std  time_stamp_range  \\\ncount  ...     2000.000000     2000.000000        2000.00000   \nmean   ...        0.004000        0.743902           2.57300   \nstd    ...        0.089376        0.264871           0.50279   \nmin    ...        0.000000        0.000000           0.00000   \n25%    ...        0.000000        0.554763           2.00000   \n50%    ...        0.000000        0.706387           3.00000   \n75%    ...        0.000000        0.903796           3.00000   \nmax    ...        2.000000        1.475535           3.00000   \n\n       seller_most_1_cnt  cat_most_1_cnt  brand_most_1_cnt  \\\ncount        2000.000000      2000.00000       2000.000000   \nmean           29.575000        30.53200         29.844500   \nstd            46.845711        39.37676         47.037364   \nmin             1.000000         1.00000          1.000000   \n25%             9.000000        10.00000          9.000000   \n50%            16.000000        19.00000         16.000000   \n75%            32.250000        34.00000         32.000000   \nmax           966.000000       532.00000        966.000000   \n\n       action_type_most_1_cnt  user_cnt_0   user_cnt_1  seller_nunique_0  \ncount             2000.000000  2000.00000  2000.000000       2000.000000  \nmean               123.778000   123.59300     0.133000         77.216000  \nstd                186.462639   186.53453     0.662216        110.667629  \nmin                  2.000000     0.00000     0.000000          0.000000  \n25%                 32.000000    32.00000     0.000000         21.000000  \n50%                 70.000000    70.00000     0.000000         46.000000  \n75%                141.250000   141.00000     0.000000         89.000000  \nmax               2270.000000  2270.00000    12.000000       1446.000000  \n\n[8 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>merchant_id</th>\n      <th>label</th>\n      <th>prob</th>\n      <th>age_range</th>\n      <th>gender</th>\n      <th>user_cnt</th>\n      <th>seller_nunique</th>\n      <th>cat_nunique</th>\n      <th>brand_nunique</th>\n      <th>...</th>\n      <th>time_stamp_min</th>\n      <th>time_stamp_std</th>\n      <th>time_stamp_range</th>\n      <th>seller_most_1_cnt</th>\n      <th>cat_most_1_cnt</th>\n      <th>brand_most_1_cnt</th>\n      <th>action_type_most_1_cnt</th>\n      <th>user_cnt_0</th>\n      <th>user_cnt_1</th>\n      <th>seller_nunique_0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>0.0</td>\n      <td>1995.000000</td>\n      <td>1983.000000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>...</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>2000.00000</td>\n      <td>2000.000000</td>\n      <td>2000.00000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>2000.00000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>208239.475500</td>\n      <td>2533.319000</td>\n      <td>0.064500</td>\n      <td>NaN</td>\n      <td>3.135840</td>\n      <td>0.349975</td>\n      <td>140.082500</td>\n      <td>35.701500</td>\n      <td>23.987000</td>\n      <td>35.374000</td>\n      <td>...</td>\n      <td>0.004000</td>\n      <td>0.743902</td>\n      <td>2.57300</td>\n      <td>29.575000</td>\n      <td>30.53200</td>\n      <td>29.844500</td>\n      <td>123.778000</td>\n      <td>123.59300</td>\n      <td>0.133000</td>\n      <td>77.216000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>122331.596829</td>\n      <td>1457.359826</td>\n      <td>0.245703</td>\n      <td>NaN</td>\n      <td>1.889127</td>\n      <td>0.530178</td>\n      <td>202.303623</td>\n      <td>35.853021</td>\n      <td>18.091365</td>\n      <td>35.298146</td>\n      <td>...</td>\n      <td>0.089376</td>\n      <td>0.264871</td>\n      <td>0.50279</td>\n      <td>46.845711</td>\n      <td>39.37676</td>\n      <td>47.037364</td>\n      <td>186.462639</td>\n      <td>186.53453</td>\n      <td>0.662216</td>\n      <td>110.667629</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>132.000000</td>\n      <td>10.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>100930.500000</td>\n      <td>1329.500000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>39.000000</td>\n      <td>14.000000</td>\n      <td>12.000000</td>\n      <td>14.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.554763</td>\n      <td>2.00000</td>\n      <td>9.000000</td>\n      <td>10.00000</td>\n      <td>9.000000</td>\n      <td>32.000000</td>\n      <td>32.00000</td>\n      <td>0.000000</td>\n      <td>21.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>206466.000000</td>\n      <td>2455.500000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>84.000000</td>\n      <td>25.000000</td>\n      <td>20.000000</td>\n      <td>25.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.706387</td>\n      <td>3.00000</td>\n      <td>16.000000</td>\n      <td>19.00000</td>\n      <td>16.000000</td>\n      <td>70.000000</td>\n      <td>70.00000</td>\n      <td>0.000000</td>\n      <td>46.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>313024.500000</td>\n      <td>3910.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>163.000000</td>\n      <td>46.000000</td>\n      <td>31.000000</td>\n      <td>45.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.903796</td>\n      <td>3.00000</td>\n      <td>32.250000</td>\n      <td>34.00000</td>\n      <td>32.000000</td>\n      <td>141.250000</td>\n      <td>141.00000</td>\n      <td>0.000000</td>\n      <td>89.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>424068.000000</td>\n      <td>4993.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>8.000000</td>\n      <td>2.000000</td>\n      <td>2470.000000</td>\n      <td>344.000000</td>\n      <td>161.000000</td>\n      <td>320.000000</td>\n      <td>...</td>\n      <td>2.000000</td>\n      <td>1.475535</td>\n      <td>3.00000</td>\n      <td>966.000000</td>\n      <td>532.00000</td>\n      <td>966.000000</td>\n      <td>2270.000000</td>\n      <td>2270.00000</td>\n      <td>12.000000</td>\n      <td>1446.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 24 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['user_id', 'merchant_id', 'label', 'prob', 'age_range', 'gender',\n       'item_path', 'cat_path', 'seller_path', 'brand_path', 'time_stamp_path',\n       'action_type_path', 'user_cnt', 'seller_nunique', 'cat_nunique',\n       'brand_nunique', 'item_nunique', 'time_stamp_nunique',\n       'action_type_nunique', 'time_stamp_max', 'time_stamp_min',\n       'time_stamp_std', 'time_stamp_range', 'seller_most_1', 'cat_most_1',\n       'brand_most_1', 'action_type_most_1', 'seller_most_1_cnt',\n       'cat_most_1_cnt', 'brand_most_1_cnt', 'action_type_most_1_cnt',\n       'user_cnt_0', 'user_cnt_1', 'seller_nunique_0'],\n      dtype='object')"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3.7 利用CountVector 和 TF-IDF提取特征"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from scipy import sparse\n",
    "\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS,\n",
    "                           ngram_range=(1,1),\n",
    "                           max_features=100)\n",
    "\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "    tfidfVec.fit(all_data_test[col])\n",
    "    data_ = tfidfVec.transform(all_data_test[col])\n",
    "    if i == 0:\n",
    "        data_cat = data_\n",
    "    else:\n",
    "        data_cat = sparse.hstack((data_cat, data_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# 特征重命名和特征合并\n",
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]\n",
    "all_data_test = pd.concat([all_data_test, df_tfidf], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "      user_id  merchant_id  label  prob  age_range  gender  \\\n0       34176         3906    0.0   NaN        6.0     0.0   \n1       34176          121    0.0   NaN        6.0     0.0   \n2       34176         4356    1.0   NaN        6.0     0.0   \n3       34176         2217    0.0   NaN        6.0     0.0   \n4      230784         4818    0.0   NaN        0.0     0.0   \n...       ...          ...    ...   ...        ...     ...   \n1995   220293         2883    0.0   NaN        4.0     1.0   \n1996   155013         3727    0.0   NaN        2.0     0.0   \n1997    24453         1487    0.0   NaN        5.0     1.0   \n1998   155781         1861    0.0   NaN        0.0     2.0   \n1999    90501         3205    0.0   NaN        3.0     0.0   \n\n                                              item_path  \\\n0     581818 879005 581818 581818 1011673 52343 2773...   \n1     581818 879005 581818 581818 1011673 52343 2773...   \n2     581818 879005 581818 581818 1011673 52343 2773...   \n3     581818 879005 581818 581818 1011673 52343 2773...   \n4     191923 191923 191923 191923 964906 229470 2294...   \n...                                                 ...   \n1995  551995 497734 703172 468886 813265 1424 1424 1...   \n1996  881928 833279 961513 305410 305410 305410 1057...   \n1997  1049364 1049364 282508 251929 291529 166817 45...   \n1998  437216 1040600 305379 168779 463220 895836 305...   \n1999  966016 420078 463793 102071 402374 463793 1517...   \n\n                                               cat_path  \\\n0     1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n1     1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n2     1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n3     1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n4     1023 1023 1023 1023 662 664 664 1544 664 662 6...   \n...                                                 ...   \n1995  50 50 683 683 331 1401 1401 1401 1401 1401 737...   \n1996  737 737 737 1349 1349 1349 737 737 737 737 737...   \n1997  1075 1075 1213 1438 1200 407 1611 407 407 178 ...   \n1998  407 407 1438 1438 178 1505 656 612 1553 184 18...   \n1999  245 1656 720 720 125 720 1023 1023 177 177 177...   \n\n                                            seller_path  \\\n0     416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n1     416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n2     416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n3     416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n4     3545 3545 3545 3545 4566 2537 2537 2420 2537 4...   \n...                                                 ...   \n1995  638 2782 2884 192 2982 2982 2982 2982 2982 298...   \n1996  4390 3718 4993 702 702 702 3727 3727 3727 3727...   \n1997  3365 3365 1256 639 4016 4346 4346 4346 4346 43...   \n1998  3163 118 4211 4685 4645 3910 816 4364 1910 203...   \n1999  4600 4600 1071 3402 306 1071 206 344 2051 3155...   \n\n                                             brand_path  ... tfidf_90  \\\n0     4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...  ...      0.0   \n1     4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...  ...      0.0   \n2     4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...  ...      0.0   \n3     4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...  ...      0.0   \n4     5860.0 5860.0 5860.0 5860.0 6322.0 6066.0 6066...  ...      0.0   \n...                                                 ...  ...      ...   \n1995  1444.0 84.0 7239.0 5638.0 5750.0 5750.0 5750.0...  ...      0.0   \n1996  5223.0 3169.0 1865.0 1360.0 1360.0 1360.0 6816...  ...      0.0   \n1997  7524.0 7524.0 1573.0 1565.0 4358.0 2919.0 2919...  ...      0.0   \n1998  3881.0 6020.0 5242.0 7647.0 7837.0 5484.0 5784...  ...      0.0   \n1999  404.0 1307.0 2100.0 4564.0 7592.0 2100.0 2512....  ...      0.0   \n\n     tfidf_91  tfidf_92  tfidf_93  tfidf_94  tfidf_95  tfidf_96  tfidf_97  \\\n0         0.0  0.012666  0.011178       0.0       0.0       0.0       0.0   \n1         0.0  0.012666  0.011178       0.0       0.0       0.0       0.0   \n2         0.0  0.012666  0.011178       0.0       0.0       0.0       0.0   \n3         0.0  0.012666  0.011178       0.0       0.0       0.0       0.0   \n4         0.0  0.000000  0.000000       0.0       0.0       0.0       0.0   \n...       ...       ...       ...       ...       ...       ...       ...   \n1995      0.0  0.000000  0.000000       0.0       0.0       0.0       0.0   \n1996      0.0  0.060478  0.000000       0.0       0.0       0.0       0.0   \n1997      0.0  0.000000  0.000000       0.0       0.0       0.0       0.0   \n1998      0.0  0.000000  0.000000       0.0       0.0       0.0       0.0   \n1999      0.0  0.000000  0.000000       0.0       0.0       0.0       0.0   \n\n      tfidf_98  tfidf_99  \n0     0.012925  0.000000  \n1     0.012925  0.000000  \n2     0.012925  0.000000  \n3     0.012925  0.000000  \n4     0.000000  0.000000  \n...        ...       ...  \n1995  0.000000  0.000000  \n1996  0.000000  0.834207  \n1997  0.000000  0.000000  \n1998  0.000000  0.000000  \n1999  0.000000  0.000000  \n\n[2000 rows x 134 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>merchant_id</th>\n      <th>label</th>\n      <th>prob</th>\n      <th>age_range</th>\n      <th>gender</th>\n      <th>item_path</th>\n      <th>cat_path</th>\n      <th>seller_path</th>\n      <th>brand_path</th>\n      <th>...</th>\n      <th>tfidf_90</th>\n      <th>tfidf_91</th>\n      <th>tfidf_92</th>\n      <th>tfidf_93</th>\n      <th>tfidf_94</th>\n      <th>tfidf_95</th>\n      <th>tfidf_96</th>\n      <th>tfidf_97</th>\n      <th>tfidf_98</th>\n      <th>tfidf_99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34176</td>\n      <td>3906</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.012666</td>\n      <td>0.011178</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.012925</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>34176</td>\n      <td>121</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.012666</td>\n      <td>0.011178</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.012925</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34176</td>\n      <td>4356</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.012666</td>\n      <td>0.011178</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.012925</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>34176</td>\n      <td>2217</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.012666</td>\n      <td>0.011178</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.012925</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>230784</td>\n      <td>4818</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>191923 191923 191923 191923 964906 229470 2294...</td>\n      <td>1023 1023 1023 1023 662 664 664 1544 664 662 6...</td>\n      <td>3545 3545 3545 3545 4566 2537 2537 2420 2537 4...</td>\n      <td>5860.0 5860.0 5860.0 5860.0 6322.0 6066.0 6066...</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>220293</td>\n      <td>2883</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>551995 497734 703172 468886 813265 1424 1424 1...</td>\n      <td>50 50 683 683 331 1401 1401 1401 1401 1401 737...</td>\n      <td>638 2782 2884 192 2982 2982 2982 2982 2982 298...</td>\n      <td>1444.0 84.0 7239.0 5638.0 5750.0 5750.0 5750.0...</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>155013</td>\n      <td>3727</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>881928 833279 961513 305410 305410 305410 1057...</td>\n      <td>737 737 737 1349 1349 1349 737 737 737 737 737...</td>\n      <td>4390 3718 4993 702 702 702 3727 3727 3727 3727...</td>\n      <td>5223.0 3169.0 1865.0 1360.0 1360.0 1360.0 6816...</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.060478</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.834207</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>24453</td>\n      <td>1487</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>1049364 1049364 282508 251929 291529 166817 45...</td>\n      <td>1075 1075 1213 1438 1200 407 1611 407 407 178 ...</td>\n      <td>3365 3365 1256 639 4016 4346 4346 4346 4346 43...</td>\n      <td>7524.0 7524.0 1573.0 1565.0 4358.0 2919.0 2919...</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>155781</td>\n      <td>1861</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>437216 1040600 305379 168779 463220 895836 305...</td>\n      <td>407 407 1438 1438 178 1505 656 612 1553 184 18...</td>\n      <td>3163 118 4211 4685 4645 3910 816 4364 1910 203...</td>\n      <td>3881.0 6020.0 5242.0 7647.0 7837.0 5484.0 5784...</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>90501</td>\n      <td>3205</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>966016 420078 463793 102071 402374 463793 1517...</td>\n      <td>245 1656 720 720 125 720 1023 1023 177 177 177...</td>\n      <td>4600 4600 1071 3402 306 1071 206 344 2051 3155...</td>\n      <td>404.0 1307.0 2100.0 4564.0 7592.0 2100.0 2512....</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 134 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3.8 嵌入特征"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# pip install gensim==3.7.0\n",
    "# notice: pip install smart-open==2.1\n",
    "import gensim\n",
    "# Train Word2Vec model\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    all_data_test['seller_path'].apply(lambda x: x.split(' ')),\n",
    "    size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=4\n",
    ")\n",
    "# model.save('product2vec.model')\n",
    "# model = gensim.models.Word2Vec.load('product2vec.model')\n",
    "\n",
    "def mean_w2v_(x, model, size=100):\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            if word in model.wv.vocab:\n",
    "                i += 1\n",
    "                if i == 1:\n",
    "                    vec = np.zeros(size)\n",
    "                vec += model.wv[word]\n",
    "        return vec / i\n",
    "    except:\n",
    "        return np.zeros(size)\n",
    "\n",
    "def get_mean_w2v(df_data, columns, model, size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v_(row[columns], model, size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embedding = get_mean_w2v(all_data_test, 'seller_path', model, 100)\n",
    "df_embedding.columns = ['embedding_' + str(i) for i in df_embedding.columns]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "      user_id  merchant_id  label  prob  age_range  gender  \\\n0       34176         3906    0.0   NaN        6.0     0.0   \n1       34176          121    0.0   NaN        6.0     0.0   \n2       34176         4356    1.0   NaN        6.0     0.0   \n3       34176         2217    0.0   NaN        6.0     0.0   \n4      230784         4818    0.0   NaN        0.0     0.0   \n...       ...          ...    ...   ...        ...     ...   \n1995   220293         2883    0.0   NaN        4.0     1.0   \n1996   155013         3727    0.0   NaN        2.0     0.0   \n1997    24453         1487    0.0   NaN        5.0     1.0   \n1998   155781         1861    0.0   NaN        0.0     2.0   \n1999    90501         3205    0.0   NaN        3.0     0.0   \n\n                                              item_path  \\\n0     581818 879005 581818 581818 1011673 52343 2773...   \n1     581818 879005 581818 581818 1011673 52343 2773...   \n2     581818 879005 581818 581818 1011673 52343 2773...   \n3     581818 879005 581818 581818 1011673 52343 2773...   \n4     191923 191923 191923 191923 964906 229470 2294...   \n...                                                 ...   \n1995  551995 497734 703172 468886 813265 1424 1424 1...   \n1996  881928 833279 961513 305410 305410 305410 1057...   \n1997  1049364 1049364 282508 251929 291529 166817 45...   \n1998  437216 1040600 305379 168779 463220 895836 305...   \n1999  966016 420078 463793 102071 402374 463793 1517...   \n\n                                               cat_path  \\\n0     1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n1     1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n2     1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n3     1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n4     1023 1023 1023 1023 662 664 664 1544 664 662 6...   \n...                                                 ...   \n1995  50 50 683 683 331 1401 1401 1401 1401 1401 737...   \n1996  737 737 737 1349 1349 1349 737 737 737 737 737...   \n1997  1075 1075 1213 1438 1200 407 1611 407 407 178 ...   \n1998  407 407 1438 1438 178 1505 656 612 1553 184 18...   \n1999  245 1656 720 720 125 720 1023 1023 177 177 177...   \n\n                                            seller_path  \\\n0     416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n1     416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n2     416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n3     416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n4     3545 3545 3545 3545 4566 2537 2537 2420 2537 4...   \n...                                                 ...   \n1995  638 2782 2884 192 2982 2982 2982 2982 2982 298...   \n1996  4390 3718 4993 702 702 702 3727 3727 3727 3727...   \n1997  3365 3365 1256 639 4016 4346 4346 4346 4346 43...   \n1998  3163 118 4211 4685 4645 3910 816 4364 1910 203...   \n1999  4600 4600 1071 3402 306 1071 206 344 2051 3155...   \n\n                                             brand_path  ... embedding_90  \\\n0     4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...  ...    -0.104695   \n1     4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...  ...    -0.104695   \n2     4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...  ...    -0.104695   \n3     4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...  ...    -0.104695   \n4     5860.0 5860.0 5860.0 5860.0 6322.0 6066.0 6066...  ...    -0.001398   \n...                                                 ...  ...          ...   \n1995  1444.0 84.0 7239.0 5638.0 5750.0 5750.0 5750.0...  ...     0.373496   \n1996  5223.0 3169.0 1865.0 1360.0 1360.0 1360.0 6816...  ...    -0.402963   \n1997  7524.0 7524.0 1573.0 1565.0 4358.0 2919.0 2919...  ...    -0.279366   \n1998  3881.0 6020.0 5242.0 7647.0 7837.0 5484.0 5784...  ...    -0.388383   \n1999  404.0 1307.0 2100.0 4564.0 7592.0 2100.0 2512....  ...    -0.319852   \n\n     embedding_91  embedding_92  embedding_93  embedding_94  embedding_95  \\\n0        0.290655     -0.637096     -0.383884     -0.146233     -0.402715   \n1        0.290655     -0.637096     -0.383884     -0.146233     -0.402715   \n2        0.290655     -0.637096     -0.383884     -0.146233     -0.402715   \n3        0.290655     -0.637096     -0.383884     -0.146233     -0.402715   \n4       -0.264722     -0.422136     -0.681098     -0.404202      0.412855   \n...           ...           ...           ...           ...           ...   \n1995     0.685982      0.393666     -0.553947     -0.841818      0.543849   \n1996     0.536334     -0.549780     -0.912703     -0.508195     -0.344142   \n1997    -0.167434     -0.129694     -1.000930     -0.835965      0.722479   \n1998    -0.007221     -0.020025     -0.164158     -0.114593      0.162273   \n1999    -0.041753      0.056271     -0.137945     -0.232775      0.114132   \n\n      embedding_96  embedding_97  embedding_98  embedding_99  \n0        -0.524051      0.234201      0.277032      0.104220  \n1        -0.524051      0.234201      0.277032      0.104220  \n2        -0.524051      0.234201      0.277032      0.104220  \n3        -0.524051      0.234201      0.277032      0.104220  \n4        -0.464492      0.248214      0.050709     -0.263386  \n...            ...           ...           ...           ...  \n1995      0.078304      0.184404      0.839376      0.393104  \n1996     -1.013008     -0.265699      1.049977      0.200655  \n1997     -0.660830      0.285251     -0.086839     -0.102232  \n1998     -0.182515      0.019234      0.239157      0.110111  \n1999     -0.138295      0.074171      0.237254      0.191675  \n\n[2000 rows x 234 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>merchant_id</th>\n      <th>label</th>\n      <th>prob</th>\n      <th>age_range</th>\n      <th>gender</th>\n      <th>item_path</th>\n      <th>cat_path</th>\n      <th>seller_path</th>\n      <th>brand_path</th>\n      <th>...</th>\n      <th>embedding_90</th>\n      <th>embedding_91</th>\n      <th>embedding_92</th>\n      <th>embedding_93</th>\n      <th>embedding_94</th>\n      <th>embedding_95</th>\n      <th>embedding_96</th>\n      <th>embedding_97</th>\n      <th>embedding_98</th>\n      <th>embedding_99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34176</td>\n      <td>3906</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n      <td>...</td>\n      <td>-0.104695</td>\n      <td>0.290655</td>\n      <td>-0.637096</td>\n      <td>-0.383884</td>\n      <td>-0.146233</td>\n      <td>-0.402715</td>\n      <td>-0.524051</td>\n      <td>0.234201</td>\n      <td>0.277032</td>\n      <td>0.104220</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>34176</td>\n      <td>121</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n      <td>...</td>\n      <td>-0.104695</td>\n      <td>0.290655</td>\n      <td>-0.637096</td>\n      <td>-0.383884</td>\n      <td>-0.146233</td>\n      <td>-0.402715</td>\n      <td>-0.524051</td>\n      <td>0.234201</td>\n      <td>0.277032</td>\n      <td>0.104220</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34176</td>\n      <td>4356</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n      <td>...</td>\n      <td>-0.104695</td>\n      <td>0.290655</td>\n      <td>-0.637096</td>\n      <td>-0.383884</td>\n      <td>-0.146233</td>\n      <td>-0.402715</td>\n      <td>-0.524051</td>\n      <td>0.234201</td>\n      <td>0.277032</td>\n      <td>0.104220</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>34176</td>\n      <td>2217</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n      <td>...</td>\n      <td>-0.104695</td>\n      <td>0.290655</td>\n      <td>-0.637096</td>\n      <td>-0.383884</td>\n      <td>-0.146233</td>\n      <td>-0.402715</td>\n      <td>-0.524051</td>\n      <td>0.234201</td>\n      <td>0.277032</td>\n      <td>0.104220</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>230784</td>\n      <td>4818</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>191923 191923 191923 191923 964906 229470 2294...</td>\n      <td>1023 1023 1023 1023 662 664 664 1544 664 662 6...</td>\n      <td>3545 3545 3545 3545 4566 2537 2537 2420 2537 4...</td>\n      <td>5860.0 5860.0 5860.0 5860.0 6322.0 6066.0 6066...</td>\n      <td>...</td>\n      <td>-0.001398</td>\n      <td>-0.264722</td>\n      <td>-0.422136</td>\n      <td>-0.681098</td>\n      <td>-0.404202</td>\n      <td>0.412855</td>\n      <td>-0.464492</td>\n      <td>0.248214</td>\n      <td>0.050709</td>\n      <td>-0.263386</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>220293</td>\n      <td>2883</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>551995 497734 703172 468886 813265 1424 1424 1...</td>\n      <td>50 50 683 683 331 1401 1401 1401 1401 1401 737...</td>\n      <td>638 2782 2884 192 2982 2982 2982 2982 2982 298...</td>\n      <td>1444.0 84.0 7239.0 5638.0 5750.0 5750.0 5750.0...</td>\n      <td>...</td>\n      <td>0.373496</td>\n      <td>0.685982</td>\n      <td>0.393666</td>\n      <td>-0.553947</td>\n      <td>-0.841818</td>\n      <td>0.543849</td>\n      <td>0.078304</td>\n      <td>0.184404</td>\n      <td>0.839376</td>\n      <td>0.393104</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>155013</td>\n      <td>3727</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>881928 833279 961513 305410 305410 305410 1057...</td>\n      <td>737 737 737 1349 1349 1349 737 737 737 737 737...</td>\n      <td>4390 3718 4993 702 702 702 3727 3727 3727 3727...</td>\n      <td>5223.0 3169.0 1865.0 1360.0 1360.0 1360.0 6816...</td>\n      <td>...</td>\n      <td>-0.402963</td>\n      <td>0.536334</td>\n      <td>-0.549780</td>\n      <td>-0.912703</td>\n      <td>-0.508195</td>\n      <td>-0.344142</td>\n      <td>-1.013008</td>\n      <td>-0.265699</td>\n      <td>1.049977</td>\n      <td>0.200655</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>24453</td>\n      <td>1487</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>1049364 1049364 282508 251929 291529 166817 45...</td>\n      <td>1075 1075 1213 1438 1200 407 1611 407 407 178 ...</td>\n      <td>3365 3365 1256 639 4016 4346 4346 4346 4346 43...</td>\n      <td>7524.0 7524.0 1573.0 1565.0 4358.0 2919.0 2919...</td>\n      <td>...</td>\n      <td>-0.279366</td>\n      <td>-0.167434</td>\n      <td>-0.129694</td>\n      <td>-1.000930</td>\n      <td>-0.835965</td>\n      <td>0.722479</td>\n      <td>-0.660830</td>\n      <td>0.285251</td>\n      <td>-0.086839</td>\n      <td>-0.102232</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>155781</td>\n      <td>1861</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>437216 1040600 305379 168779 463220 895836 305...</td>\n      <td>407 407 1438 1438 178 1505 656 612 1553 184 18...</td>\n      <td>3163 118 4211 4685 4645 3910 816 4364 1910 203...</td>\n      <td>3881.0 6020.0 5242.0 7647.0 7837.0 5484.0 5784...</td>\n      <td>...</td>\n      <td>-0.388383</td>\n      <td>-0.007221</td>\n      <td>-0.020025</td>\n      <td>-0.164158</td>\n      <td>-0.114593</td>\n      <td>0.162273</td>\n      <td>-0.182515</td>\n      <td>0.019234</td>\n      <td>0.239157</td>\n      <td>0.110111</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>90501</td>\n      <td>3205</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>966016 420078 463793 102071 402374 463793 1517...</td>\n      <td>245 1656 720 720 125 720 1023 1023 177 177 177...</td>\n      <td>4600 4600 1071 3402 306 1071 206 344 2051 3155...</td>\n      <td>404.0 1307.0 2100.0 4564.0 7592.0 2100.0 2512....</td>\n      <td>...</td>\n      <td>-0.319852</td>\n      <td>-0.041753</td>\n      <td>0.056271</td>\n      <td>-0.137945</td>\n      <td>-0.232775</td>\n      <td>0.114132</td>\n      <td>-0.138295</td>\n      <td>0.074171</td>\n      <td>0.237254</td>\n      <td>0.191675</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 234 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 嵌入特征和原始特征合并：\n",
    "all_data_test = pd.concat([all_data_test, df_embedding], axis=1)\n",
    "all_data_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3.9 Stacking分类特征"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# 1. Stacking特征工具包\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# 2. 定义Stacking分类特征相关函数\n",
    "def stacking_clf(clf, train_x, train_y, test_x, clf_name, kf, label_split=None):\n",
    "    train = np.zeros((train_x.shape[0], 1))\n",
    "    test = np.zeros((test_x.shape[0], 1))\n",
    "    test_pre = np.empty((folds, test_x.shape[0], 1))\n",
    "    cv_scores = []\n",
    "    for i ,(train_index, test_index) in enumerate(kf.split(train_x, label_split)):\n",
    "        tr_x = train_x[train_index]\n",
    "        tr_y = train_y[train_index]\n",
    "        te_x = train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "\n",
    "        if clf_name in ['rf', 'ada', 'gb', 'et', 'lr', 'knn', 'gnb']:\n",
    "            clf.fit(tr_x, tr_y)\n",
    "            pre = clf.predict(te_x)\n",
    "            train[test_index] = pre[:,0].reshape(-1,1)\n",
    "            test_pre[i, :] = clf.predict_proba(test_x)[:,0].reshape(-1, 1)\n",
    "            cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in ['xgb']:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            # te_y -> train_y\n",
    "            z = clf.DMatrix(test_x, label=train_y, missing=-1)\n",
    "            params = {\n",
    "                'booster': 'gbtree',\n",
    "                'objective': 'multi:softprob',\n",
    "                'eval_metric': 'mlogloss',\n",
    "                'gamma': 1,\n",
    "                'min_child_weight': 1.5,\n",
    "                'max_depth': 5,\n",
    "                'lambda': 10,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                'eta': 0.03,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2021,\n",
    "                'num_class': 2\n",
    "            }\n",
    "\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'), (test_matrix, 'eval')]\n",
    "            if test_matrix:\n",
    "                model = clf.train(\n",
    "                    params,\n",
    "                    train_matrix,\n",
    "                    num_boost_round=num_round,\n",
    "                    evals=watchlist,\n",
    "                    early_stopping_rounds=early_stopping_rounds\n",
    "                )\n",
    "                pre = model.predict(\n",
    "                    test_matrix,\n",
    "                    ntree_limit=model.best_ntree_limit\n",
    "                )\n",
    "                train[test_index] = pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :] = model.predict(\n",
    "                    z, ntree_limit=model.best_ntree_limit\n",
    "                )[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in ['lgb']:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'multiclass',\n",
    "                'metric': 'multi_logloss',\n",
    "                'min_child_weight': 1.5,\n",
    "                'num_leaves': 2**5,\n",
    "                'lambda_l2': 10,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                'learning_rate': 0.03,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2021,\n",
    "                'num_class': 2,\n",
    "                'silent': True\n",
    "            }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            # watchlist = [(train_matrix, 'train'), (test_matrix, 'eval')]\n",
    "            if test_matrix:\n",
    "                model = clf.train(\n",
    "                    params,\n",
    "                    train_matrix,\n",
    "                    num_round,\n",
    "                    valid_sets=test_matrix,\n",
    "                    early_stopping_rounds=early_stopping_rounds\n",
    "                )\n",
    "                pre = model.predict(\n",
    "                    te_x,\n",
    "                    num_iteration=model.best_iteration\n",
    "                )\n",
    "                train[test_index] = pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :] = model.predict(\n",
    "                    test_x, num_iteration=model.best_iteration\n",
    "                )[:,0].reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre[:,0].reshape(-1,1)))\n",
    "        else:\n",
    "            raise IOError('Please add new clf.')\n",
    "        print('%s now score is:' %clf_name, cv_scores)\n",
    "    test[:] = test_pre.mean(axis=0)\n",
    "    print('%s_score_list:' %clf_name, cv_scores)\n",
    "    print('%s_score_mean:' %clf_name, np.mean(cv_scores))\n",
    "    return train.reshape(-1,1), test.reshape(-1,1)\n",
    "\n",
    "\n",
    "def rf_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestClassifier(\n",
    "        n_estimators=1200,\n",
    "        max_depth=20,\n",
    "        n_jobs=-1,\n",
    "        random_state=2021,\n",
    "        max_features='auto',\n",
    "        verbose=1\n",
    "    )\n",
    "    rf_train, rf_test = stacking_clf(\n",
    "        randomforest,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_valid,\n",
    "        'rf',\n",
    "        kf,\n",
    "        label_split=label_split\n",
    "    )\n",
    "\n",
    "    return rf_train, rf_test, 'rf'\n",
    "\n",
    "def ada_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(\n",
    "        n_estimators=50,\n",
    "        random_state=2021,\n",
    "        learning_rate=0.01\n",
    "    )\n",
    "    ada_train, ada_test = stacking_clf(\n",
    "        adaboost,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_valid,\n",
    "        'ada',\n",
    "        kf,\n",
    "        label_split=label_split\n",
    "    )\n",
    "\n",
    "    return ada_train, ada_test, 'ada'\n",
    "\n",
    "def gb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(\n",
    "        learning_rate=0.04,\n",
    "        n_estimators=100,\n",
    "        subsample=0.8,\n",
    "        random_state=2021,\n",
    "        max_depth=5,\n",
    "        verbose=1\n",
    "    )\n",
    "    gbdt_train, gbdt_test = stacking_clf(\n",
    "        gbdt,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_valid,\n",
    "        'gb',\n",
    "        kf,\n",
    "        label_split=label_split\n",
    "    )\n",
    "\n",
    "    return gbdt_train, gbdt_test, 'gb'\n",
    "\n",
    "def et_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesClassifier(\n",
    "        n_estimators=1200,\n",
    "        max_depth=35,\n",
    "        n_jobs=-1,\n",
    "        random_state=2021,\n",
    "        max_features='auto',\n",
    "        verbose=1\n",
    "    )\n",
    "    et_train, et_test = stacking_clf(\n",
    "        extratree,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_valid,\n",
    "        'et',\n",
    "        kf,\n",
    "        label_split=label_split\n",
    "    )\n",
    "\n",
    "    return et_train, et_test, 'et'\n",
    "\n",
    "def xgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(\n",
    "        xgboost,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_valid,\n",
    "        'xgb',\n",
    "        kf,\n",
    "        label_split=label_split\n",
    "    )\n",
    "\n",
    "    return xgb_train, xgb_test, 'xgb'\n",
    "\n",
    "def lgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lgb_train, lgb_test = stacking_clf(\n",
    "        lightgbm,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_valid,\n",
    "        'lgb',\n",
    "        kf,\n",
    "        label_split=label_split\n",
    "    )\n",
    "\n",
    "    return lgb_train, lgb_test, 'lgb'\n",
    "\n",
    "def gnb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gnb = GaussianNB()\n",
    "    gnb_train, gnb_test = stacking_clf(\n",
    "        gnb,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_valid,\n",
    "        'gnb',\n",
    "        kf,\n",
    "        label_split=label_split\n",
    "    )\n",
    "\n",
    "    return gnb_train, gnb_test, 'gnb'\n",
    "\n",
    "def lr_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    logisticregression = LogisticRegression(\n",
    "        n_jobs=-1,\n",
    "        random_state=2021,\n",
    "        C=0.1,\n",
    "        max_iter=200\n",
    "    )\n",
    "    lr_train, lr_test = stacking_clf(\n",
    "        logisticregression,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_valid,\n",
    "        'lr',\n",
    "        kf,\n",
    "        label_split=label_split\n",
    "    )\n",
    "\n",
    "    return lr_train, lr_test, 'lr'\n",
    "\n",
    "def knn_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    kneighbors = KNeighborsClassifier(\n",
    "        n_neighbors=200,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    knn_train, knn_test = stacking_clf(\n",
    "        kneighbors,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_valid,\n",
    "        'knn',\n",
    "        kf,\n",
    "        label_split=label_split\n",
    "    )\n",
    "\n",
    "    return knn_train, knn_test, 'knn'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# 3. 读取训练数据和验证数据\n",
    "features_columns = [\n",
    "    c for c in all_data_test.columns if c not in [\n",
    "        'label','prob','seller_path','cat_path','brand_path','action_type_path','item_path','time_stamp_path'\n",
    "    ]\n",
    "]\n",
    "x_train = all_data_test[\n",
    "    ~all_data_test['label'].isna()][features_columns].values\n",
    "\n",
    "y_train = all_data_test[\n",
    "    ~all_data_test['label'].isna()]['label'].values\n",
    "\n",
    "x_valid = all_data_test[\n",
    "    all_data_test['label'].isna()][features_columns].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def get_matrix(data):\n",
    "    where_are_nan = np.isnan(data)\n",
    "    where_are_inf = np.isinf(data)\n",
    "    data[where_are_nan] = 0\n",
    "    data[where_are_inf] = 0\n",
    "    return data\n",
    "x_train = np.float_(get_matrix(np.float_(x_train)))\n",
    "y_train = np.int_(y_train)\n",
    "x_valid = x_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# 4. 使用lgb和xgb分类模型构造Stacking特征\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "folds = 5\n",
    "seed = 1\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# 选择lgb xgb作为基模型\n",
    "clf_list = [lgb_clf, xgb_clf]\n",
    "clf_list_col = ['lgb_clf', 'xgb_clf']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31308\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 223\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Info] Start training from score -0.066541\n",
      "[LightGBM] [Info] Start training from score -2.743030\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 0.240456\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 0.240268\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's multi_logloss: 0.240007\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.23997\n",
      "[5]\tvalid_0's multi_logloss: 0.239757\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.239495\n",
      "[7]\tvalid_0's multi_logloss: 0.239338\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 0.239388\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's multi_logloss: 0.239434\n",
      "[10]\tvalid_0's multi_logloss: 0.239262\n",
      "[11]\tvalid_0's multi_logloss: 0.239154\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's multi_logloss: 0.239194\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's multi_logloss: 0.239412\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's multi_logloss: 0.239434\n",
      "[15]\tvalid_0's multi_logloss: 0.239492\n",
      "[16]\tvalid_0's multi_logloss: 0.239645\n",
      "[17]\tvalid_0's multi_logloss: 0.239853\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's multi_logloss: 0.240116\n",
      "[19]\tvalid_0's multi_logloss: 0.240031\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's multi_logloss: 0.240023\n",
      "[21]\tvalid_0's multi_logloss: 0.240024\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's multi_logloss: 0.239924\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's multi_logloss: 0.239849\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tvalid_0's multi_logloss: 0.239917\n",
      "[25]\tvalid_0's multi_logloss: 0.239835\n",
      "[26]\tvalid_0's multi_logloss: 0.239927\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's multi_logloss: 0.240059\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's multi_logloss: 0.240118\n",
      "[29]\tvalid_0's multi_logloss: 0.240049\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's multi_logloss: 0.239955\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's multi_logloss: 0.240075\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\tvalid_0's multi_logloss: 0.24036\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\tvalid_0's multi_logloss: 0.240608\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\tvalid_0's multi_logloss: 0.240582\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's multi_logloss: 0.240627\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tvalid_0's multi_logloss: 0.240505\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\tvalid_0's multi_logloss: 0.240716\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tvalid_0's multi_logloss: 0.240975\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\tvalid_0's multi_logloss: 0.240953\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tvalid_0's multi_logloss: 0.241108\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tvalid_0's multi_logloss: 0.241329\n",
      "[42]\tvalid_0's multi_logloss: 0.241342\n",
      "[43]\tvalid_0's multi_logloss: 0.241238\n",
      "[44]\tvalid_0's multi_logloss: 0.241321\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's multi_logloss: 0.241614\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\tvalid_0's multi_logloss: 0.241863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's multi_logloss: 0.241889\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's multi_logloss: 0.242066\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\tvalid_0's multi_logloss: 0.242324\n",
      "[50]\tvalid_0's multi_logloss: 0.24261\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\tvalid_0's multi_logloss: 0.242493\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tvalid_0's multi_logloss: 0.242514\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's multi_logloss: 0.242528\n",
      "[54]\tvalid_0's multi_logloss: 0.242539\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\tvalid_0's multi_logloss: 0.242532\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\tvalid_0's multi_logloss: 0.242501\n",
      "[57]\tvalid_0's multi_logloss: 0.242769\n",
      "[58]\tvalid_0's multi_logloss: 0.242837\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\tvalid_0's multi_logloss: 0.242893\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\tvalid_0's multi_logloss: 0.243046\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\tvalid_0's multi_logloss: 0.243578\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\tvalid_0's multi_logloss: 0.243775\n",
      "[63]\tvalid_0's multi_logloss: 0.243724\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\tvalid_0's multi_logloss: 0.243728\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\tvalid_0's multi_logloss: 0.244023\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\tvalid_0's multi_logloss: 0.244064\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[67]\tvalid_0's multi_logloss: 0.244269\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[68]\tvalid_0's multi_logloss: 0.244548\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\tvalid_0's multi_logloss: 0.244689\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\tvalid_0's multi_logloss: 0.245014\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\tvalid_0's multi_logloss: 0.245169\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\tvalid_0's multi_logloss: 0.245215\n",
      "[73]\tvalid_0's multi_logloss: 0.245592\n",
      "[74]\tvalid_0's multi_logloss: 0.24568\n",
      "[75]\tvalid_0's multi_logloss: 0.245799\n",
      "[76]\tvalid_0's multi_logloss: 0.245839\n",
      "[77]\tvalid_0's multi_logloss: 0.246121\n",
      "[78]\tvalid_0's multi_logloss: 0.246236\n",
      "[79]\tvalid_0's multi_logloss: 0.246164\n",
      "[80]\tvalid_0's multi_logloss: 0.246463\n",
      "[81]\tvalid_0's multi_logloss: 0.246693\n",
      "[82]\tvalid_0's multi_logloss: 0.246955\n",
      "[83]\tvalid_0's multi_logloss: 0.247025\n",
      "[84]\tvalid_0's multi_logloss: 0.247169\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[85]\tvalid_0's multi_logloss: 0.247489\n",
      "[86]\tvalid_0's multi_logloss: 0.247586\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[87]\tvalid_0's multi_logloss: 0.247698\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\tvalid_0's multi_logloss: 0.247701\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[89]\tvalid_0's multi_logloss: 0.248084\n",
      "[90]\tvalid_0's multi_logloss: 0.248182\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[91]\tvalid_0's multi_logloss: 0.248683\n",
      "[92]\tvalid_0's multi_logloss: 0.24893\n",
      "[93]\tvalid_0's multi_logloss: 0.249254\n",
      "[94]\tvalid_0's multi_logloss: 0.249371\n",
      "[95]\tvalid_0's multi_logloss: 0.249559\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[96]\tvalid_0's multi_logloss: 0.24971\n",
      "[97]\tvalid_0's multi_logloss: 0.250128\n",
      "[98]\tvalid_0's multi_logloss: 0.250233\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\tvalid_0's multi_logloss: 0.250422\n",
      "[100]\tvalid_0's multi_logloss: 0.250679\n",
      "[101]\tvalid_0's multi_logloss: 0.25072\n",
      "[102]\tvalid_0's multi_logloss: 0.251091\n",
      "[103]\tvalid_0's multi_logloss: 0.251557\n",
      "[104]\tvalid_0's multi_logloss: 0.251661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[105]\tvalid_0's multi_logloss: 0.252047\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[106]\tvalid_0's multi_logloss: 0.252224\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[107]\tvalid_0's multi_logloss: 0.252196\n",
      "[108]\tvalid_0's multi_logloss: 0.252466\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[109]\tvalid_0's multi_logloss: 0.252858\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[110]\tvalid_0's multi_logloss: 0.253074\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[111]\tvalid_0's multi_logloss: 0.25317\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's multi_logloss: 0.239154\n",
      "lgb now score is: [0.8236879671053865]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31416\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 224\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Info] Start training from score -0.062541\n",
      "[LightGBM] [Info] Start training from score -2.803048\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 0.282011\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 0.282511\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's multi_logloss: 0.282545\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.28251\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's multi_logloss: 0.282903\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.283047\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's multi_logloss: 0.283592\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 0.283866\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's multi_logloss: 0.284468\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's multi_logloss: 0.284726\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 0.284714\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's multi_logloss: 0.284921\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's multi_logloss: 0.285235\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's multi_logloss: 0.285596\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's multi_logloss: 0.286207\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's multi_logloss: 0.28667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's multi_logloss: 0.286914\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's multi_logloss: 0.287367\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's multi_logloss: 0.288124\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's multi_logloss: 0.288816\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tvalid_0's multi_logloss: 0.28921\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's multi_logloss: 0.289834\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's multi_logloss: 0.290483\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tvalid_0's multi_logloss: 0.291005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\tvalid_0's multi_logloss: 0.291746\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tvalid_0's multi_logloss: 0.292542\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's multi_logloss: 0.292973\n",
      "[28]\tvalid_0's multi_logloss: 0.293117\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\tvalid_0's multi_logloss: 0.293508\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's multi_logloss: 0.293809\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's multi_logloss: 0.294395\n",
      "[32]\tvalid_0's multi_logloss: 0.29448\n",
      "[33]\tvalid_0's multi_logloss: 0.295078\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\tvalid_0's multi_logloss: 0.295313\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's multi_logloss: 0.295359\n",
      "[36]\tvalid_0's multi_logloss: 0.295684\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\tvalid_0's multi_logloss: 0.296026\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tvalid_0's multi_logloss: 0.296253\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\tvalid_0's multi_logloss: 0.296628\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tvalid_0's multi_logloss: 0.297119\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tvalid_0's multi_logloss: 0.297086\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\tvalid_0's multi_logloss: 0.297195\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\tvalid_0's multi_logloss: 0.29766\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\tvalid_0's multi_logloss: 0.297885\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's multi_logloss: 0.298292\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\tvalid_0's multi_logloss: 0.298709\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's multi_logloss: 0.299091\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's multi_logloss: 0.299431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\tvalid_0's multi_logloss: 0.299479\n",
      "[50]\tvalid_0's multi_logloss: 0.299615\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\tvalid_0's multi_logloss: 0.300193\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tvalid_0's multi_logloss: 0.300666\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's multi_logloss: 0.3009\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tvalid_0's multi_logloss: 0.301425\n",
      "[55]\tvalid_0's multi_logloss: 0.301585\n",
      "[56]\tvalid_0's multi_logloss: 0.301942\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\tvalid_0's multi_logloss: 0.302593\n",
      "[58]\tvalid_0's multi_logloss: 0.302645\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\tvalid_0's multi_logloss: 0.303032\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\tvalid_0's multi_logloss: 0.303514\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\tvalid_0's multi_logloss: 0.303873\n",
      "[62]\tvalid_0's multi_logloss: 0.304361\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[63]\tvalid_0's multi_logloss: 0.304969\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\tvalid_0's multi_logloss: 0.30553\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\tvalid_0's multi_logloss: 0.305993\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\tvalid_0's multi_logloss: 0.306216\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[67]\tvalid_0's multi_logloss: 0.306735\n",
      "[68]\tvalid_0's multi_logloss: 0.307251\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\tvalid_0's multi_logloss: 0.307902\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\tvalid_0's multi_logloss: 0.308447\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\tvalid_0's multi_logloss: 0.308964\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\tvalid_0's multi_logloss: 0.309707\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\tvalid_0's multi_logloss: 0.310426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\tvalid_0's multi_logloss: 0.310616\n",
      "[75]\tvalid_0's multi_logloss: 0.311181\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\tvalid_0's multi_logloss: 0.311651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\tvalid_0's multi_logloss: 0.312149\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\tvalid_0's multi_logloss: 0.312525\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\tvalid_0's multi_logloss: 0.313033\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\tvalid_0's multi_logloss: 0.313581\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\tvalid_0's multi_logloss: 0.313715\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\tvalid_0's multi_logloss: 0.314106\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\tvalid_0's multi_logloss: 0.314931\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[84]\tvalid_0's multi_logloss: 0.315238\n",
      "[85]\tvalid_0's multi_logloss: 0.315716\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\tvalid_0's multi_logloss: 0.316087\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[87]\tvalid_0's multi_logloss: 0.316487\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\tvalid_0's multi_logloss: 0.316793\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[89]\tvalid_0's multi_logloss: 0.317093\n",
      "[90]\tvalid_0's multi_logloss: 0.317471\n",
      "[91]\tvalid_0's multi_logloss: 0.318126\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[92]\tvalid_0's multi_logloss: 0.318675\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[93]\tvalid_0's multi_logloss: 0.319268\n",
      "[94]\tvalid_0's multi_logloss: 0.319684\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[95]\tvalid_0's multi_logloss: 0.320363\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[96]\tvalid_0's multi_logloss: 0.320833\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[97]\tvalid_0's multi_logloss: 0.321429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[98]\tvalid_0's multi_logloss: 0.321803\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\tvalid_0's multi_logloss: 0.322198\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's multi_logloss: 0.322317\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[101]\tvalid_0's multi_logloss: 0.322923\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 0.282011\n",
      "lgb now score is: [0.8236879671053865, 0.812680570741607]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31389\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 223\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Info] Start training from score -0.065205\n",
      "[LightGBM] [Info] Start training from score -2.762638\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 0.253807\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.253604\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's multi_logloss: 0.253673\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.253677\n",
      "[5]\tvalid_0's multi_logloss: 0.253683\n",
      "[6]\tvalid_0's multi_logloss: 0.253657\n",
      "[7]\tvalid_0's multi_logloss: 0.253333\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 0.25335\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's multi_logloss: 0.253447\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's multi_logloss: 0.253661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 0.253582\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's multi_logloss: 0.253601\n",
      "[13]\tvalid_0's multi_logloss: 0.253637\n",
      "[14]\tvalid_0's multi_logloss: 0.253833\n",
      "[15]\tvalid_0's multi_logloss: 0.253622\n",
      "[16]\tvalid_0's multi_logloss: 0.253728\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's multi_logloss: 0.253739\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's multi_logloss: 0.253918\n",
      "[19]\tvalid_0's multi_logloss: 0.254012\n",
      "[20]\tvalid_0's multi_logloss: 0.254188\n",
      "[21]\tvalid_0's multi_logloss: 0.254039\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's multi_logloss: 0.254139\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's multi_logloss: 0.254219\n",
      "[24]\tvalid_0's multi_logloss: 0.254193\n",
      "[25]\tvalid_0's multi_logloss: 0.254116\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tvalid_0's multi_logloss: 0.254049\n",
      "[27]\tvalid_0's multi_logloss: 0.253956\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's multi_logloss: 0.254098\n",
      "[29]\tvalid_0's multi_logloss: 0.254339\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's multi_logloss: 0.254414\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's multi_logloss: 0.254583\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\tvalid_0's multi_logloss: 0.254556\n",
      "[33]\tvalid_0's multi_logloss: 0.254813\n",
      "[34]\tvalid_0's multi_logloss: 0.255006\n",
      "[35]\tvalid_0's multi_logloss: 0.25524\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tvalid_0's multi_logloss: 0.255213\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\tvalid_0's multi_logloss: 0.255167\n",
      "[38]\tvalid_0's multi_logloss: 0.255579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\tvalid_0's multi_logloss: 0.255773\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tvalid_0's multi_logloss: 0.255835\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tvalid_0's multi_logloss: 0.256004\n",
      "[42]\tvalid_0's multi_logloss: 0.256083\n",
      "[43]\tvalid_0's multi_logloss: 0.256236\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\tvalid_0's multi_logloss: 0.256499\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's multi_logloss: 0.256476\n",
      "[46]\tvalid_0's multi_logloss: 0.256599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's multi_logloss: 0.256836\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's multi_logloss: 0.257048\n",
      "[49]\tvalid_0's multi_logloss: 0.257344\n",
      "[50]\tvalid_0's multi_logloss: 0.257567\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\tvalid_0's multi_logloss: 0.25802\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tvalid_0's multi_logloss: 0.258278\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's multi_logloss: 0.258569\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tvalid_0's multi_logloss: 0.258646\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\tvalid_0's multi_logloss: 0.258968\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\tvalid_0's multi_logloss: 0.259332\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\tvalid_0's multi_logloss: 0.259647\n",
      "[58]\tvalid_0's multi_logloss: 0.259923\n",
      "[59]\tvalid_0's multi_logloss: 0.260188\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\tvalid_0's multi_logloss: 0.260741\n",
      "[61]\tvalid_0's multi_logloss: 0.260855\n",
      "[62]\tvalid_0's multi_logloss: 0.261212\n",
      "[63]\tvalid_0's multi_logloss: 0.261563\n",
      "[64]\tvalid_0's multi_logloss: 0.261725\n",
      "[65]\tvalid_0's multi_logloss: 0.262012\n",
      "[66]\tvalid_0's multi_logloss: 0.262202\n",
      "[67]\tvalid_0's multi_logloss: 0.262675\n",
      "[68]\tvalid_0's multi_logloss: 0.263017\n",
      "[69]\tvalid_0's multi_logloss: 0.263326\n",
      "[70]\tvalid_0's multi_logloss: 0.263476\n",
      "[71]\tvalid_0's multi_logloss: 0.263883\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\tvalid_0's multi_logloss: 0.264175\n",
      "[73]\tvalid_0's multi_logloss: 0.264479\n",
      "[74]\tvalid_0's multi_logloss: 0.264531\n",
      "[75]\tvalid_0's multi_logloss: 0.264666\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\tvalid_0's multi_logloss: 0.26493\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\tvalid_0's multi_logloss: 0.265264\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\tvalid_0's multi_logloss: 0.265541\n",
      "[79]\tvalid_0's multi_logloss: 0.265697\n",
      "[80]\tvalid_0's multi_logloss: 0.266046\n",
      "[81]\tvalid_0's multi_logloss: 0.266585\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\tvalid_0's multi_logloss: 0.266725\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\tvalid_0's multi_logloss: 0.267084\n",
      "[84]\tvalid_0's multi_logloss: 0.267188\n",
      "[85]\tvalid_0's multi_logloss: 0.267537\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\tvalid_0's multi_logloss: 0.26763\n",
      "[87]\tvalid_0's multi_logloss: 0.267995\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\tvalid_0's multi_logloss: 0.268253\n",
      "[89]\tvalid_0's multi_logloss: 0.268473\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[90]\tvalid_0's multi_logloss: 0.268755\n",
      "[91]\tvalid_0's multi_logloss: 0.26886\n",
      "[92]\tvalid_0's multi_logloss: 0.269065\n",
      "[93]\tvalid_0's multi_logloss: 0.269205\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[94]\tvalid_0's multi_logloss: 0.269316\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[95]\tvalid_0's multi_logloss: 0.269545\n",
      "[96]\tvalid_0's multi_logloss: 0.269763\n",
      "[97]\tvalid_0's multi_logloss: 0.270153\n",
      "[98]\tvalid_0's multi_logloss: 0.270617\n",
      "[99]\tvalid_0's multi_logloss: 0.270761\n",
      "[100]\tvalid_0's multi_logloss: 0.271034\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[101]\tvalid_0's multi_logloss: 0.271395\n",
      "[102]\tvalid_0's multi_logloss: 0.27162\n",
      "[103]\tvalid_0's multi_logloss: 0.271858\n",
      "[104]\tvalid_0's multi_logloss: 0.272219\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[105]\tvalid_0's multi_logloss: 0.272298\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[106]\tvalid_0's multi_logloss: 0.272497\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[107]\tvalid_0's multi_logloss: 0.272548\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's multi_logloss: 0.253333\n",
      "lgb now score is: [0.8236879671053865, 0.812680570741607, 0.8215533571736799]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31330\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 223\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Info] Start training from score -0.069886\n",
      "[LightGBM] [Info] Start training from score -2.695628\n",
      "[1]\tvalid_0's multi_logloss: 0.207419\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 0.207322\n",
      "[3]\tvalid_0's multi_logloss: 0.207428\n",
      "[4]\tvalid_0's multi_logloss: 0.207282\n",
      "[5]\tvalid_0's multi_logloss: 0.207139\n",
      "[6]\tvalid_0's multi_logloss: 0.207136\n",
      "[7]\tvalid_0's multi_logloss: 0.207152\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 0.207278\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's multi_logloss: 0.207096\n",
      "[10]\tvalid_0's multi_logloss: 0.206923\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 0.206824\n",
      "[12]\tvalid_0's multi_logloss: 0.206723\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's multi_logloss: 0.206771\n",
      "[14]\tvalid_0's multi_logloss: 0.206837\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's multi_logloss: 0.206721\n",
      "[16]\tvalid_0's multi_logloss: 0.206937\n",
      "[17]\tvalid_0's multi_logloss: 0.206981\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's multi_logloss: 0.207035\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's multi_logloss: 0.20703\n",
      "[20]\tvalid_0's multi_logloss: 0.206931\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tvalid_0's multi_logloss: 0.206889\n",
      "[22]\tvalid_0's multi_logloss: 0.206806\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's multi_logloss: 0.206913\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tvalid_0's multi_logloss: 0.206907\n",
      "[25]\tvalid_0's multi_logloss: 0.206946\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tvalid_0's multi_logloss: 0.207068\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's multi_logloss: 0.207059\n",
      "[28]\tvalid_0's multi_logloss: 0.206986\n",
      "[29]\tvalid_0's multi_logloss: 0.20702\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's multi_logloss: 0.207111\n",
      "[31]\tvalid_0's multi_logloss: 0.207017\n",
      "[32]\tvalid_0's multi_logloss: 0.20687\n",
      "[33]\tvalid_0's multi_logloss: 0.206973\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\tvalid_0's multi_logloss: 0.206937\n",
      "[35]\tvalid_0's multi_logloss: 0.207063\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tvalid_0's multi_logloss: 0.207164\n",
      "[37]\tvalid_0's multi_logloss: 0.207155\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tvalid_0's multi_logloss: 0.207212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\tvalid_0's multi_logloss: 0.207266\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tvalid_0's multi_logloss: 0.207169\n",
      "[41]\tvalid_0's multi_logloss: 0.207276\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\tvalid_0's multi_logloss: 0.207413\n",
      "[43]\tvalid_0's multi_logloss: 0.207385\n",
      "[44]\tvalid_0's multi_logloss: 0.207589\n",
      "[45]\tvalid_0's multi_logloss: 0.207726\n",
      "[46]\tvalid_0's multi_logloss: 0.20799\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's multi_logloss: 0.208167\n",
      "[48]\tvalid_0's multi_logloss: 0.20821\n",
      "[49]\tvalid_0's multi_logloss: 0.208438\n",
      "[50]\tvalid_0's multi_logloss: 0.208572\n",
      "[51]\tvalid_0's multi_logloss: 0.208772\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tvalid_0's multi_logloss: 0.20901\n",
      "[53]\tvalid_0's multi_logloss: 0.209007\n",
      "[54]\tvalid_0's multi_logloss: 0.209106\n",
      "[55]\tvalid_0's multi_logloss: 0.20931\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\tvalid_0's multi_logloss: 0.209533\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\tvalid_0's multi_logloss: 0.209889\n",
      "[58]\tvalid_0's multi_logloss: 0.210127\n",
      "[59]\tvalid_0's multi_logloss: 0.210197\n",
      "[60]\tvalid_0's multi_logloss: 0.210465\n",
      "[61]\tvalid_0's multi_logloss: 0.210557\n",
      "[62]\tvalid_0's multi_logloss: 0.210696\n",
      "[63]\tvalid_0's multi_logloss: 0.210701\n",
      "[64]\tvalid_0's multi_logloss: 0.210688\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\tvalid_0's multi_logloss: 0.211044\n",
      "[66]\tvalid_0's multi_logloss: 0.211037\n",
      "[67]\tvalid_0's multi_logloss: 0.21115\n",
      "[68]\tvalid_0's multi_logloss: 0.211183\n",
      "[69]\tvalid_0's multi_logloss: 0.211269\n",
      "[70]\tvalid_0's multi_logloss: 0.211335\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\tvalid_0's multi_logloss: 0.211358\n",
      "[72]\tvalid_0's multi_logloss: 0.211728\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\tvalid_0's multi_logloss: 0.212155\n",
      "[74]\tvalid_0's multi_logloss: 0.212177\n",
      "[75]\tvalid_0's multi_logloss: 0.212585\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\tvalid_0's multi_logloss: 0.213133\n",
      "[77]\tvalid_0's multi_logloss: 0.213421\n",
      "[78]\tvalid_0's multi_logloss: 0.213588\n",
      "[79]\tvalid_0's multi_logloss: 0.213818\n",
      "[80]\tvalid_0's multi_logloss: 0.214198\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\tvalid_0's multi_logloss: 0.214442\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\tvalid_0's multi_logloss: 0.214904\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\tvalid_0's multi_logloss: 0.21524\n",
      "[84]\tvalid_0's multi_logloss: 0.215345\n",
      "[85]\tvalid_0's multi_logloss: 0.215777\n",
      "[86]\tvalid_0's multi_logloss: 0.216139\n",
      "[87]\tvalid_0's multi_logloss: 0.216403\n",
      "[88]\tvalid_0's multi_logloss: 0.216679\n",
      "[89]\tvalid_0's multi_logloss: 0.216872\n",
      "[90]\tvalid_0's multi_logloss: 0.216851\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[91]\tvalid_0's multi_logloss: 0.21724\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[92]\tvalid_0's multi_logloss: 0.217586\n",
      "[93]\tvalid_0's multi_logloss: 0.217591\n",
      "[94]\tvalid_0's multi_logloss: 0.217748\n",
      "[95]\tvalid_0's multi_logloss: 0.218069\n",
      "[96]\tvalid_0's multi_logloss: 0.21828\n",
      "[97]\tvalid_0's multi_logloss: 0.218624\n",
      "[98]\tvalid_0's multi_logloss: 0.218761\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\tvalid_0's multi_logloss: 0.219004\n",
      "[100]\tvalid_0's multi_logloss: 0.219166\n",
      "[101]\tvalid_0's multi_logloss: 0.219604\n",
      "[102]\tvalid_0's multi_logloss: 0.219716\n",
      "[103]\tvalid_0's multi_logloss: 0.219989\n",
      "[104]\tvalid_0's multi_logloss: 0.220289\n",
      "[105]\tvalid_0's multi_logloss: 0.220602\n",
      "[106]\tvalid_0's multi_logloss: 0.220936\n",
      "[107]\tvalid_0's multi_logloss: 0.221289\n",
      "[108]\tvalid_0's multi_logloss: 0.22164\n",
      "[109]\tvalid_0's multi_logloss: 0.221642\n",
      "[110]\tvalid_0's multi_logloss: 0.221893\n",
      "[111]\tvalid_0's multi_logloss: 0.222196\n",
      "[112]\tvalid_0's multi_logloss: 0.222414\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[113]\tvalid_0's multi_logloss: 0.222739\n",
      "[114]\tvalid_0's multi_logloss: 0.223\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[115]\tvalid_0's multi_logloss: 0.223236\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's multi_logloss: 0.206721\n",
      "lgb now score is: [0.8236879671053865, 0.812680570741607, 0.8215533571736799, 0.8308923459029347]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31304\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 223\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Info] Start training from score -0.069216\n",
      "[LightGBM] [Info] Start training from score -2.704930\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 0.214035\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.214213\n",
      "[3]\tvalid_0's multi_logloss: 0.213813\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.213815\n",
      "[5]\tvalid_0's multi_logloss: 0.213397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.213433\n",
      "[7]\tvalid_0's multi_logloss: 0.213601\n",
      "[8]\tvalid_0's multi_logloss: 0.213493\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's multi_logloss: 0.212976\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's multi_logloss: 0.212464\n",
      "[11]\tvalid_0's multi_logloss: 0.212371\n",
      "[12]\tvalid_0's multi_logloss: 0.212452\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's multi_logloss: 0.212729\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's multi_logloss: 0.212842\n",
      "[15]\tvalid_0's multi_logloss: 0.21257\n",
      "[16]\tvalid_0's multi_logloss: 0.212232\n",
      "[17]\tvalid_0's multi_logloss: 0.212122\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's multi_logloss: 0.212203\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's multi_logloss: 0.211938\n",
      "[20]\tvalid_0's multi_logloss: 0.211735\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tvalid_0's multi_logloss: 0.211852\n",
      "[22]\tvalid_0's multi_logloss: 0.211686\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's multi_logloss: 0.211595\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tvalid_0's multi_logloss: 0.21131\n",
      "[25]\tvalid_0's multi_logloss: 0.211074\n",
      "[26]\tvalid_0's multi_logloss: 0.211097\n",
      "[27]\tvalid_0's multi_logloss: 0.210911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's multi_logloss: 0.211155\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\tvalid_0's multi_logloss: 0.210965\n",
      "[30]\tvalid_0's multi_logloss: 0.210675\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's multi_logloss: 0.210447\n",
      "[32]\tvalid_0's multi_logloss: 0.210549\n",
      "[33]\tvalid_0's multi_logloss: 0.210701\n",
      "[34]\tvalid_0's multi_logloss: 0.210444\n",
      "[35]\tvalid_0's multi_logloss: 0.210575\n",
      "[36]\tvalid_0's multi_logloss: 0.210549\n",
      "[37]\tvalid_0's multi_logloss: 0.210609\n",
      "[38]\tvalid_0's multi_logloss: 0.210514\n",
      "[39]\tvalid_0's multi_logloss: 0.210499\n",
      "[40]\tvalid_0's multi_logloss: 0.210452\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tvalid_0's multi_logloss: 0.210817\n",
      "[42]\tvalid_0's multi_logloss: 0.210899\n",
      "[43]\tvalid_0's multi_logloss: 0.210887\n",
      "[44]\tvalid_0's multi_logloss: 0.210763\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's multi_logloss: 0.210801\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\tvalid_0's multi_logloss: 0.211049\n",
      "[47]\tvalid_0's multi_logloss: 0.211238\n",
      "[48]\tvalid_0's multi_logloss: 0.211533\n",
      "[49]\tvalid_0's multi_logloss: 0.211704\n",
      "[50]\tvalid_0's multi_logloss: 0.211899\n",
      "[51]\tvalid_0's multi_logloss: 0.212213\n",
      "[52]\tvalid_0's multi_logloss: 0.212053\n",
      "[53]\tvalid_0's multi_logloss: 0.212283\n",
      "[54]\tvalid_0's multi_logloss: 0.212504\n",
      "[55]\tvalid_0's multi_logloss: 0.212803\n",
      "[56]\tvalid_0's multi_logloss: 0.212911\n",
      "[57]\tvalid_0's multi_logloss: 0.213066\n",
      "[58]\tvalid_0's multi_logloss: 0.213303\n",
      "[59]\tvalid_0's multi_logloss: 0.21341\n",
      "[60]\tvalid_0's multi_logloss: 0.213426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\tvalid_0's multi_logloss: 0.213588\n",
      "[62]\tvalid_0's multi_logloss: 0.213691\n",
      "[63]\tvalid_0's multi_logloss: 0.213616\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\tvalid_0's multi_logloss: 0.213783\n",
      "[65]\tvalid_0's multi_logloss: 0.21371\n",
      "[66]\tvalid_0's multi_logloss: 0.21384\n",
      "[67]\tvalid_0's multi_logloss: 0.214424\n",
      "[68]\tvalid_0's multi_logloss: 0.21477\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\tvalid_0's multi_logloss: 0.214759\n",
      "[70]\tvalid_0's multi_logloss: 0.214712\n",
      "[71]\tvalid_0's multi_logloss: 0.214599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\tvalid_0's multi_logloss: 0.214681\n",
      "[73]\tvalid_0's multi_logloss: 0.214892\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\tvalid_0's multi_logloss: 0.214864\n",
      "[75]\tvalid_0's multi_logloss: 0.215245\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\tvalid_0's multi_logloss: 0.215338\n",
      "[77]\tvalid_0's multi_logloss: 0.215272\n",
      "[78]\tvalid_0's multi_logloss: 0.215444\n",
      "[79]\tvalid_0's multi_logloss: 0.215765\n",
      "[80]\tvalid_0's multi_logloss: 0.215936\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\tvalid_0's multi_logloss: 0.216229\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\tvalid_0's multi_logloss: 0.216221\n",
      "[83]\tvalid_0's multi_logloss: 0.216279\n",
      "[84]\tvalid_0's multi_logloss: 0.216726\n",
      "[85]\tvalid_0's multi_logloss: 0.216806\n",
      "[86]\tvalid_0's multi_logloss: 0.21701\n",
      "[87]\tvalid_0's multi_logloss: 0.217256\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\tvalid_0's multi_logloss: 0.217306\n",
      "[89]\tvalid_0's multi_logloss: 0.217445\n",
      "[90]\tvalid_0's multi_logloss: 0.217731\n",
      "[91]\tvalid_0's multi_logloss: 0.218042\n",
      "[92]\tvalid_0's multi_logloss: 0.218449\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[93]\tvalid_0's multi_logloss: 0.218614\n",
      "[94]\tvalid_0's multi_logloss: 0.218679\n",
      "[95]\tvalid_0's multi_logloss: 0.21886\n",
      "[96]\tvalid_0's multi_logloss: 0.218997\n",
      "[97]\tvalid_0's multi_logloss: 0.219332\n",
      "[98]\tvalid_0's multi_logloss: 0.21975\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\tvalid_0's multi_logloss: 0.219807\n",
      "[100]\tvalid_0's multi_logloss: 0.21989\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[101]\tvalid_0's multi_logloss: 0.220118\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[102]\tvalid_0's multi_logloss: 0.22038\n",
      "[103]\tvalid_0's multi_logloss: 0.220745\n",
      "[104]\tvalid_0's multi_logloss: 0.221039\n",
      "[105]\tvalid_0's multi_logloss: 0.221128\n",
      "[106]\tvalid_0's multi_logloss: 0.221394\n",
      "[107]\tvalid_0's multi_logloss: 0.221753\n",
      "[108]\tvalid_0's multi_logloss: 0.222127\n",
      "[109]\tvalid_0's multi_logloss: 0.222356\n",
      "[110]\tvalid_0's multi_logloss: 0.222847\n",
      "[111]\tvalid_0's multi_logloss: 0.222986\n",
      "[112]\tvalid_0's multi_logloss: 0.223335\n",
      "[113]\tvalid_0's multi_logloss: 0.22367\n",
      "[114]\tvalid_0's multi_logloss: 0.223837\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[115]\tvalid_0's multi_logloss: 0.223942\n",
      "[116]\tvalid_0's multi_logloss: 0.224286\n",
      "[117]\tvalid_0's multi_logloss: 0.224556\n",
      "[118]\tvalid_0's multi_logloss: 0.224837\n",
      "[119]\tvalid_0's multi_logloss: 0.225091\n",
      "[120]\tvalid_0's multi_logloss: 0.225254\n",
      "[121]\tvalid_0's multi_logloss: 0.225649\n",
      "[122]\tvalid_0's multi_logloss: 0.225916\n",
      "[123]\tvalid_0's multi_logloss: 0.226033\n",
      "[124]\tvalid_0's multi_logloss: 0.226293\n",
      "[125]\tvalid_0's multi_logloss: 0.226635\n",
      "[126]\tvalid_0's multi_logloss: 0.226859\n",
      "[127]\tvalid_0's multi_logloss: 0.227069\n",
      "[128]\tvalid_0's multi_logloss: 0.227371\n",
      "[129]\tvalid_0's multi_logloss: 0.227682\n",
      "[130]\tvalid_0's multi_logloss: 0.227822\n",
      "[131]\tvalid_0's multi_logloss: 0.228001\n",
      "[132]\tvalid_0's multi_logloss: 0.228392\n",
      "[133]\tvalid_0's multi_logloss: 0.228759\n",
      "[134]\tvalid_0's multi_logloss: 0.228969\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's multi_logloss: 0.210444\n",
      "lgb now score is: [0.8236879671053865, 0.812680570741607, 0.8215533571736799, 0.8308923459029347, 0.8404422895429664]\n",
      "lgb_score_list: [0.8236879671053865, 0.812680570741607, 0.8215533571736799, 0.8308923459029347, 0.8404422895429664]\n",
      "lgb_score_mean: 0.8258513060933149\n",
      "[0]\ttrain-mlogloss:0.67083\teval-mlogloss:0.67126\n",
      "[1]\ttrain-mlogloss:0.64970\teval-mlogloss:0.65042\n",
      "[2]\ttrain-mlogloss:0.62983\teval-mlogloss:0.63091\n",
      "[3]\ttrain-mlogloss:0.61103\teval-mlogloss:0.61274\n",
      "[4]\ttrain-mlogloss:0.59316\teval-mlogloss:0.59526\n",
      "[5]\ttrain-mlogloss:0.57632\teval-mlogloss:0.57856\n",
      "[6]\ttrain-mlogloss:0.56034\teval-mlogloss:0.56287\n",
      "[7]\ttrain-mlogloss:0.54513\teval-mlogloss:0.54799\n",
      "[8]\ttrain-mlogloss:0.53061\teval-mlogloss:0.53410\n",
      "[9]\ttrain-mlogloss:0.51690\teval-mlogloss:0.52069\n",
      "[10]\ttrain-mlogloss:0.50386\teval-mlogloss:0.50812\n",
      "[11]\ttrain-mlogloss:0.49161\teval-mlogloss:0.49602\n",
      "[12]\ttrain-mlogloss:0.47943\teval-mlogloss:0.48434\n",
      "[13]\ttrain-mlogloss:0.46825\teval-mlogloss:0.47337\n",
      "[14]\ttrain-mlogloss:0.45760\teval-mlogloss:0.46303\n",
      "[15]\ttrain-mlogloss:0.44745\teval-mlogloss:0.45344\n",
      "[16]\ttrain-mlogloss:0.43765\teval-mlogloss:0.44412\n",
      "[17]\ttrain-mlogloss:0.42808\teval-mlogloss:0.43483\n",
      "[18]\ttrain-mlogloss:0.41918\teval-mlogloss:0.42637\n",
      "[19]\ttrain-mlogloss:0.41066\teval-mlogloss:0.41814\n",
      "[20]\ttrain-mlogloss:0.40253\teval-mlogloss:0.41041\n",
      "[21]\ttrain-mlogloss:0.39465\teval-mlogloss:0.40291\n",
      "[22]\ttrain-mlogloss:0.38703\teval-mlogloss:0.39572\n",
      "[23]\ttrain-mlogloss:0.37979\teval-mlogloss:0.38878\n",
      "[24]\ttrain-mlogloss:0.37278\teval-mlogloss:0.38223\n",
      "[25]\ttrain-mlogloss:0.36609\teval-mlogloss:0.37589\n",
      "[26]\ttrain-mlogloss:0.35967\teval-mlogloss:0.36980\n",
      "[27]\ttrain-mlogloss:0.35364\teval-mlogloss:0.36408\n",
      "[28]\ttrain-mlogloss:0.34794\teval-mlogloss:0.35869\n",
      "[29]\ttrain-mlogloss:0.34241\teval-mlogloss:0.35367\n",
      "[30]\ttrain-mlogloss:0.33696\teval-mlogloss:0.34879\n",
      "[31]\ttrain-mlogloss:0.33178\teval-mlogloss:0.34420\n",
      "[32]\ttrain-mlogloss:0.32679\teval-mlogloss:0.33969\n",
      "[33]\ttrain-mlogloss:0.32189\teval-mlogloss:0.33529\n",
      "[34]\ttrain-mlogloss:0.31720\teval-mlogloss:0.33092\n",
      "[35]\ttrain-mlogloss:0.31283\teval-mlogloss:0.32701\n",
      "[36]\ttrain-mlogloss:0.30849\teval-mlogloss:0.32311\n",
      "[37]\ttrain-mlogloss:0.30420\teval-mlogloss:0.31945\n",
      "[38]\ttrain-mlogloss:0.30022\teval-mlogloss:0.31578\n",
      "[39]\ttrain-mlogloss:0.29648\teval-mlogloss:0.31249\n",
      "[40]\ttrain-mlogloss:0.29257\teval-mlogloss:0.30945\n",
      "[41]\ttrain-mlogloss:0.28908\teval-mlogloss:0.30643\n",
      "[42]\ttrain-mlogloss:0.28573\teval-mlogloss:0.30361\n",
      "[43]\ttrain-mlogloss:0.28254\teval-mlogloss:0.30075\n",
      "[44]\ttrain-mlogloss:0.27930\teval-mlogloss:0.29800\n",
      "[45]\ttrain-mlogloss:0.27624\teval-mlogloss:0.29528\n",
      "[46]\ttrain-mlogloss:0.27331\teval-mlogloss:0.29260\n",
      "[47]\ttrain-mlogloss:0.27041\teval-mlogloss:0.29039\n",
      "[48]\ttrain-mlogloss:0.26759\teval-mlogloss:0.28820\n",
      "[49]\ttrain-mlogloss:0.26502\teval-mlogloss:0.28608\n",
      "[50]\ttrain-mlogloss:0.26210\teval-mlogloss:0.28388\n",
      "[51]\ttrain-mlogloss:0.25958\teval-mlogloss:0.28199\n",
      "[52]\ttrain-mlogloss:0.25706\teval-mlogloss:0.28007\n",
      "[53]\ttrain-mlogloss:0.25461\teval-mlogloss:0.27822\n",
      "[54]\ttrain-mlogloss:0.25245\teval-mlogloss:0.27660\n",
      "[55]\ttrain-mlogloss:0.25009\teval-mlogloss:0.27523\n",
      "[56]\ttrain-mlogloss:0.24802\teval-mlogloss:0.27352\n",
      "[57]\ttrain-mlogloss:0.24582\teval-mlogloss:0.27215\n",
      "[58]\ttrain-mlogloss:0.24373\teval-mlogloss:0.27041\n",
      "[59]\ttrain-mlogloss:0.24198\teval-mlogloss:0.26902\n",
      "[60]\ttrain-mlogloss:0.23979\teval-mlogloss:0.26772\n",
      "[61]\ttrain-mlogloss:0.23791\teval-mlogloss:0.26619\n",
      "[62]\ttrain-mlogloss:0.23592\teval-mlogloss:0.26506\n",
      "[63]\ttrain-mlogloss:0.23414\teval-mlogloss:0.26393\n",
      "[64]\ttrain-mlogloss:0.23227\teval-mlogloss:0.26292\n",
      "[65]\ttrain-mlogloss:0.23045\teval-mlogloss:0.26177\n",
      "[66]\ttrain-mlogloss:0.22897\teval-mlogloss:0.26074\n",
      "[67]\ttrain-mlogloss:0.22757\teval-mlogloss:0.25977\n",
      "[68]\ttrain-mlogloss:0.22588\teval-mlogloss:0.25863\n",
      "[69]\ttrain-mlogloss:0.22419\teval-mlogloss:0.25791\n",
      "[70]\ttrain-mlogloss:0.22261\teval-mlogloss:0.25703\n",
      "[71]\ttrain-mlogloss:0.22104\teval-mlogloss:0.25641\n",
      "[72]\ttrain-mlogloss:0.21967\teval-mlogloss:0.25582\n",
      "[73]\ttrain-mlogloss:0.21814\teval-mlogloss:0.25504\n",
      "[74]\ttrain-mlogloss:0.21692\teval-mlogloss:0.25438\n",
      "[75]\ttrain-mlogloss:0.21547\teval-mlogloss:0.25376\n",
      "[76]\ttrain-mlogloss:0.21412\teval-mlogloss:0.25328\n",
      "[77]\ttrain-mlogloss:0.21263\teval-mlogloss:0.25288\n",
      "[78]\ttrain-mlogloss:0.21152\teval-mlogloss:0.25227\n",
      "[79]\ttrain-mlogloss:0.21008\teval-mlogloss:0.25154\n",
      "[80]\ttrain-mlogloss:0.20876\teval-mlogloss:0.25122\n",
      "[81]\ttrain-mlogloss:0.20727\teval-mlogloss:0.25061\n",
      "[82]\ttrain-mlogloss:0.20603\teval-mlogloss:0.25026\n",
      "[83]\ttrain-mlogloss:0.20466\teval-mlogloss:0.24987\n",
      "[84]\ttrain-mlogloss:0.20349\teval-mlogloss:0.24946\n",
      "[85]\ttrain-mlogloss:0.20246\teval-mlogloss:0.24925\n",
      "[86]\ttrain-mlogloss:0.20123\teval-mlogloss:0.24875\n",
      "[87]\ttrain-mlogloss:0.20024\teval-mlogloss:0.24832\n",
      "[88]\ttrain-mlogloss:0.19929\teval-mlogloss:0.24794\n",
      "[89]\ttrain-mlogloss:0.19810\teval-mlogloss:0.24770\n",
      "[90]\ttrain-mlogloss:0.19699\teval-mlogloss:0.24752\n",
      "[91]\ttrain-mlogloss:0.19587\teval-mlogloss:0.24726\n",
      "[92]\ttrain-mlogloss:0.19492\teval-mlogloss:0.24703\n",
      "[93]\ttrain-mlogloss:0.19377\teval-mlogloss:0.24693\n",
      "[94]\ttrain-mlogloss:0.19278\teval-mlogloss:0.24649\n",
      "[95]\ttrain-mlogloss:0.19176\teval-mlogloss:0.24618\n",
      "[96]\ttrain-mlogloss:0.19078\teval-mlogloss:0.24608\n",
      "[97]\ttrain-mlogloss:0.18976\teval-mlogloss:0.24564\n",
      "[98]\ttrain-mlogloss:0.18903\teval-mlogloss:0.24543\n",
      "[99]\ttrain-mlogloss:0.18809\teval-mlogloss:0.24555\n",
      "[100]\ttrain-mlogloss:0.18717\teval-mlogloss:0.24544\n",
      "[101]\ttrain-mlogloss:0.18645\teval-mlogloss:0.24518\n",
      "[102]\ttrain-mlogloss:0.18537\teval-mlogloss:0.24510\n",
      "[103]\ttrain-mlogloss:0.18461\teval-mlogloss:0.24510\n",
      "[104]\ttrain-mlogloss:0.18359\teval-mlogloss:0.24512\n",
      "[105]\ttrain-mlogloss:0.18264\teval-mlogloss:0.24507\n",
      "[106]\ttrain-mlogloss:0.18167\teval-mlogloss:0.24513\n",
      "[107]\ttrain-mlogloss:0.18061\teval-mlogloss:0.24529\n",
      "[108]\ttrain-mlogloss:0.17975\teval-mlogloss:0.24509\n",
      "[109]\ttrain-mlogloss:0.17885\teval-mlogloss:0.24508\n",
      "[110]\ttrain-mlogloss:0.17809\teval-mlogloss:0.24507\n",
      "[111]\ttrain-mlogloss:0.17734\teval-mlogloss:0.24497\n",
      "[112]\ttrain-mlogloss:0.17646\teval-mlogloss:0.24502\n",
      "[113]\ttrain-mlogloss:0.17557\teval-mlogloss:0.24506\n",
      "[114]\ttrain-mlogloss:0.17479\teval-mlogloss:0.24522\n",
      "[115]\ttrain-mlogloss:0.17392\teval-mlogloss:0.24509\n",
      "[116]\ttrain-mlogloss:0.17330\teval-mlogloss:0.24498\n",
      "[117]\ttrain-mlogloss:0.17246\teval-mlogloss:0.24510\n",
      "[118]\ttrain-mlogloss:0.17158\teval-mlogloss:0.24538\n",
      "[119]\ttrain-mlogloss:0.17083\teval-mlogloss:0.24540\n",
      "[120]\ttrain-mlogloss:0.17014\teval-mlogloss:0.24546\n",
      "[121]\ttrain-mlogloss:0.16939\teval-mlogloss:0.24571\n",
      "[122]\ttrain-mlogloss:0.16858\teval-mlogloss:0.24556\n",
      "[123]\ttrain-mlogloss:0.16778\teval-mlogloss:0.24543\n",
      "[124]\ttrain-mlogloss:0.16697\teval-mlogloss:0.24536\n",
      "[125]\ttrain-mlogloss:0.16629\teval-mlogloss:0.24560\n",
      "[126]\ttrain-mlogloss:0.16563\teval-mlogloss:0.24563\n",
      "[127]\ttrain-mlogloss:0.16486\teval-mlogloss:0.24585\n",
      "[128]\ttrain-mlogloss:0.16422\teval-mlogloss:0.24601\n",
      "[129]\ttrain-mlogloss:0.16355\teval-mlogloss:0.24595\n",
      "[130]\ttrain-mlogloss:0.16291\teval-mlogloss:0.24601\n",
      "[131]\ttrain-mlogloss:0.16211\teval-mlogloss:0.24647\n",
      "[132]\ttrain-mlogloss:0.16143\teval-mlogloss:0.24662\n",
      "[133]\ttrain-mlogloss:0.16066\teval-mlogloss:0.24671\n",
      "[134]\ttrain-mlogloss:0.16000\teval-mlogloss:0.24699\n",
      "[135]\ttrain-mlogloss:0.15931\teval-mlogloss:0.24711\n",
      "[136]\ttrain-mlogloss:0.15879\teval-mlogloss:0.24714\n",
      "[137]\ttrain-mlogloss:0.15810\teval-mlogloss:0.24741\n",
      "[138]\ttrain-mlogloss:0.15733\teval-mlogloss:0.24762\n",
      "[139]\ttrain-mlogloss:0.15676\teval-mlogloss:0.24795\n",
      "[140]\ttrain-mlogloss:0.15606\teval-mlogloss:0.24806\n",
      "[141]\ttrain-mlogloss:0.15532\teval-mlogloss:0.24786\n",
      "[142]\ttrain-mlogloss:0.15470\teval-mlogloss:0.24794\n",
      "[143]\ttrain-mlogloss:0.15405\teval-mlogloss:0.24810\n",
      "[144]\ttrain-mlogloss:0.15346\teval-mlogloss:0.24829\n",
      "[145]\ttrain-mlogloss:0.15287\teval-mlogloss:0.24834\n",
      "[146]\ttrain-mlogloss:0.15222\teval-mlogloss:0.24832\n",
      "[147]\ttrain-mlogloss:0.15162\teval-mlogloss:0.24837\n",
      "[148]\ttrain-mlogloss:0.15094\teval-mlogloss:0.24866\n",
      "[149]\ttrain-mlogloss:0.15021\teval-mlogloss:0.24881\n",
      "[150]\ttrain-mlogloss:0.14969\teval-mlogloss:0.24913\n",
      "[151]\ttrain-mlogloss:0.14911\teval-mlogloss:0.24917\n",
      "[152]\ttrain-mlogloss:0.14845\teval-mlogloss:0.24921\n",
      "[153]\ttrain-mlogloss:0.14782\teval-mlogloss:0.24924\n",
      "[154]\ttrain-mlogloss:0.14725\teval-mlogloss:0.24896\n",
      "[155]\ttrain-mlogloss:0.14667\teval-mlogloss:0.24917\n",
      "[156]\ttrain-mlogloss:0.14613\teval-mlogloss:0.24949\n",
      "[157]\ttrain-mlogloss:0.14574\teval-mlogloss:0.24972\n",
      "[158]\ttrain-mlogloss:0.14522\teval-mlogloss:0.24995\n",
      "[159]\ttrain-mlogloss:0.14465\teval-mlogloss:0.25009\n",
      "[160]\ttrain-mlogloss:0.14420\teval-mlogloss:0.25011\n",
      "[161]\ttrain-mlogloss:0.14371\teval-mlogloss:0.25011\n",
      "[162]\ttrain-mlogloss:0.14319\teval-mlogloss:0.25030\n",
      "[163]\ttrain-mlogloss:0.14251\teval-mlogloss:0.25081\n",
      "[164]\ttrain-mlogloss:0.14204\teval-mlogloss:0.25101\n",
      "[165]\ttrain-mlogloss:0.14136\teval-mlogloss:0.25089\n",
      "[166]\ttrain-mlogloss:0.14075\teval-mlogloss:0.25127\n",
      "[167]\ttrain-mlogloss:0.14025\teval-mlogloss:0.25147\n",
      "[168]\ttrain-mlogloss:0.13972\teval-mlogloss:0.25162\n",
      "[169]\ttrain-mlogloss:0.13928\teval-mlogloss:0.25168\n",
      "[170]\ttrain-mlogloss:0.13884\teval-mlogloss:0.25156\n",
      "[171]\ttrain-mlogloss:0.13838\teval-mlogloss:0.25189\n",
      "[172]\ttrain-mlogloss:0.13792\teval-mlogloss:0.25226\n",
      "[173]\ttrain-mlogloss:0.13737\teval-mlogloss:0.25239\n",
      "[174]\ttrain-mlogloss:0.13683\teval-mlogloss:0.25243\n",
      "[175]\ttrain-mlogloss:0.13634\teval-mlogloss:0.25259\n",
      "[176]\ttrain-mlogloss:0.13595\teval-mlogloss:0.25266\n",
      "[177]\ttrain-mlogloss:0.13555\teval-mlogloss:0.25261\n",
      "[178]\ttrain-mlogloss:0.13506\teval-mlogloss:0.25268\n",
      "[179]\ttrain-mlogloss:0.13456\teval-mlogloss:0.25272\n",
      "[180]\ttrain-mlogloss:0.13405\teval-mlogloss:0.25289\n",
      "[181]\ttrain-mlogloss:0.13370\teval-mlogloss:0.25293\n",
      "[182]\ttrain-mlogloss:0.13327\teval-mlogloss:0.25316\n",
      "[183]\ttrain-mlogloss:0.13279\teval-mlogloss:0.25330\n",
      "[184]\ttrain-mlogloss:0.13228\teval-mlogloss:0.25356\n",
      "[185]\ttrain-mlogloss:0.13186\teval-mlogloss:0.25360\n",
      "[186]\ttrain-mlogloss:0.13146\teval-mlogloss:0.25376\n",
      "[187]\ttrain-mlogloss:0.13112\teval-mlogloss:0.25396\n",
      "[188]\ttrain-mlogloss:0.13068\teval-mlogloss:0.25401\n",
      "[189]\ttrain-mlogloss:0.13033\teval-mlogloss:0.25411\n",
      "[190]\ttrain-mlogloss:0.13002\teval-mlogloss:0.25398\n",
      "[191]\ttrain-mlogloss:0.12962\teval-mlogloss:0.25410\n",
      "[192]\ttrain-mlogloss:0.12918\teval-mlogloss:0.25412\n",
      "[193]\ttrain-mlogloss:0.12878\teval-mlogloss:0.25412\n",
      "[194]\ttrain-mlogloss:0.12839\teval-mlogloss:0.25425\n",
      "[195]\ttrain-mlogloss:0.12796\teval-mlogloss:0.25403\n",
      "[196]\ttrain-mlogloss:0.12754\teval-mlogloss:0.25401\n",
      "[197]\ttrain-mlogloss:0.12720\teval-mlogloss:0.25416\n",
      "[198]\ttrain-mlogloss:0.12664\teval-mlogloss:0.25425\n",
      "[199]\ttrain-mlogloss:0.12623\teval-mlogloss:0.25444\n",
      "[200]\ttrain-mlogloss:0.12597\teval-mlogloss:0.25464\n",
      "[201]\ttrain-mlogloss:0.12562\teval-mlogloss:0.25481\n",
      "[202]\ttrain-mlogloss:0.12535\teval-mlogloss:0.25491\n",
      "[203]\ttrain-mlogloss:0.12502\teval-mlogloss:0.25513\n",
      "[204]\ttrain-mlogloss:0.12461\teval-mlogloss:0.25510\n",
      "[205]\ttrain-mlogloss:0.12424\teval-mlogloss:0.25533\n",
      "[206]\ttrain-mlogloss:0.12388\teval-mlogloss:0.25535\n",
      "[207]\ttrain-mlogloss:0.12354\teval-mlogloss:0.25557\n",
      "[208]\ttrain-mlogloss:0.12317\teval-mlogloss:0.25577\n",
      "[209]\ttrain-mlogloss:0.12271\teval-mlogloss:0.25579\n",
      "[210]\ttrain-mlogloss:0.12228\teval-mlogloss:0.25584\n",
      "xgb now score is: [2.4585792262014]\n",
      "[0]\ttrain-mlogloss:0.67051\teval-mlogloss:0.67215\n",
      "[1]\ttrain-mlogloss:0.64916\teval-mlogloss:0.65220\n",
      "[2]\ttrain-mlogloss:0.62892\teval-mlogloss:0.63321\n",
      "[3]\ttrain-mlogloss:0.60979\teval-mlogloss:0.61535\n",
      "[4]\ttrain-mlogloss:0.59156\teval-mlogloss:0.59871\n",
      "[5]\ttrain-mlogloss:0.57431\teval-mlogloss:0.58293\n",
      "[6]\ttrain-mlogloss:0.55807\teval-mlogloss:0.56794\n",
      "[7]\ttrain-mlogloss:0.54279\teval-mlogloss:0.55417\n",
      "[8]\ttrain-mlogloss:0.52809\teval-mlogloss:0.54075\n",
      "[9]\ttrain-mlogloss:0.51402\teval-mlogloss:0.52781\n",
      "[10]\ttrain-mlogloss:0.50067\teval-mlogloss:0.51581\n",
      "[11]\ttrain-mlogloss:0.48787\teval-mlogloss:0.50428\n",
      "[12]\ttrain-mlogloss:0.47572\teval-mlogloss:0.49336\n",
      "[13]\ttrain-mlogloss:0.46426\teval-mlogloss:0.48314\n",
      "[14]\ttrain-mlogloss:0.45332\teval-mlogloss:0.47323\n",
      "[15]\ttrain-mlogloss:0.44282\teval-mlogloss:0.46390\n",
      "[16]\ttrain-mlogloss:0.43274\teval-mlogloss:0.45507\n",
      "[17]\ttrain-mlogloss:0.42306\teval-mlogloss:0.44664\n",
      "[18]\ttrain-mlogloss:0.41403\teval-mlogloss:0.43894\n",
      "[19]\ttrain-mlogloss:0.40525\teval-mlogloss:0.43141\n",
      "[20]\ttrain-mlogloss:0.39692\teval-mlogloss:0.42409\n",
      "[21]\ttrain-mlogloss:0.38888\teval-mlogloss:0.41689\n",
      "[22]\ttrain-mlogloss:0.38115\teval-mlogloss:0.41017\n",
      "[23]\ttrain-mlogloss:0.37369\teval-mlogloss:0.40382\n",
      "[24]\ttrain-mlogloss:0.36672\teval-mlogloss:0.39790\n",
      "[25]\ttrain-mlogloss:0.35982\teval-mlogloss:0.39194\n",
      "[26]\ttrain-mlogloss:0.35335\teval-mlogloss:0.38647\n",
      "[27]\ttrain-mlogloss:0.34710\teval-mlogloss:0.38130\n",
      "[28]\ttrain-mlogloss:0.34117\teval-mlogloss:0.37667\n",
      "[29]\ttrain-mlogloss:0.33538\teval-mlogloss:0.37190\n",
      "[30]\ttrain-mlogloss:0.32990\teval-mlogloss:0.36751\n",
      "[31]\ttrain-mlogloss:0.32464\teval-mlogloss:0.36340\n",
      "[32]\ttrain-mlogloss:0.31943\teval-mlogloss:0.35941\n",
      "[33]\ttrain-mlogloss:0.31449\teval-mlogloss:0.35548\n",
      "[34]\ttrain-mlogloss:0.30981\teval-mlogloss:0.35163\n",
      "[35]\ttrain-mlogloss:0.30521\teval-mlogloss:0.34790\n",
      "[36]\ttrain-mlogloss:0.30075\teval-mlogloss:0.34461\n",
      "[37]\ttrain-mlogloss:0.29661\teval-mlogloss:0.34156\n",
      "[38]\ttrain-mlogloss:0.29263\teval-mlogloss:0.33843\n",
      "[39]\ttrain-mlogloss:0.28868\teval-mlogloss:0.33547\n",
      "[40]\ttrain-mlogloss:0.28474\teval-mlogloss:0.33264\n",
      "[41]\ttrain-mlogloss:0.28115\teval-mlogloss:0.33026\n",
      "[42]\ttrain-mlogloss:0.27753\teval-mlogloss:0.32783\n",
      "[43]\ttrain-mlogloss:0.27409\teval-mlogloss:0.32536\n",
      "[44]\ttrain-mlogloss:0.27067\teval-mlogloss:0.32344\n",
      "[45]\ttrain-mlogloss:0.26754\teval-mlogloss:0.32135\n",
      "[46]\ttrain-mlogloss:0.26463\teval-mlogloss:0.31932\n",
      "[47]\ttrain-mlogloss:0.26184\teval-mlogloss:0.31739\n",
      "[48]\ttrain-mlogloss:0.25899\teval-mlogloss:0.31566\n",
      "[49]\ttrain-mlogloss:0.25617\teval-mlogloss:0.31410\n",
      "[50]\ttrain-mlogloss:0.25362\teval-mlogloss:0.31253\n",
      "[51]\ttrain-mlogloss:0.25102\teval-mlogloss:0.31110\n",
      "[52]\ttrain-mlogloss:0.24852\teval-mlogloss:0.30950\n",
      "[53]\ttrain-mlogloss:0.24616\teval-mlogloss:0.30799\n",
      "[54]\ttrain-mlogloss:0.24364\teval-mlogloss:0.30666\n",
      "[55]\ttrain-mlogloss:0.24146\teval-mlogloss:0.30556\n",
      "[56]\ttrain-mlogloss:0.23936\teval-mlogloss:0.30445\n",
      "[57]\ttrain-mlogloss:0.23730\teval-mlogloss:0.30328\n",
      "[58]\ttrain-mlogloss:0.23534\teval-mlogloss:0.30231\n",
      "[59]\ttrain-mlogloss:0.23332\teval-mlogloss:0.30155\n",
      "[60]\ttrain-mlogloss:0.23128\teval-mlogloss:0.30072\n",
      "[61]\ttrain-mlogloss:0.22937\teval-mlogloss:0.29993\n",
      "[62]\ttrain-mlogloss:0.22760\teval-mlogloss:0.29889\n",
      "[63]\ttrain-mlogloss:0.22584\teval-mlogloss:0.29831\n",
      "[64]\ttrain-mlogloss:0.22400\teval-mlogloss:0.29749\n",
      "[65]\ttrain-mlogloss:0.22220\teval-mlogloss:0.29693\n",
      "[66]\ttrain-mlogloss:0.22053\teval-mlogloss:0.29629\n",
      "[67]\ttrain-mlogloss:0.21878\teval-mlogloss:0.29573\n",
      "[68]\ttrain-mlogloss:0.21715\teval-mlogloss:0.29521\n",
      "[69]\ttrain-mlogloss:0.21553\teval-mlogloss:0.29467\n",
      "[70]\ttrain-mlogloss:0.21407\teval-mlogloss:0.29423\n",
      "[71]\ttrain-mlogloss:0.21259\teval-mlogloss:0.29375\n",
      "[72]\ttrain-mlogloss:0.21133\teval-mlogloss:0.29350\n",
      "[73]\ttrain-mlogloss:0.20975\teval-mlogloss:0.29318\n",
      "[74]\ttrain-mlogloss:0.20845\teval-mlogloss:0.29290\n",
      "[75]\ttrain-mlogloss:0.20699\teval-mlogloss:0.29250\n",
      "[76]\ttrain-mlogloss:0.20564\teval-mlogloss:0.29234\n",
      "[77]\ttrain-mlogloss:0.20417\teval-mlogloss:0.29198\n",
      "[78]\ttrain-mlogloss:0.20276\teval-mlogloss:0.29198\n",
      "[79]\ttrain-mlogloss:0.20167\teval-mlogloss:0.29184\n",
      "[80]\ttrain-mlogloss:0.20039\teval-mlogloss:0.29164\n",
      "[81]\ttrain-mlogloss:0.19922\teval-mlogloss:0.29143\n",
      "[82]\ttrain-mlogloss:0.19802\teval-mlogloss:0.29144\n",
      "[83]\ttrain-mlogloss:0.19676\teval-mlogloss:0.29133\n",
      "[84]\ttrain-mlogloss:0.19554\teval-mlogloss:0.29134\n",
      "[85]\ttrain-mlogloss:0.19435\teval-mlogloss:0.29147\n",
      "[86]\ttrain-mlogloss:0.19310\teval-mlogloss:0.29122\n",
      "[87]\ttrain-mlogloss:0.19205\teval-mlogloss:0.29123\n",
      "[88]\ttrain-mlogloss:0.19107\teval-mlogloss:0.29124\n",
      "[89]\ttrain-mlogloss:0.18981\teval-mlogloss:0.29125\n",
      "[90]\ttrain-mlogloss:0.18854\teval-mlogloss:0.29158\n",
      "[91]\ttrain-mlogloss:0.18741\teval-mlogloss:0.29134\n",
      "[92]\ttrain-mlogloss:0.18624\teval-mlogloss:0.29145\n",
      "[93]\ttrain-mlogloss:0.18539\teval-mlogloss:0.29161\n",
      "[94]\ttrain-mlogloss:0.18445\teval-mlogloss:0.29166\n",
      "[95]\ttrain-mlogloss:0.18325\teval-mlogloss:0.29194\n",
      "[96]\ttrain-mlogloss:0.18226\teval-mlogloss:0.29219\n",
      "[97]\ttrain-mlogloss:0.18146\teval-mlogloss:0.29233\n",
      "[98]\ttrain-mlogloss:0.18073\teval-mlogloss:0.29262\n",
      "[99]\ttrain-mlogloss:0.17966\teval-mlogloss:0.29285\n",
      "[100]\ttrain-mlogloss:0.17856\teval-mlogloss:0.29324\n",
      "[101]\ttrain-mlogloss:0.17739\teval-mlogloss:0.29332\n",
      "[102]\ttrain-mlogloss:0.17626\teval-mlogloss:0.29322\n",
      "[103]\ttrain-mlogloss:0.17536\teval-mlogloss:0.29332\n",
      "[104]\ttrain-mlogloss:0.17449\teval-mlogloss:0.29350\n",
      "[105]\ttrain-mlogloss:0.17358\teval-mlogloss:0.29377\n",
      "[106]\ttrain-mlogloss:0.17269\teval-mlogloss:0.29409\n",
      "[107]\ttrain-mlogloss:0.17172\teval-mlogloss:0.29455\n",
      "[108]\ttrain-mlogloss:0.17076\teval-mlogloss:0.29491\n",
      "[109]\ttrain-mlogloss:0.16976\teval-mlogloss:0.29478\n",
      "[110]\ttrain-mlogloss:0.16885\teval-mlogloss:0.29508\n",
      "[111]\ttrain-mlogloss:0.16807\teval-mlogloss:0.29539\n",
      "[112]\ttrain-mlogloss:0.16738\teval-mlogloss:0.29556\n",
      "[113]\ttrain-mlogloss:0.16654\teval-mlogloss:0.29571\n",
      "[114]\ttrain-mlogloss:0.16567\teval-mlogloss:0.29609\n",
      "[115]\ttrain-mlogloss:0.16469\teval-mlogloss:0.29646\n",
      "[116]\ttrain-mlogloss:0.16392\teval-mlogloss:0.29695\n",
      "[117]\ttrain-mlogloss:0.16311\teval-mlogloss:0.29750\n",
      "[118]\ttrain-mlogloss:0.16227\teval-mlogloss:0.29779\n",
      "[119]\ttrain-mlogloss:0.16158\teval-mlogloss:0.29804\n",
      "[120]\ttrain-mlogloss:0.16081\teval-mlogloss:0.29853\n",
      "[121]\ttrain-mlogloss:0.15999\teval-mlogloss:0.29891\n",
      "[122]\ttrain-mlogloss:0.15925\teval-mlogloss:0.29915\n",
      "[123]\ttrain-mlogloss:0.15850\teval-mlogloss:0.29929\n",
      "[124]\ttrain-mlogloss:0.15780\teval-mlogloss:0.29954\n",
      "[125]\ttrain-mlogloss:0.15715\teval-mlogloss:0.29996\n",
      "[126]\ttrain-mlogloss:0.15651\teval-mlogloss:0.30024\n",
      "[127]\ttrain-mlogloss:0.15566\teval-mlogloss:0.30065\n",
      "[128]\ttrain-mlogloss:0.15488\teval-mlogloss:0.30107\n",
      "[129]\ttrain-mlogloss:0.15414\teval-mlogloss:0.30152\n",
      "[130]\ttrain-mlogloss:0.15342\teval-mlogloss:0.30202\n",
      "[131]\ttrain-mlogloss:0.15268\teval-mlogloss:0.30232\n",
      "[132]\ttrain-mlogloss:0.15198\teval-mlogloss:0.30265\n",
      "[133]\ttrain-mlogloss:0.15121\teval-mlogloss:0.30297\n",
      "[134]\ttrain-mlogloss:0.15058\teval-mlogloss:0.30339\n",
      "[135]\ttrain-mlogloss:0.14995\teval-mlogloss:0.30383\n",
      "[136]\ttrain-mlogloss:0.14928\teval-mlogloss:0.30393\n",
      "[137]\ttrain-mlogloss:0.14854\teval-mlogloss:0.30421\n",
      "[138]\ttrain-mlogloss:0.14793\teval-mlogloss:0.30452\n",
      "[139]\ttrain-mlogloss:0.14729\teval-mlogloss:0.30487\n",
      "[140]\ttrain-mlogloss:0.14669\teval-mlogloss:0.30501\n",
      "[141]\ttrain-mlogloss:0.14612\teval-mlogloss:0.30528\n",
      "[142]\ttrain-mlogloss:0.14532\teval-mlogloss:0.30530\n",
      "[143]\ttrain-mlogloss:0.14479\teval-mlogloss:0.30545\n",
      "[144]\ttrain-mlogloss:0.14409\teval-mlogloss:0.30609\n",
      "[145]\ttrain-mlogloss:0.14356\teval-mlogloss:0.30643\n",
      "[146]\ttrain-mlogloss:0.14289\teval-mlogloss:0.30684\n",
      "[147]\ttrain-mlogloss:0.14242\teval-mlogloss:0.30734\n",
      "[148]\ttrain-mlogloss:0.14187\teval-mlogloss:0.30734\n",
      "[149]\ttrain-mlogloss:0.14127\teval-mlogloss:0.30765\n",
      "[150]\ttrain-mlogloss:0.14067\teval-mlogloss:0.30800\n",
      "[151]\ttrain-mlogloss:0.14018\teval-mlogloss:0.30829\n",
      "[152]\ttrain-mlogloss:0.13973\teval-mlogloss:0.30822\n",
      "[153]\ttrain-mlogloss:0.13924\teval-mlogloss:0.30834\n",
      "[154]\ttrain-mlogloss:0.13868\teval-mlogloss:0.30868\n",
      "[155]\ttrain-mlogloss:0.13801\teval-mlogloss:0.30912\n",
      "[156]\ttrain-mlogloss:0.13756\teval-mlogloss:0.30920\n",
      "[157]\ttrain-mlogloss:0.13703\teval-mlogloss:0.30954\n",
      "[158]\ttrain-mlogloss:0.13647\teval-mlogloss:0.31025\n",
      "[159]\ttrain-mlogloss:0.13588\teval-mlogloss:0.31053\n",
      "[160]\ttrain-mlogloss:0.13534\teval-mlogloss:0.31084\n",
      "[161]\ttrain-mlogloss:0.13488\teval-mlogloss:0.31096\n",
      "[162]\ttrain-mlogloss:0.13423\teval-mlogloss:0.31118\n",
      "[163]\ttrain-mlogloss:0.13368\teval-mlogloss:0.31149\n",
      "[164]\ttrain-mlogloss:0.13306\teval-mlogloss:0.31177\n",
      "[165]\ttrain-mlogloss:0.13265\teval-mlogloss:0.31205\n",
      "[166]\ttrain-mlogloss:0.13207\teval-mlogloss:0.31232\n",
      "[167]\ttrain-mlogloss:0.13164\teval-mlogloss:0.31253\n",
      "[168]\ttrain-mlogloss:0.13122\teval-mlogloss:0.31273\n",
      "[169]\ttrain-mlogloss:0.13076\teval-mlogloss:0.31311\n",
      "[170]\ttrain-mlogloss:0.13029\teval-mlogloss:0.31336\n",
      "[171]\ttrain-mlogloss:0.12987\teval-mlogloss:0.31374\n",
      "[172]\ttrain-mlogloss:0.12945\teval-mlogloss:0.31400\n",
      "[173]\ttrain-mlogloss:0.12892\teval-mlogloss:0.31437\n",
      "[174]\ttrain-mlogloss:0.12839\teval-mlogloss:0.31473\n",
      "[175]\ttrain-mlogloss:0.12797\teval-mlogloss:0.31499\n",
      "[176]\ttrain-mlogloss:0.12762\teval-mlogloss:0.31524\n",
      "[177]\ttrain-mlogloss:0.12710\teval-mlogloss:0.31576\n",
      "[178]\ttrain-mlogloss:0.12656\teval-mlogloss:0.31613\n",
      "[179]\ttrain-mlogloss:0.12614\teval-mlogloss:0.31670\n",
      "[180]\ttrain-mlogloss:0.12571\teval-mlogloss:0.31704\n",
      "[181]\ttrain-mlogloss:0.12530\teval-mlogloss:0.31715\n",
      "[182]\ttrain-mlogloss:0.12489\teval-mlogloss:0.31742\n",
      "[183]\ttrain-mlogloss:0.12451\teval-mlogloss:0.31761\n",
      "[184]\ttrain-mlogloss:0.12417\teval-mlogloss:0.31769\n",
      "[185]\ttrain-mlogloss:0.12367\teval-mlogloss:0.31786\n",
      "[186]\ttrain-mlogloss:0.12333\teval-mlogloss:0.31794\n",
      "xgb now score is: [2.4585792262014, 2.21614713832736]\n",
      "[0]\ttrain-mlogloss:0.67066\teval-mlogloss:0.67144\n",
      "[1]\ttrain-mlogloss:0.64943\teval-mlogloss:0.65096\n",
      "[2]\ttrain-mlogloss:0.62928\teval-mlogloss:0.63183\n",
      "[3]\ttrain-mlogloss:0.61018\teval-mlogloss:0.61347\n",
      "[4]\ttrain-mlogloss:0.59232\teval-mlogloss:0.59637\n",
      "[5]\ttrain-mlogloss:0.57541\teval-mlogloss:0.57999\n",
      "[6]\ttrain-mlogloss:0.55920\teval-mlogloss:0.56461\n",
      "[7]\ttrain-mlogloss:0.54392\teval-mlogloss:0.54982\n",
      "[8]\ttrain-mlogloss:0.52945\teval-mlogloss:0.53612\n",
      "[9]\ttrain-mlogloss:0.51574\teval-mlogloss:0.52305\n",
      "[10]\ttrain-mlogloss:0.50271\teval-mlogloss:0.51076\n",
      "[11]\ttrain-mlogloss:0.49008\teval-mlogloss:0.49878\n",
      "[12]\ttrain-mlogloss:0.47798\teval-mlogloss:0.48723\n",
      "[13]\ttrain-mlogloss:0.46657\teval-mlogloss:0.47641\n",
      "[14]\ttrain-mlogloss:0.45565\teval-mlogloss:0.46615\n",
      "[15]\ttrain-mlogloss:0.44532\teval-mlogloss:0.45624\n",
      "[16]\ttrain-mlogloss:0.43549\teval-mlogloss:0.44711\n",
      "[17]\ttrain-mlogloss:0.42595\teval-mlogloss:0.43822\n",
      "[18]\ttrain-mlogloss:0.41679\teval-mlogloss:0.42959\n",
      "[19]\ttrain-mlogloss:0.40804\teval-mlogloss:0.42146\n",
      "[20]\ttrain-mlogloss:0.39966\teval-mlogloss:0.41367\n",
      "[21]\ttrain-mlogloss:0.39187\teval-mlogloss:0.40650\n",
      "[22]\ttrain-mlogloss:0.38439\teval-mlogloss:0.39955\n",
      "[23]\ttrain-mlogloss:0.37704\teval-mlogloss:0.39276\n",
      "[24]\ttrain-mlogloss:0.37012\teval-mlogloss:0.38626\n",
      "[25]\ttrain-mlogloss:0.36354\teval-mlogloss:0.38016\n",
      "[26]\ttrain-mlogloss:0.35727\teval-mlogloss:0.37430\n",
      "[27]\ttrain-mlogloss:0.35118\teval-mlogloss:0.36878\n",
      "[28]\ttrain-mlogloss:0.34525\teval-mlogloss:0.36340\n",
      "[29]\ttrain-mlogloss:0.33944\teval-mlogloss:0.35845\n",
      "[30]\ttrain-mlogloss:0.33396\teval-mlogloss:0.35351\n",
      "[31]\ttrain-mlogloss:0.32884\teval-mlogloss:0.34889\n",
      "[32]\ttrain-mlogloss:0.32373\teval-mlogloss:0.34447\n",
      "[33]\ttrain-mlogloss:0.31897\teval-mlogloss:0.34032\n",
      "[34]\ttrain-mlogloss:0.31430\teval-mlogloss:0.33630\n",
      "[35]\ttrain-mlogloss:0.30985\teval-mlogloss:0.33255\n",
      "[36]\ttrain-mlogloss:0.30560\teval-mlogloss:0.32879\n",
      "[37]\ttrain-mlogloss:0.30134\teval-mlogloss:0.32522\n",
      "[38]\ttrain-mlogloss:0.29746\teval-mlogloss:0.32206\n",
      "[39]\ttrain-mlogloss:0.29362\teval-mlogloss:0.31899\n",
      "[40]\ttrain-mlogloss:0.28991\teval-mlogloss:0.31590\n",
      "[41]\ttrain-mlogloss:0.28630\teval-mlogloss:0.31309\n",
      "[42]\ttrain-mlogloss:0.28278\teval-mlogloss:0.31029\n",
      "[43]\ttrain-mlogloss:0.27952\teval-mlogloss:0.30770\n",
      "[44]\ttrain-mlogloss:0.27624\teval-mlogloss:0.30525\n",
      "[45]\ttrain-mlogloss:0.27324\teval-mlogloss:0.30294\n",
      "[46]\ttrain-mlogloss:0.27041\teval-mlogloss:0.30074\n",
      "[47]\ttrain-mlogloss:0.26752\teval-mlogloss:0.29855\n",
      "[48]\ttrain-mlogloss:0.26480\teval-mlogloss:0.29646\n",
      "[49]\ttrain-mlogloss:0.26210\teval-mlogloss:0.29437\n",
      "[50]\ttrain-mlogloss:0.25957\teval-mlogloss:0.29253\n",
      "[51]\ttrain-mlogloss:0.25676\teval-mlogloss:0.29065\n",
      "[52]\ttrain-mlogloss:0.25428\teval-mlogloss:0.28894\n",
      "[53]\ttrain-mlogloss:0.25189\teval-mlogloss:0.28703\n",
      "[54]\ttrain-mlogloss:0.24964\teval-mlogloss:0.28560\n",
      "[55]\ttrain-mlogloss:0.24752\teval-mlogloss:0.28413\n",
      "[56]\ttrain-mlogloss:0.24543\teval-mlogloss:0.28272\n",
      "[57]\ttrain-mlogloss:0.24333\teval-mlogloss:0.28131\n",
      "[58]\ttrain-mlogloss:0.24123\teval-mlogloss:0.28014\n",
      "[59]\ttrain-mlogloss:0.23928\teval-mlogloss:0.27898\n",
      "[60]\ttrain-mlogloss:0.23745\teval-mlogloss:0.27786\n",
      "[61]\ttrain-mlogloss:0.23522\teval-mlogloss:0.27680\n",
      "[62]\ttrain-mlogloss:0.23354\teval-mlogloss:0.27581\n",
      "[63]\ttrain-mlogloss:0.23170\teval-mlogloss:0.27470\n",
      "[64]\ttrain-mlogloss:0.22985\teval-mlogloss:0.27366\n",
      "[65]\ttrain-mlogloss:0.22810\teval-mlogloss:0.27270\n",
      "[66]\ttrain-mlogloss:0.22661\teval-mlogloss:0.27176\n",
      "[67]\ttrain-mlogloss:0.22512\teval-mlogloss:0.27089\n",
      "[68]\ttrain-mlogloss:0.22348\teval-mlogloss:0.26991\n",
      "[69]\ttrain-mlogloss:0.22209\teval-mlogloss:0.26932\n",
      "[70]\ttrain-mlogloss:0.22071\teval-mlogloss:0.26850\n",
      "[71]\ttrain-mlogloss:0.21932\teval-mlogloss:0.26786\n",
      "[72]\ttrain-mlogloss:0.21778\teval-mlogloss:0.26739\n",
      "[73]\ttrain-mlogloss:0.21626\teval-mlogloss:0.26695\n",
      "[74]\ttrain-mlogloss:0.21488\teval-mlogloss:0.26643\n",
      "[75]\ttrain-mlogloss:0.21349\teval-mlogloss:0.26606\n",
      "[76]\ttrain-mlogloss:0.21255\teval-mlogloss:0.26558\n",
      "[77]\ttrain-mlogloss:0.21118\teval-mlogloss:0.26527\n",
      "[78]\ttrain-mlogloss:0.20965\teval-mlogloss:0.26492\n",
      "[79]\ttrain-mlogloss:0.20827\teval-mlogloss:0.26466\n",
      "[80]\ttrain-mlogloss:0.20696\teval-mlogloss:0.26447\n",
      "[81]\ttrain-mlogloss:0.20551\teval-mlogloss:0.26407\n",
      "[82]\ttrain-mlogloss:0.20427\teval-mlogloss:0.26360\n",
      "[83]\ttrain-mlogloss:0.20297\teval-mlogloss:0.26324\n",
      "[84]\ttrain-mlogloss:0.20186\teval-mlogloss:0.26299\n",
      "[85]\ttrain-mlogloss:0.20058\teval-mlogloss:0.26271\n",
      "[86]\ttrain-mlogloss:0.19925\teval-mlogloss:0.26250\n",
      "[87]\ttrain-mlogloss:0.19807\teval-mlogloss:0.26218\n",
      "[88]\ttrain-mlogloss:0.19695\teval-mlogloss:0.26189\n",
      "[89]\ttrain-mlogloss:0.19578\teval-mlogloss:0.26174\n",
      "[90]\ttrain-mlogloss:0.19482\teval-mlogloss:0.26146\n",
      "[91]\ttrain-mlogloss:0.19362\teval-mlogloss:0.26136\n",
      "[92]\ttrain-mlogloss:0.19257\teval-mlogloss:0.26135\n",
      "[93]\ttrain-mlogloss:0.19153\teval-mlogloss:0.26134\n",
      "[94]\ttrain-mlogloss:0.19056\teval-mlogloss:0.26132\n",
      "[95]\ttrain-mlogloss:0.18949\teval-mlogloss:0.26124\n",
      "[96]\ttrain-mlogloss:0.18846\teval-mlogloss:0.26120\n",
      "[97]\ttrain-mlogloss:0.18760\teval-mlogloss:0.26098\n",
      "[98]\ttrain-mlogloss:0.18660\teval-mlogloss:0.26094\n",
      "[99]\ttrain-mlogloss:0.18577\teval-mlogloss:0.26081\n",
      "[100]\ttrain-mlogloss:0.18467\teval-mlogloss:0.26084\n",
      "[101]\ttrain-mlogloss:0.18357\teval-mlogloss:0.26088\n",
      "[102]\ttrain-mlogloss:0.18243\teval-mlogloss:0.26075\n",
      "[103]\ttrain-mlogloss:0.18143\teval-mlogloss:0.26077\n",
      "[104]\ttrain-mlogloss:0.18060\teval-mlogloss:0.26082\n",
      "[105]\ttrain-mlogloss:0.17958\teval-mlogloss:0.26075\n",
      "[106]\ttrain-mlogloss:0.17860\teval-mlogloss:0.26071\n",
      "[107]\ttrain-mlogloss:0.17768\teval-mlogloss:0.26087\n",
      "[108]\ttrain-mlogloss:0.17700\teval-mlogloss:0.26099\n",
      "[109]\ttrain-mlogloss:0.17616\teval-mlogloss:0.26110\n",
      "[110]\ttrain-mlogloss:0.17530\teval-mlogloss:0.26105\n",
      "[111]\ttrain-mlogloss:0.17457\teval-mlogloss:0.26143\n",
      "[112]\ttrain-mlogloss:0.17364\teval-mlogloss:0.26155\n",
      "[113]\ttrain-mlogloss:0.17291\teval-mlogloss:0.26167\n",
      "[114]\ttrain-mlogloss:0.17209\teval-mlogloss:0.26188\n",
      "[115]\ttrain-mlogloss:0.17132\teval-mlogloss:0.26197\n",
      "[116]\ttrain-mlogloss:0.17050\teval-mlogloss:0.26192\n",
      "[117]\ttrain-mlogloss:0.16970\teval-mlogloss:0.26178\n",
      "[118]\ttrain-mlogloss:0.16908\teval-mlogloss:0.26182\n",
      "[119]\ttrain-mlogloss:0.16835\teval-mlogloss:0.26170\n",
      "[120]\ttrain-mlogloss:0.16758\teval-mlogloss:0.26187\n",
      "[121]\ttrain-mlogloss:0.16689\teval-mlogloss:0.26154\n",
      "[122]\ttrain-mlogloss:0.16601\teval-mlogloss:0.26144\n",
      "[123]\ttrain-mlogloss:0.16530\teval-mlogloss:0.26158\n",
      "[124]\ttrain-mlogloss:0.16456\teval-mlogloss:0.26170\n",
      "[125]\ttrain-mlogloss:0.16390\teval-mlogloss:0.26181\n",
      "[126]\ttrain-mlogloss:0.16324\teval-mlogloss:0.26193\n",
      "[127]\ttrain-mlogloss:0.16238\teval-mlogloss:0.26186\n",
      "[128]\ttrain-mlogloss:0.16179\teval-mlogloss:0.26230\n",
      "[129]\ttrain-mlogloss:0.16108\teval-mlogloss:0.26249\n",
      "[130]\ttrain-mlogloss:0.16044\teval-mlogloss:0.26269\n",
      "[131]\ttrain-mlogloss:0.15976\teval-mlogloss:0.26287\n",
      "[132]\ttrain-mlogloss:0.15910\teval-mlogloss:0.26278\n",
      "[133]\ttrain-mlogloss:0.15845\teval-mlogloss:0.26281\n",
      "[134]\ttrain-mlogloss:0.15769\teval-mlogloss:0.26277\n",
      "[135]\ttrain-mlogloss:0.15702\teval-mlogloss:0.26286\n",
      "[136]\ttrain-mlogloss:0.15650\teval-mlogloss:0.26291\n",
      "[137]\ttrain-mlogloss:0.15576\teval-mlogloss:0.26332\n",
      "[138]\ttrain-mlogloss:0.15504\teval-mlogloss:0.26317\n",
      "[139]\ttrain-mlogloss:0.15438\teval-mlogloss:0.26342\n",
      "[140]\ttrain-mlogloss:0.15360\teval-mlogloss:0.26340\n",
      "[141]\ttrain-mlogloss:0.15295\teval-mlogloss:0.26358\n",
      "[142]\ttrain-mlogloss:0.15235\teval-mlogloss:0.26372\n",
      "[143]\ttrain-mlogloss:0.15173\teval-mlogloss:0.26370\n",
      "[144]\ttrain-mlogloss:0.15114\teval-mlogloss:0.26409\n",
      "[145]\ttrain-mlogloss:0.15056\teval-mlogloss:0.26403\n",
      "[146]\ttrain-mlogloss:0.14991\teval-mlogloss:0.26421\n",
      "[147]\ttrain-mlogloss:0.14935\teval-mlogloss:0.26425\n",
      "[148]\ttrain-mlogloss:0.14884\teval-mlogloss:0.26443\n",
      "[149]\ttrain-mlogloss:0.14829\teval-mlogloss:0.26449\n",
      "[150]\ttrain-mlogloss:0.14773\teval-mlogloss:0.26459\n",
      "[151]\ttrain-mlogloss:0.14714\teval-mlogloss:0.26471\n",
      "[152]\ttrain-mlogloss:0.14653\teval-mlogloss:0.26483\n",
      "[153]\ttrain-mlogloss:0.14580\teval-mlogloss:0.26506\n",
      "[154]\ttrain-mlogloss:0.14538\teval-mlogloss:0.26499\n",
      "[155]\ttrain-mlogloss:0.14483\teval-mlogloss:0.26539\n",
      "[156]\ttrain-mlogloss:0.14428\teval-mlogloss:0.26535\n",
      "[157]\ttrain-mlogloss:0.14380\teval-mlogloss:0.26544\n",
      "[158]\ttrain-mlogloss:0.14328\teval-mlogloss:0.26541\n",
      "[159]\ttrain-mlogloss:0.14264\teval-mlogloss:0.26565\n",
      "[160]\ttrain-mlogloss:0.14216\teval-mlogloss:0.26581\n",
      "[161]\ttrain-mlogloss:0.14159\teval-mlogloss:0.26585\n",
      "[162]\ttrain-mlogloss:0.14113\teval-mlogloss:0.26608\n",
      "[163]\ttrain-mlogloss:0.14065\teval-mlogloss:0.26622\n",
      "[164]\ttrain-mlogloss:0.14016\teval-mlogloss:0.26632\n",
      "[165]\ttrain-mlogloss:0.13968\teval-mlogloss:0.26639\n",
      "[166]\ttrain-mlogloss:0.13914\teval-mlogloss:0.26641\n",
      "[167]\ttrain-mlogloss:0.13854\teval-mlogloss:0.26667\n",
      "[168]\ttrain-mlogloss:0.13807\teval-mlogloss:0.26690\n",
      "[169]\ttrain-mlogloss:0.13751\teval-mlogloss:0.26710\n",
      "[170]\ttrain-mlogloss:0.13712\teval-mlogloss:0.26713\n",
      "[171]\ttrain-mlogloss:0.13655\teval-mlogloss:0.26737\n",
      "[172]\ttrain-mlogloss:0.13600\teval-mlogloss:0.26760\n",
      "[173]\ttrain-mlogloss:0.13554\teval-mlogloss:0.26757\n",
      "[174]\ttrain-mlogloss:0.13505\teval-mlogloss:0.26755\n",
      "[175]\ttrain-mlogloss:0.13469\teval-mlogloss:0.26760\n",
      "[176]\ttrain-mlogloss:0.13437\teval-mlogloss:0.26787\n",
      "[177]\ttrain-mlogloss:0.13394\teval-mlogloss:0.26797\n",
      "[178]\ttrain-mlogloss:0.13346\teval-mlogloss:0.26813\n",
      "[179]\ttrain-mlogloss:0.13284\teval-mlogloss:0.26839\n",
      "[180]\ttrain-mlogloss:0.13243\teval-mlogloss:0.26867\n",
      "[181]\ttrain-mlogloss:0.13200\teval-mlogloss:0.26912\n",
      "[182]\ttrain-mlogloss:0.13153\teval-mlogloss:0.26932\n",
      "[183]\ttrain-mlogloss:0.13114\teval-mlogloss:0.26960\n",
      "[184]\ttrain-mlogloss:0.13068\teval-mlogloss:0.26955\n",
      "[185]\ttrain-mlogloss:0.13033\teval-mlogloss:0.26950\n",
      "[186]\ttrain-mlogloss:0.13002\teval-mlogloss:0.26964\n",
      "[187]\ttrain-mlogloss:0.12962\teval-mlogloss:0.26970\n",
      "[188]\ttrain-mlogloss:0.12911\teval-mlogloss:0.26978\n",
      "[189]\ttrain-mlogloss:0.12871\teval-mlogloss:0.26988\n",
      "[190]\ttrain-mlogloss:0.12837\teval-mlogloss:0.27008\n",
      "[191]\ttrain-mlogloss:0.12799\teval-mlogloss:0.27019\n",
      "[192]\ttrain-mlogloss:0.12762\teval-mlogloss:0.27024\n",
      "[193]\ttrain-mlogloss:0.12712\teval-mlogloss:0.26982\n",
      "[194]\ttrain-mlogloss:0.12686\teval-mlogloss:0.26999\n",
      "[195]\ttrain-mlogloss:0.12650\teval-mlogloss:0.27021\n",
      "[196]\ttrain-mlogloss:0.12610\teval-mlogloss:0.27040\n",
      "[197]\ttrain-mlogloss:0.12575\teval-mlogloss:0.27071\n",
      "[198]\ttrain-mlogloss:0.12537\teval-mlogloss:0.27066\n",
      "[199]\ttrain-mlogloss:0.12499\teval-mlogloss:0.27076\n",
      "[200]\ttrain-mlogloss:0.12459\teval-mlogloss:0.27066\n",
      "[201]\ttrain-mlogloss:0.12421\teval-mlogloss:0.27058\n",
      "[202]\ttrain-mlogloss:0.12385\teval-mlogloss:0.27054\n",
      "[203]\ttrain-mlogloss:0.12350\teval-mlogloss:0.27061\n",
      "[204]\ttrain-mlogloss:0.12311\teval-mlogloss:0.27039\n",
      "[205]\ttrain-mlogloss:0.12286\teval-mlogloss:0.27054\n",
      "xgb now score is: [2.4585792262014, 2.21614713832736, 2.467062568301335]\n",
      "[0]\ttrain-mlogloss:0.67100\teval-mlogloss:0.67081\n",
      "[1]\ttrain-mlogloss:0.65021\teval-mlogloss:0.64950\n",
      "[2]\ttrain-mlogloss:0.63062\teval-mlogloss:0.62964\n",
      "[3]\ttrain-mlogloss:0.61222\teval-mlogloss:0.61103\n",
      "[4]\ttrain-mlogloss:0.59462\teval-mlogloss:0.59306\n",
      "[5]\ttrain-mlogloss:0.57829\teval-mlogloss:0.57645\n",
      "[6]\ttrain-mlogloss:0.56254\teval-mlogloss:0.56044\n",
      "[7]\ttrain-mlogloss:0.54752\teval-mlogloss:0.54477\n",
      "[8]\ttrain-mlogloss:0.53345\teval-mlogloss:0.53042\n",
      "[9]\ttrain-mlogloss:0.51986\teval-mlogloss:0.51660\n",
      "[10]\ttrain-mlogloss:0.50713\teval-mlogloss:0.50337\n",
      "[11]\ttrain-mlogloss:0.49470\teval-mlogloss:0.49063\n",
      "[12]\ttrain-mlogloss:0.48322\teval-mlogloss:0.47874\n",
      "[13]\ttrain-mlogloss:0.47202\teval-mlogloss:0.46749\n",
      "[14]\ttrain-mlogloss:0.46147\teval-mlogloss:0.45671\n",
      "[15]\ttrain-mlogloss:0.45139\teval-mlogloss:0.44624\n",
      "[16]\ttrain-mlogloss:0.44149\teval-mlogloss:0.43600\n",
      "[17]\ttrain-mlogloss:0.43245\teval-mlogloss:0.42677\n",
      "[18]\ttrain-mlogloss:0.42370\teval-mlogloss:0.41795\n",
      "[19]\ttrain-mlogloss:0.41516\teval-mlogloss:0.40917\n",
      "[20]\ttrain-mlogloss:0.40708\teval-mlogloss:0.40081\n",
      "[21]\ttrain-mlogloss:0.39944\teval-mlogloss:0.39288\n",
      "[22]\ttrain-mlogloss:0.39220\teval-mlogloss:0.38553\n",
      "[23]\ttrain-mlogloss:0.38521\teval-mlogloss:0.37825\n",
      "[24]\ttrain-mlogloss:0.37859\teval-mlogloss:0.37165\n",
      "[25]\ttrain-mlogloss:0.37215\teval-mlogloss:0.36511\n",
      "[26]\ttrain-mlogloss:0.36603\teval-mlogloss:0.35880\n",
      "[27]\ttrain-mlogloss:0.36019\teval-mlogloss:0.35282\n",
      "[28]\ttrain-mlogloss:0.35447\teval-mlogloss:0.34690\n",
      "[29]\ttrain-mlogloss:0.34899\teval-mlogloss:0.34120\n",
      "[30]\ttrain-mlogloss:0.34374\teval-mlogloss:0.33576\n",
      "[31]\ttrain-mlogloss:0.33872\teval-mlogloss:0.33053\n",
      "[32]\ttrain-mlogloss:0.33401\teval-mlogloss:0.32559\n",
      "[33]\ttrain-mlogloss:0.32929\teval-mlogloss:0.32062\n",
      "[34]\ttrain-mlogloss:0.32481\teval-mlogloss:0.31603\n",
      "[35]\ttrain-mlogloss:0.32059\teval-mlogloss:0.31178\n",
      "[36]\ttrain-mlogloss:0.31646\teval-mlogloss:0.30748\n",
      "[37]\ttrain-mlogloss:0.31244\teval-mlogloss:0.30351\n",
      "[38]\ttrain-mlogloss:0.30863\teval-mlogloss:0.29960\n",
      "[39]\ttrain-mlogloss:0.30483\teval-mlogloss:0.29570\n",
      "[40]\ttrain-mlogloss:0.30127\teval-mlogloss:0.29200\n",
      "[41]\ttrain-mlogloss:0.29790\teval-mlogloss:0.28865\n",
      "[42]\ttrain-mlogloss:0.29474\teval-mlogloss:0.28539\n",
      "[43]\ttrain-mlogloss:0.29129\teval-mlogloss:0.28211\n",
      "[44]\ttrain-mlogloss:0.28818\teval-mlogloss:0.27891\n",
      "[45]\ttrain-mlogloss:0.28525\teval-mlogloss:0.27603\n",
      "[46]\ttrain-mlogloss:0.28234\teval-mlogloss:0.27336\n",
      "[47]\ttrain-mlogloss:0.27957\teval-mlogloss:0.27055\n",
      "[48]\ttrain-mlogloss:0.27690\teval-mlogloss:0.26794\n",
      "[49]\ttrain-mlogloss:0.27416\teval-mlogloss:0.26556\n",
      "[50]\ttrain-mlogloss:0.27164\teval-mlogloss:0.26312\n",
      "[51]\ttrain-mlogloss:0.26897\teval-mlogloss:0.26083\n",
      "[52]\ttrain-mlogloss:0.26656\teval-mlogloss:0.25867\n",
      "[53]\ttrain-mlogloss:0.26445\teval-mlogloss:0.25657\n",
      "[54]\ttrain-mlogloss:0.26191\teval-mlogloss:0.25460\n",
      "[55]\ttrain-mlogloss:0.25968\teval-mlogloss:0.25252\n",
      "[56]\ttrain-mlogloss:0.25749\teval-mlogloss:0.25070\n",
      "[57]\ttrain-mlogloss:0.25537\teval-mlogloss:0.24900\n",
      "[58]\ttrain-mlogloss:0.25343\teval-mlogloss:0.24708\n",
      "[59]\ttrain-mlogloss:0.25166\teval-mlogloss:0.24527\n",
      "[60]\ttrain-mlogloss:0.25004\teval-mlogloss:0.24377\n",
      "[61]\ttrain-mlogloss:0.24827\teval-mlogloss:0.24254\n",
      "[62]\ttrain-mlogloss:0.24623\teval-mlogloss:0.24108\n",
      "[63]\ttrain-mlogloss:0.24474\teval-mlogloss:0.23968\n",
      "[64]\ttrain-mlogloss:0.24274\teval-mlogloss:0.23823\n",
      "[65]\ttrain-mlogloss:0.24087\teval-mlogloss:0.23682\n",
      "[66]\ttrain-mlogloss:0.23920\teval-mlogloss:0.23552\n",
      "[67]\ttrain-mlogloss:0.23772\teval-mlogloss:0.23445\n",
      "[68]\ttrain-mlogloss:0.23603\teval-mlogloss:0.23319\n",
      "[69]\ttrain-mlogloss:0.23466\teval-mlogloss:0.23217\n",
      "[70]\ttrain-mlogloss:0.23313\teval-mlogloss:0.23115\n",
      "[71]\ttrain-mlogloss:0.23163\teval-mlogloss:0.23037\n",
      "[72]\ttrain-mlogloss:0.23021\teval-mlogloss:0.22929\n",
      "[73]\ttrain-mlogloss:0.22864\teval-mlogloss:0.22838\n",
      "[74]\ttrain-mlogloss:0.22734\teval-mlogloss:0.22744\n",
      "[75]\ttrain-mlogloss:0.22587\teval-mlogloss:0.22659\n",
      "[76]\ttrain-mlogloss:0.22458\teval-mlogloss:0.22578\n",
      "[77]\ttrain-mlogloss:0.22323\teval-mlogloss:0.22502\n",
      "[78]\ttrain-mlogloss:0.22182\teval-mlogloss:0.22420\n",
      "[79]\ttrain-mlogloss:0.22033\teval-mlogloss:0.22351\n",
      "[80]\ttrain-mlogloss:0.21893\teval-mlogloss:0.22295\n",
      "[81]\ttrain-mlogloss:0.21771\teval-mlogloss:0.22225\n",
      "[82]\ttrain-mlogloss:0.21631\teval-mlogloss:0.22169\n",
      "[83]\ttrain-mlogloss:0.21491\teval-mlogloss:0.22114\n",
      "[84]\ttrain-mlogloss:0.21366\teval-mlogloss:0.22039\n",
      "[85]\ttrain-mlogloss:0.21247\teval-mlogloss:0.21979\n",
      "[86]\ttrain-mlogloss:0.21116\teval-mlogloss:0.21903\n",
      "[87]\ttrain-mlogloss:0.21003\teval-mlogloss:0.21845\n",
      "[88]\ttrain-mlogloss:0.20897\teval-mlogloss:0.21817\n",
      "[89]\ttrain-mlogloss:0.20797\teval-mlogloss:0.21746\n",
      "[90]\ttrain-mlogloss:0.20678\teval-mlogloss:0.21687\n",
      "[91]\ttrain-mlogloss:0.20565\teval-mlogloss:0.21637\n",
      "[92]\ttrain-mlogloss:0.20460\teval-mlogloss:0.21602\n",
      "[93]\ttrain-mlogloss:0.20330\teval-mlogloss:0.21568\n",
      "[94]\ttrain-mlogloss:0.20232\teval-mlogloss:0.21533\n",
      "[95]\ttrain-mlogloss:0.20146\teval-mlogloss:0.21477\n",
      "[96]\ttrain-mlogloss:0.20053\teval-mlogloss:0.21458\n",
      "[97]\ttrain-mlogloss:0.19963\teval-mlogloss:0.21425\n",
      "[98]\ttrain-mlogloss:0.19863\teval-mlogloss:0.21388\n",
      "[99]\ttrain-mlogloss:0.19766\teval-mlogloss:0.21376\n",
      "[100]\ttrain-mlogloss:0.19652\teval-mlogloss:0.21385\n",
      "[101]\ttrain-mlogloss:0.19557\teval-mlogloss:0.21321\n",
      "[102]\ttrain-mlogloss:0.19450\teval-mlogloss:0.21280\n",
      "[103]\ttrain-mlogloss:0.19342\teval-mlogloss:0.21251\n",
      "[104]\ttrain-mlogloss:0.19242\teval-mlogloss:0.21237\n",
      "[105]\ttrain-mlogloss:0.19141\teval-mlogloss:0.21246\n",
      "[106]\ttrain-mlogloss:0.19043\teval-mlogloss:0.21241\n",
      "[107]\ttrain-mlogloss:0.18960\teval-mlogloss:0.21233\n",
      "[108]\ttrain-mlogloss:0.18857\teval-mlogloss:0.21213\n",
      "[109]\ttrain-mlogloss:0.18758\teval-mlogloss:0.21196\n",
      "[110]\ttrain-mlogloss:0.18669\teval-mlogloss:0.21203\n",
      "[111]\ttrain-mlogloss:0.18578\teval-mlogloss:0.21188\n",
      "[112]\ttrain-mlogloss:0.18499\teval-mlogloss:0.21193\n",
      "[113]\ttrain-mlogloss:0.18408\teval-mlogloss:0.21184\n",
      "[114]\ttrain-mlogloss:0.18314\teval-mlogloss:0.21168\n",
      "[115]\ttrain-mlogloss:0.18213\teval-mlogloss:0.21163\n",
      "[116]\ttrain-mlogloss:0.18125\teval-mlogloss:0.21168\n",
      "[117]\ttrain-mlogloss:0.18030\teval-mlogloss:0.21169\n",
      "[118]\ttrain-mlogloss:0.17937\teval-mlogloss:0.21149\n",
      "[119]\ttrain-mlogloss:0.17846\teval-mlogloss:0.21110\n",
      "[120]\ttrain-mlogloss:0.17749\teval-mlogloss:0.21130\n",
      "[121]\ttrain-mlogloss:0.17665\teval-mlogloss:0.21128\n",
      "[122]\ttrain-mlogloss:0.17576\teval-mlogloss:0.21106\n",
      "[123]\ttrain-mlogloss:0.17495\teval-mlogloss:0.21067\n",
      "[124]\ttrain-mlogloss:0.17414\teval-mlogloss:0.21088\n",
      "[125]\ttrain-mlogloss:0.17335\teval-mlogloss:0.21084\n",
      "[126]\ttrain-mlogloss:0.17257\teval-mlogloss:0.21111\n",
      "[127]\ttrain-mlogloss:0.17192\teval-mlogloss:0.21106\n",
      "[128]\ttrain-mlogloss:0.17119\teval-mlogloss:0.21104\n",
      "[129]\ttrain-mlogloss:0.17034\teval-mlogloss:0.21107\n",
      "[130]\ttrain-mlogloss:0.16964\teval-mlogloss:0.21098\n",
      "[131]\ttrain-mlogloss:0.16889\teval-mlogloss:0.21094\n",
      "[132]\ttrain-mlogloss:0.16821\teval-mlogloss:0.21090\n",
      "[133]\ttrain-mlogloss:0.16740\teval-mlogloss:0.21090\n",
      "[134]\ttrain-mlogloss:0.16652\teval-mlogloss:0.21116\n",
      "[135]\ttrain-mlogloss:0.16577\teval-mlogloss:0.21114\n",
      "[136]\ttrain-mlogloss:0.16494\teval-mlogloss:0.21117\n",
      "[137]\ttrain-mlogloss:0.16419\teval-mlogloss:0.21121\n",
      "[138]\ttrain-mlogloss:0.16359\teval-mlogloss:0.21127\n",
      "[139]\ttrain-mlogloss:0.16299\teval-mlogloss:0.21154\n",
      "[140]\ttrain-mlogloss:0.16219\teval-mlogloss:0.21154\n",
      "[141]\ttrain-mlogloss:0.16146\teval-mlogloss:0.21133\n",
      "[142]\ttrain-mlogloss:0.16072\teval-mlogloss:0.21138\n",
      "[143]\ttrain-mlogloss:0.16000\teval-mlogloss:0.21138\n",
      "[144]\ttrain-mlogloss:0.15933\teval-mlogloss:0.21130\n",
      "[145]\ttrain-mlogloss:0.15859\teval-mlogloss:0.21135\n",
      "[146]\ttrain-mlogloss:0.15788\teval-mlogloss:0.21137\n",
      "[147]\ttrain-mlogloss:0.15715\teval-mlogloss:0.21146\n",
      "[148]\ttrain-mlogloss:0.15668\teval-mlogloss:0.21127\n",
      "[149]\ttrain-mlogloss:0.15604\teval-mlogloss:0.21159\n",
      "[150]\ttrain-mlogloss:0.15546\teval-mlogloss:0.21172\n",
      "[151]\ttrain-mlogloss:0.15488\teval-mlogloss:0.21167\n",
      "[152]\ttrain-mlogloss:0.15424\teval-mlogloss:0.21180\n",
      "[153]\ttrain-mlogloss:0.15357\teval-mlogloss:0.21188\n",
      "[154]\ttrain-mlogloss:0.15291\teval-mlogloss:0.21202\n",
      "[155]\ttrain-mlogloss:0.15227\teval-mlogloss:0.21205\n",
      "[156]\ttrain-mlogloss:0.15161\teval-mlogloss:0.21216\n",
      "[157]\ttrain-mlogloss:0.15097\teval-mlogloss:0.21245\n",
      "[158]\ttrain-mlogloss:0.15034\teval-mlogloss:0.21274\n",
      "[159]\ttrain-mlogloss:0.14971\teval-mlogloss:0.21288\n",
      "[160]\ttrain-mlogloss:0.14915\teval-mlogloss:0.21303\n",
      "[161]\ttrain-mlogloss:0.14849\teval-mlogloss:0.21311\n",
      "[162]\ttrain-mlogloss:0.14802\teval-mlogloss:0.21315\n",
      "[163]\ttrain-mlogloss:0.14744\teval-mlogloss:0.21320\n",
      "[164]\ttrain-mlogloss:0.14688\teval-mlogloss:0.21313\n",
      "[165]\ttrain-mlogloss:0.14635\teval-mlogloss:0.21328\n",
      "[166]\ttrain-mlogloss:0.14598\teval-mlogloss:0.21327\n",
      "[167]\ttrain-mlogloss:0.14541\teval-mlogloss:0.21357\n",
      "[168]\ttrain-mlogloss:0.14488\teval-mlogloss:0.21361\n",
      "[169]\ttrain-mlogloss:0.14433\teval-mlogloss:0.21367\n",
      "[170]\ttrain-mlogloss:0.14387\teval-mlogloss:0.21394\n",
      "[171]\ttrain-mlogloss:0.14346\teval-mlogloss:0.21408\n",
      "[172]\ttrain-mlogloss:0.14297\teval-mlogloss:0.21422\n",
      "[173]\ttrain-mlogloss:0.14236\teval-mlogloss:0.21433\n",
      "[174]\ttrain-mlogloss:0.14184\teval-mlogloss:0.21475\n",
      "[175]\ttrain-mlogloss:0.14122\teval-mlogloss:0.21497\n",
      "[176]\ttrain-mlogloss:0.14071\teval-mlogloss:0.21510\n",
      "[177]\ttrain-mlogloss:0.14037\teval-mlogloss:0.21509\n",
      "[178]\ttrain-mlogloss:0.13986\teval-mlogloss:0.21523\n",
      "[179]\ttrain-mlogloss:0.13934\teval-mlogloss:0.21540\n",
      "[180]\ttrain-mlogloss:0.13889\teval-mlogloss:0.21553\n",
      "[181]\ttrain-mlogloss:0.13852\teval-mlogloss:0.21566\n",
      "[182]\ttrain-mlogloss:0.13806\teval-mlogloss:0.21579\n",
      "[183]\ttrain-mlogloss:0.13772\teval-mlogloss:0.21589\n",
      "[184]\ttrain-mlogloss:0.13717\teval-mlogloss:0.21588\n",
      "[185]\ttrain-mlogloss:0.13665\teval-mlogloss:0.21604\n",
      "[186]\ttrain-mlogloss:0.13616\teval-mlogloss:0.21601\n",
      "[187]\ttrain-mlogloss:0.13576\teval-mlogloss:0.21604\n",
      "[188]\ttrain-mlogloss:0.13524\teval-mlogloss:0.21603\n",
      "[189]\ttrain-mlogloss:0.13483\teval-mlogloss:0.21639\n",
      "[190]\ttrain-mlogloss:0.13440\teval-mlogloss:0.21666\n",
      "[191]\ttrain-mlogloss:0.13405\teval-mlogloss:0.21659\n",
      "[192]\ttrain-mlogloss:0.13354\teval-mlogloss:0.21666\n",
      "[193]\ttrain-mlogloss:0.13316\teval-mlogloss:0.21683\n",
      "[194]\ttrain-mlogloss:0.13277\teval-mlogloss:0.21705\n",
      "[195]\ttrain-mlogloss:0.13244\teval-mlogloss:0.21724\n",
      "[196]\ttrain-mlogloss:0.13212\teval-mlogloss:0.21731\n",
      "[197]\ttrain-mlogloss:0.13171\teval-mlogloss:0.21758\n",
      "[198]\ttrain-mlogloss:0.13132\teval-mlogloss:0.21770\n",
      "[199]\ttrain-mlogloss:0.13090\teval-mlogloss:0.21782\n",
      "[200]\ttrain-mlogloss:0.13049\teval-mlogloss:0.21796\n",
      "[201]\ttrain-mlogloss:0.13009\teval-mlogloss:0.21811\n",
      "[202]\ttrain-mlogloss:0.12965\teval-mlogloss:0.21834\n",
      "[203]\ttrain-mlogloss:0.12936\teval-mlogloss:0.21843\n",
      "[204]\ttrain-mlogloss:0.12891\teval-mlogloss:0.21847\n",
      "[205]\ttrain-mlogloss:0.12851\teval-mlogloss:0.21871\n",
      "[206]\ttrain-mlogloss:0.12810\teval-mlogloss:0.21878\n",
      "[207]\ttrain-mlogloss:0.12778\teval-mlogloss:0.21886\n",
      "[208]\ttrain-mlogloss:0.12742\teval-mlogloss:0.21899\n",
      "[209]\ttrain-mlogloss:0.12711\teval-mlogloss:0.21907\n",
      "[210]\ttrain-mlogloss:0.12677\teval-mlogloss:0.21911\n",
      "[211]\ttrain-mlogloss:0.12646\teval-mlogloss:0.21924\n",
      "[212]\ttrain-mlogloss:0.12603\teval-mlogloss:0.21948\n",
      "[213]\ttrain-mlogloss:0.12564\teval-mlogloss:0.21929\n",
      "[214]\ttrain-mlogloss:0.12519\teval-mlogloss:0.21951\n",
      "[215]\ttrain-mlogloss:0.12487\teval-mlogloss:0.21948\n",
      "[216]\ttrain-mlogloss:0.12460\teval-mlogloss:0.21967\n",
      "[217]\ttrain-mlogloss:0.12430\teval-mlogloss:0.21956\n",
      "[218]\ttrain-mlogloss:0.12396\teval-mlogloss:0.21956\n",
      "[219]\ttrain-mlogloss:0.12371\teval-mlogloss:0.21978\n",
      "[220]\ttrain-mlogloss:0.12322\teval-mlogloss:0.21994\n",
      "[221]\ttrain-mlogloss:0.12290\teval-mlogloss:0.22000\n",
      "[222]\ttrain-mlogloss:0.12262\teval-mlogloss:0.22027\n",
      "xgb now score is: [2.4585792262014, 2.21614713832736, 2.467062568301335, 2.5213224390428515]\n",
      "[0]\ttrain-mlogloss:0.67108\teval-mlogloss:0.67131\n",
      "[1]\ttrain-mlogloss:0.65033\teval-mlogloss:0.65045\n",
      "[2]\ttrain-mlogloss:0.63058\teval-mlogloss:0.63069\n",
      "[3]\ttrain-mlogloss:0.61174\teval-mlogloss:0.61155\n",
      "[4]\ttrain-mlogloss:0.59406\teval-mlogloss:0.59369\n",
      "[5]\ttrain-mlogloss:0.57720\teval-mlogloss:0.57660\n",
      "[6]\ttrain-mlogloss:0.56134\teval-mlogloss:0.56068\n",
      "[7]\ttrain-mlogloss:0.54631\teval-mlogloss:0.54535\n",
      "[8]\ttrain-mlogloss:0.53197\teval-mlogloss:0.53086\n",
      "[9]\ttrain-mlogloss:0.51829\teval-mlogloss:0.51717\n",
      "[10]\ttrain-mlogloss:0.50534\teval-mlogloss:0.50411\n",
      "[11]\ttrain-mlogloss:0.49301\teval-mlogloss:0.49152\n",
      "[12]\ttrain-mlogloss:0.48142\teval-mlogloss:0.47977\n",
      "[13]\ttrain-mlogloss:0.47011\teval-mlogloss:0.46845\n",
      "[14]\ttrain-mlogloss:0.45942\teval-mlogloss:0.45755\n",
      "[15]\ttrain-mlogloss:0.44947\teval-mlogloss:0.44747\n",
      "[16]\ttrain-mlogloss:0.43981\teval-mlogloss:0.43762\n",
      "[17]\ttrain-mlogloss:0.43057\teval-mlogloss:0.42841\n",
      "[18]\ttrain-mlogloss:0.42168\teval-mlogloss:0.41913\n",
      "[19]\ttrain-mlogloss:0.41306\teval-mlogloss:0.41081\n",
      "[20]\ttrain-mlogloss:0.40507\teval-mlogloss:0.40274\n",
      "[21]\ttrain-mlogloss:0.39739\teval-mlogloss:0.39499\n",
      "[22]\ttrain-mlogloss:0.39008\teval-mlogloss:0.38755\n",
      "[23]\ttrain-mlogloss:0.38300\teval-mlogloss:0.38035\n",
      "[24]\ttrain-mlogloss:0.37605\teval-mlogloss:0.37329\n",
      "[25]\ttrain-mlogloss:0.36957\teval-mlogloss:0.36685\n",
      "[26]\ttrain-mlogloss:0.36331\teval-mlogloss:0.36045\n",
      "[27]\ttrain-mlogloss:0.35752\teval-mlogloss:0.35450\n",
      "[28]\ttrain-mlogloss:0.35176\teval-mlogloss:0.34872\n",
      "[29]\ttrain-mlogloss:0.34629\teval-mlogloss:0.34329\n",
      "[30]\ttrain-mlogloss:0.34105\teval-mlogloss:0.33799\n",
      "[31]\ttrain-mlogloss:0.33566\teval-mlogloss:0.33292\n",
      "[32]\ttrain-mlogloss:0.33065\teval-mlogloss:0.32794\n",
      "[33]\ttrain-mlogloss:0.32610\teval-mlogloss:0.32331\n",
      "[34]\ttrain-mlogloss:0.32146\teval-mlogloss:0.31877\n",
      "[35]\ttrain-mlogloss:0.31701\teval-mlogloss:0.31431\n",
      "[36]\ttrain-mlogloss:0.31276\teval-mlogloss:0.31006\n",
      "[37]\ttrain-mlogloss:0.30868\teval-mlogloss:0.30609\n",
      "[38]\ttrain-mlogloss:0.30483\teval-mlogloss:0.30224\n",
      "[39]\ttrain-mlogloss:0.30096\teval-mlogloss:0.29850\n",
      "[40]\ttrain-mlogloss:0.29715\teval-mlogloss:0.29512\n",
      "[41]\ttrain-mlogloss:0.29362\teval-mlogloss:0.29180\n",
      "[42]\ttrain-mlogloss:0.29030\teval-mlogloss:0.28871\n",
      "[43]\ttrain-mlogloss:0.28721\teval-mlogloss:0.28571\n",
      "[44]\ttrain-mlogloss:0.28394\teval-mlogloss:0.28283\n",
      "[45]\ttrain-mlogloss:0.28101\teval-mlogloss:0.27982\n",
      "[46]\ttrain-mlogloss:0.27821\teval-mlogloss:0.27723\n",
      "[47]\ttrain-mlogloss:0.27552\teval-mlogloss:0.27475\n",
      "[48]\ttrain-mlogloss:0.27299\teval-mlogloss:0.27242\n",
      "[49]\ttrain-mlogloss:0.27053\teval-mlogloss:0.26982\n",
      "[50]\ttrain-mlogloss:0.26814\teval-mlogloss:0.26754\n",
      "[51]\ttrain-mlogloss:0.26572\teval-mlogloss:0.26532\n",
      "[52]\ttrain-mlogloss:0.26313\teval-mlogloss:0.26301\n",
      "[53]\ttrain-mlogloss:0.26080\teval-mlogloss:0.26103\n",
      "[54]\ttrain-mlogloss:0.25862\teval-mlogloss:0.25903\n",
      "[55]\ttrain-mlogloss:0.25620\teval-mlogloss:0.25712\n",
      "[56]\ttrain-mlogloss:0.25402\teval-mlogloss:0.25500\n",
      "[57]\ttrain-mlogloss:0.25207\teval-mlogloss:0.25313\n",
      "[58]\ttrain-mlogloss:0.25007\teval-mlogloss:0.25141\n",
      "[59]\ttrain-mlogloss:0.24813\teval-mlogloss:0.24979\n",
      "[60]\ttrain-mlogloss:0.24620\teval-mlogloss:0.24837\n",
      "[61]\ttrain-mlogloss:0.24435\teval-mlogloss:0.24692\n",
      "[62]\ttrain-mlogloss:0.24252\teval-mlogloss:0.24529\n",
      "[63]\ttrain-mlogloss:0.24077\teval-mlogloss:0.24414\n",
      "[64]\ttrain-mlogloss:0.23909\teval-mlogloss:0.24285\n",
      "[65]\ttrain-mlogloss:0.23730\teval-mlogloss:0.24159\n",
      "[66]\ttrain-mlogloss:0.23573\teval-mlogloss:0.24046\n",
      "[67]\ttrain-mlogloss:0.23395\teval-mlogloss:0.23923\n",
      "[68]\ttrain-mlogloss:0.23250\teval-mlogloss:0.23825\n",
      "[69]\ttrain-mlogloss:0.23117\teval-mlogloss:0.23721\n",
      "[70]\ttrain-mlogloss:0.22981\teval-mlogloss:0.23622\n",
      "[71]\ttrain-mlogloss:0.22855\teval-mlogloss:0.23536\n",
      "[72]\ttrain-mlogloss:0.22699\teval-mlogloss:0.23454\n",
      "[73]\ttrain-mlogloss:0.22559\teval-mlogloss:0.23342\n",
      "[74]\ttrain-mlogloss:0.22416\teval-mlogloss:0.23272\n",
      "[75]\ttrain-mlogloss:0.22280\teval-mlogloss:0.23205\n",
      "[76]\ttrain-mlogloss:0.22145\teval-mlogloss:0.23136\n",
      "[77]\ttrain-mlogloss:0.22000\teval-mlogloss:0.23054\n",
      "[78]\ttrain-mlogloss:0.21863\teval-mlogloss:0.22996\n",
      "[79]\ttrain-mlogloss:0.21730\teval-mlogloss:0.22953\n",
      "[80]\ttrain-mlogloss:0.21614\teval-mlogloss:0.22883\n",
      "[81]\ttrain-mlogloss:0.21467\teval-mlogloss:0.22796\n",
      "[82]\ttrain-mlogloss:0.21336\teval-mlogloss:0.22734\n",
      "[83]\ttrain-mlogloss:0.21220\teval-mlogloss:0.22688\n",
      "[84]\ttrain-mlogloss:0.21099\teval-mlogloss:0.22647\n",
      "[85]\ttrain-mlogloss:0.20993\teval-mlogloss:0.22600\n",
      "[86]\ttrain-mlogloss:0.20888\teval-mlogloss:0.22534\n",
      "[87]\ttrain-mlogloss:0.20780\teval-mlogloss:0.22490\n",
      "[88]\ttrain-mlogloss:0.20662\teval-mlogloss:0.22429\n",
      "[89]\ttrain-mlogloss:0.20537\teval-mlogloss:0.22379\n",
      "[90]\ttrain-mlogloss:0.20404\teval-mlogloss:0.22329\n",
      "[91]\ttrain-mlogloss:0.20283\teval-mlogloss:0.22287\n",
      "[92]\ttrain-mlogloss:0.20168\teval-mlogloss:0.22248\n",
      "[93]\ttrain-mlogloss:0.20062\teval-mlogloss:0.22215\n",
      "[94]\ttrain-mlogloss:0.19966\teval-mlogloss:0.22172\n",
      "[95]\ttrain-mlogloss:0.19850\teval-mlogloss:0.22114\n",
      "[96]\ttrain-mlogloss:0.19754\teval-mlogloss:0.22088\n",
      "[97]\ttrain-mlogloss:0.19664\teval-mlogloss:0.22064\n",
      "[98]\ttrain-mlogloss:0.19554\teval-mlogloss:0.22051\n",
      "[99]\ttrain-mlogloss:0.19456\teval-mlogloss:0.22016\n",
      "[100]\ttrain-mlogloss:0.19361\teval-mlogloss:0.22012\n",
      "[101]\ttrain-mlogloss:0.19264\teval-mlogloss:0.21983\n",
      "[102]\ttrain-mlogloss:0.19174\teval-mlogloss:0.21960\n",
      "[103]\ttrain-mlogloss:0.19075\teval-mlogloss:0.21943\n",
      "[104]\ttrain-mlogloss:0.18992\teval-mlogloss:0.21906\n",
      "[105]\ttrain-mlogloss:0.18902\teval-mlogloss:0.21879\n",
      "[106]\ttrain-mlogloss:0.18799\teval-mlogloss:0.21861\n",
      "[107]\ttrain-mlogloss:0.18709\teval-mlogloss:0.21858\n",
      "[108]\ttrain-mlogloss:0.18620\teval-mlogloss:0.21842\n",
      "[109]\ttrain-mlogloss:0.18535\teval-mlogloss:0.21834\n",
      "[110]\ttrain-mlogloss:0.18441\teval-mlogloss:0.21845\n",
      "[111]\ttrain-mlogloss:0.18345\teval-mlogloss:0.21827\n",
      "[112]\ttrain-mlogloss:0.18255\teval-mlogloss:0.21792\n",
      "[113]\ttrain-mlogloss:0.18165\teval-mlogloss:0.21797\n",
      "[114]\ttrain-mlogloss:0.18075\teval-mlogloss:0.21801\n",
      "[115]\ttrain-mlogloss:0.18005\teval-mlogloss:0.21790\n",
      "[116]\ttrain-mlogloss:0.17924\teval-mlogloss:0.21801\n",
      "[117]\ttrain-mlogloss:0.17834\teval-mlogloss:0.21815\n",
      "[118]\ttrain-mlogloss:0.17751\teval-mlogloss:0.21807\n",
      "[119]\ttrain-mlogloss:0.17667\teval-mlogloss:0.21793\n",
      "[120]\ttrain-mlogloss:0.17597\teval-mlogloss:0.21799\n",
      "[121]\ttrain-mlogloss:0.17509\teval-mlogloss:0.21765\n",
      "[122]\ttrain-mlogloss:0.17409\teval-mlogloss:0.21731\n",
      "[123]\ttrain-mlogloss:0.17341\teval-mlogloss:0.21720\n",
      "[124]\ttrain-mlogloss:0.17257\teval-mlogloss:0.21704\n",
      "[125]\ttrain-mlogloss:0.17181\teval-mlogloss:0.21699\n",
      "[126]\ttrain-mlogloss:0.17118\teval-mlogloss:0.21679\n",
      "[127]\ttrain-mlogloss:0.17058\teval-mlogloss:0.21647\n",
      "[128]\ttrain-mlogloss:0.16997\teval-mlogloss:0.21641\n",
      "[129]\ttrain-mlogloss:0.16928\teval-mlogloss:0.21642\n",
      "[130]\ttrain-mlogloss:0.16872\teval-mlogloss:0.21652\n",
      "[131]\ttrain-mlogloss:0.16798\teval-mlogloss:0.21630\n",
      "[132]\ttrain-mlogloss:0.16734\teval-mlogloss:0.21643\n",
      "[133]\ttrain-mlogloss:0.16653\teval-mlogloss:0.21644\n",
      "[134]\ttrain-mlogloss:0.16574\teval-mlogloss:0.21659\n",
      "[135]\ttrain-mlogloss:0.16498\teval-mlogloss:0.21652\n",
      "[136]\ttrain-mlogloss:0.16421\teval-mlogloss:0.21649\n",
      "[137]\ttrain-mlogloss:0.16366\teval-mlogloss:0.21644\n",
      "[138]\ttrain-mlogloss:0.16294\teval-mlogloss:0.21665\n",
      "[139]\ttrain-mlogloss:0.16232\teval-mlogloss:0.21652\n",
      "[140]\ttrain-mlogloss:0.16178\teval-mlogloss:0.21643\n",
      "[141]\ttrain-mlogloss:0.16109\teval-mlogloss:0.21652\n",
      "[142]\ttrain-mlogloss:0.16051\teval-mlogloss:0.21675\n",
      "[143]\ttrain-mlogloss:0.15997\teval-mlogloss:0.21679\n",
      "[144]\ttrain-mlogloss:0.15946\teval-mlogloss:0.21667\n",
      "[145]\ttrain-mlogloss:0.15882\teval-mlogloss:0.21677\n",
      "[146]\ttrain-mlogloss:0.15822\teval-mlogloss:0.21674\n",
      "[147]\ttrain-mlogloss:0.15754\teval-mlogloss:0.21690\n",
      "[148]\ttrain-mlogloss:0.15699\teval-mlogloss:0.21704\n",
      "[149]\ttrain-mlogloss:0.15647\teval-mlogloss:0.21686\n",
      "[150]\ttrain-mlogloss:0.15584\teval-mlogloss:0.21698\n",
      "[151]\ttrain-mlogloss:0.15524\teval-mlogloss:0.21699\n",
      "[152]\ttrain-mlogloss:0.15467\teval-mlogloss:0.21704\n",
      "[153]\ttrain-mlogloss:0.15408\teval-mlogloss:0.21705\n",
      "[154]\ttrain-mlogloss:0.15356\teval-mlogloss:0.21701\n",
      "[155]\ttrain-mlogloss:0.15294\teval-mlogloss:0.21711\n",
      "[156]\ttrain-mlogloss:0.15230\teval-mlogloss:0.21725\n",
      "[157]\ttrain-mlogloss:0.15180\teval-mlogloss:0.21725\n",
      "[158]\ttrain-mlogloss:0.15120\teval-mlogloss:0.21742\n",
      "[159]\ttrain-mlogloss:0.15063\teval-mlogloss:0.21725\n",
      "[160]\ttrain-mlogloss:0.15000\teval-mlogloss:0.21728\n",
      "[161]\ttrain-mlogloss:0.14949\teval-mlogloss:0.21728\n",
      "[162]\ttrain-mlogloss:0.14910\teval-mlogloss:0.21730\n",
      "[163]\ttrain-mlogloss:0.14860\teval-mlogloss:0.21723\n",
      "[164]\ttrain-mlogloss:0.14815\teval-mlogloss:0.21745\n",
      "[165]\ttrain-mlogloss:0.14758\teval-mlogloss:0.21749\n",
      "[166]\ttrain-mlogloss:0.14705\teval-mlogloss:0.21756\n",
      "[167]\ttrain-mlogloss:0.14646\teval-mlogloss:0.21766\n",
      "[168]\ttrain-mlogloss:0.14594\teval-mlogloss:0.21776\n",
      "[169]\ttrain-mlogloss:0.14543\teval-mlogloss:0.21764\n",
      "[170]\ttrain-mlogloss:0.14491\teval-mlogloss:0.21771\n",
      "[171]\ttrain-mlogloss:0.14447\teval-mlogloss:0.21813\n",
      "[172]\ttrain-mlogloss:0.14395\teval-mlogloss:0.21815\n",
      "[173]\ttrain-mlogloss:0.14341\teval-mlogloss:0.21811\n",
      "[174]\ttrain-mlogloss:0.14307\teval-mlogloss:0.21820\n",
      "[175]\ttrain-mlogloss:0.14263\teval-mlogloss:0.21828\n",
      "[176]\ttrain-mlogloss:0.14217\teval-mlogloss:0.21808\n",
      "[177]\ttrain-mlogloss:0.14170\teval-mlogloss:0.21813\n",
      "[178]\ttrain-mlogloss:0.14128\teval-mlogloss:0.21806\n",
      "[179]\ttrain-mlogloss:0.14088\teval-mlogloss:0.21789\n",
      "[180]\ttrain-mlogloss:0.14037\teval-mlogloss:0.21801\n",
      "[181]\ttrain-mlogloss:0.13995\teval-mlogloss:0.21795\n",
      "[182]\ttrain-mlogloss:0.13937\teval-mlogloss:0.21806\n",
      "[183]\ttrain-mlogloss:0.13888\teval-mlogloss:0.21810\n",
      "[184]\ttrain-mlogloss:0.13843\teval-mlogloss:0.21787\n",
      "[185]\ttrain-mlogloss:0.13805\teval-mlogloss:0.21787\n",
      "[186]\ttrain-mlogloss:0.13763\teval-mlogloss:0.21794\n",
      "[187]\ttrain-mlogloss:0.13712\teval-mlogloss:0.21817\n",
      "[188]\ttrain-mlogloss:0.13666\teval-mlogloss:0.21823\n",
      "[189]\ttrain-mlogloss:0.13615\teval-mlogloss:0.21831\n",
      "[190]\ttrain-mlogloss:0.13576\teval-mlogloss:0.21860\n",
      "[191]\ttrain-mlogloss:0.13528\teval-mlogloss:0.21863\n",
      "[192]\ttrain-mlogloss:0.13478\teval-mlogloss:0.21869\n",
      "[193]\ttrain-mlogloss:0.13440\teval-mlogloss:0.21871\n",
      "[194]\ttrain-mlogloss:0.13397\teval-mlogloss:0.21880\n",
      "[195]\ttrain-mlogloss:0.13351\teval-mlogloss:0.21879\n",
      "[196]\ttrain-mlogloss:0.13306\teval-mlogloss:0.21888\n",
      "[197]\ttrain-mlogloss:0.13272\teval-mlogloss:0.21884\n",
      "[198]\ttrain-mlogloss:0.13234\teval-mlogloss:0.21907\n",
      "[199]\ttrain-mlogloss:0.13199\teval-mlogloss:0.21916\n",
      "[200]\ttrain-mlogloss:0.13156\teval-mlogloss:0.21904\n",
      "[201]\ttrain-mlogloss:0.13119\teval-mlogloss:0.21876\n",
      "[202]\ttrain-mlogloss:0.13074\teval-mlogloss:0.21887\n",
      "[203]\ttrain-mlogloss:0.13033\teval-mlogloss:0.21892\n",
      "[204]\ttrain-mlogloss:0.12997\teval-mlogloss:0.21895\n",
      "[205]\ttrain-mlogloss:0.12959\teval-mlogloss:0.21915\n",
      "[206]\ttrain-mlogloss:0.12920\teval-mlogloss:0.21900\n",
      "[207]\ttrain-mlogloss:0.12883\teval-mlogloss:0.21929\n",
      "[208]\ttrain-mlogloss:0.12841\teval-mlogloss:0.21949\n",
      "[209]\ttrain-mlogloss:0.12803\teval-mlogloss:0.21973\n",
      "[210]\ttrain-mlogloss:0.12769\teval-mlogloss:0.21969\n",
      "[211]\ttrain-mlogloss:0.12727\teval-mlogloss:0.21966\n",
      "[212]\ttrain-mlogloss:0.12684\teval-mlogloss:0.21970\n",
      "[213]\ttrain-mlogloss:0.12663\teval-mlogloss:0.21981\n",
      "[214]\ttrain-mlogloss:0.12627\teval-mlogloss:0.21995\n",
      "[215]\ttrain-mlogloss:0.12603\teval-mlogloss:0.22009\n",
      "[216]\ttrain-mlogloss:0.12571\teval-mlogloss:0.22004\n",
      "[217]\ttrain-mlogloss:0.12537\teval-mlogloss:0.22015\n",
      "[218]\ttrain-mlogloss:0.12498\teval-mlogloss:0.22032\n",
      "[219]\ttrain-mlogloss:0.12456\teval-mlogloss:0.22031\n",
      "[220]\ttrain-mlogloss:0.12425\teval-mlogloss:0.22059\n",
      "[221]\ttrain-mlogloss:0.12401\teval-mlogloss:0.22053\n",
      "[222]\ttrain-mlogloss:0.12370\teval-mlogloss:0.22069\n",
      "[223]\ttrain-mlogloss:0.12328\teval-mlogloss:0.22078\n",
      "[224]\ttrain-mlogloss:0.12299\teval-mlogloss:0.22087\n",
      "[225]\ttrain-mlogloss:0.12275\teval-mlogloss:0.22082\n",
      "[226]\ttrain-mlogloss:0.12243\teval-mlogloss:0.22070\n",
      "[227]\ttrain-mlogloss:0.12207\teval-mlogloss:0.22070\n",
      "[228]\ttrain-mlogloss:0.12180\teval-mlogloss:0.22070\n",
      "[229]\ttrain-mlogloss:0.12157\teval-mlogloss:0.22059\n",
      "[230]\ttrain-mlogloss:0.12127\teval-mlogloss:0.22044\n"
     ]
    }
   ],
   "source": [
    "# 获取stacking特征\n",
    "clf_list = clf_list\n",
    "column_list = []\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "for clf in clf_list:\n",
    "    train_data, test_data, clf_name = clf(x_train, y_train, x_valid, kf, label_split=None)\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data_list)\n",
    "train_stacking = np.concatenate(train_data_list, axis=1)\n",
    "test_stacking = np.concatenate(test_data_list, axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "(2000, 226)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 原始特征和Stacking特征合并\n",
    "train = pd.DataFrame(np.concatenate([x_train, train_stacking], axis=1))\n",
    "test = pd.DataFrame(np.concatenate([x_valid, test_stacking], axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 特征重命名\n",
    "df_train_all = pd.DataFrame(train)\n",
    "df_train_all.columns = features_columns + clf_list_col\n",
    "df_test_all = pd.DataFrame(test)\n",
    "df_test_all.columns = features_columns + clf_list_col"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 获取数据ID及特征标签label：\n",
    "df_train_all['user_id'] = all_data_test[~all_data_test['label'].isna()]['user_id']\n",
    "df_test_all['user_id'] = all_data_test[~all_data_test['label'].isna()]['user_id']\n",
    "df_train_all['label'] = all_data_test[~all_data_test['label'].isna()]['label']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 6. 保存特征\n",
    "df_train_all.to_csv('./dataset/train_all.csv', header=True, index=False)\n",
    "df_test_all.to_csv('./dataset/test_all.csv', header=True, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}